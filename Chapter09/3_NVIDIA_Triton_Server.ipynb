{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ca49e0-7b9d-42e4-bc8d-c97b413e1145",
   "metadata": {},
   "source": [
    "# Using Triton Model Server\n",
    "\n",
    "**NVIDIA Triton** is an open source model server developed by NVIDIA. It supports multiple DL frameworks (such as TensorFlow, PyTorch, ONNX, Python, and OpenVINO), as well various hardware platforms and runtime environments (NVIDIA GPUs, x86 and ARM CPUs, and AWS Inferentia). Triton can be used for inference in cloud and data center environments and edge or mobile devices. Triton is optimized for performance and scalability on various CPU and GPU platforms. NVIDIA provides a specialized utility for performance analysis and model analysis to improve Tritonâ€™s performance.\n",
    "\n",
    "You can use Triton model servers by utilizing a pre-built SageMaker DL container with it. Note that SageMaker Triton containers are not open source. You can find the latest list of Triton containers here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md#nvidia-triton-inference-containers-sm-support-only.\n",
    "\n",
    "In this example, we will deploy the image classification PyTorch ResNet50 model using Triton. First, we need to compile the model to the TensorRT runtime; then, the compiled model will be packaged and deployed to the Triton model server. \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "There are several prerequisites in order to compile ResNet50 model to TensorRT runtime:\n",
    "- Compilation environment with preinstalled dependencies. For this, we will use official NVIDIA PyTorch container `nvcr.io/nvidia/ pytorch:22.05-py3`.\n",
    "- Compute instance with NVIDIA Docker and CUDA. You can use `g4dn` instance, for instance.\n",
    "\n",
    "\n",
    "## Compiling Model for TensorRT\n",
    "\n",
    "To compile model, we need to run compilation code within compilation environment - NVIDIA PyTorch Container. Let's review compilation code below.\n",
    "\n",
    "### Preparing Compilation Code\n",
    "\n",
    "Compilation code is available here: `3_src/compile_tensorrt.py`. We highlight key code blocks below:\n",
    "1. We will start by loading the model from PyTorch Hub, setting it to evaluation mode, and placing it on the GPU device:\n",
    "```python\n",
    "    MODEL_NAME = \"resnet50\"\n",
    "    model = (\n",
    "        torch.hub.load(\"pytorch/vision:v0.10.0\", MODEL_NAME, pretrained=True)\n",
    "        .eval()\n",
    "        .to(device)\n",
    "    )\n",
    "```\n",
    "2. Next, we will compile it using the TensorRT-Torch compiler. As part of the compiler configuration, we will specify the expected inputs and target precision. Note that since we plan to use dynamic batching for our model, we will provide several input shapes with different values for the batch dimensions:\n",
    "```python\n",
    "    trt_model = torch_tensorrt.compile(\n",
    "        model,\n",
    "        inputs=[torch_tensorrt.Input((1, 3, 224, 224))],\n",
    "        enabled_precisions={torch.float32},\n",
    "    )\n",
    "```\n",
    "3. We then save compiled model:\n",
    "```python\n",
    "    torch.jit.save(trt_model, os.path.join(model_dir, \"model.pt\"))\n",
    "```\n",
    "\n",
    "### Running Compilation Code\n",
    "\n",
    "Once we have compilation code ready, we can run it inside NVIDIA PyTorch container. \n",
    "\n",
    "1. First, we need to start a Docker container with following command in separate console (note, not in Jupyter notebook): \n",
    "`docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it --rm -v $PWD/Chapter09/3_src:/workspace/3_src nvcr.io/nvidia/ pytorch:22.05-py3`\n",
    "2. Your console session will open inside a container, where you can execute the compilation script by running the `python 3_src/compile_tensorrt.py` command.\n",
    "\n",
    "The resulting model.pt file will be available outside of the Docker container in the `3_src` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82232ba",
   "metadata": {},
   "source": [
    "## Preparing Model Package\n",
    "\n",
    "Once we have model compiled, we need to prepare model package. We mentioned previously that Triton uses a configuration file with a specific convention to define model signatures and runtime configuration. \n",
    "\n",
    "\n",
    "### Creating Inference Configuration\n",
    "Run cell below below to create `config.pbtxt` file that we can use to host the ResNet50 model. Here, we define batching parameters (the max batch size and dynamic batching config), input and output signatures, as well as model copies and the target hardware environment (via the instance_group object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8931d9-bafe-4469-8a9e-3746554f4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./3_src/resnet50/config.pbtxt\n",
    "name: \"resnet50\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, 224, 224 ]\n",
    "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 1000 ,1, 1]\n",
    "    reshape { shape: [ 1, 1000 ] }\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ad427",
   "metadata": {},
   "source": [
    "### Packaging Model Artifacts\n",
    "\n",
    "To deploy the compiled model with its configuration, we need to bundle everything into a single tar.gz archive and upload it to Amazon S3. The following code shows the directory structure within the model archive:\n",
    "\n",
    "```python\n",
    "resnet50 \n",
    "    |- 1\n",
    "        |- model.pt\n",
    "    |- config.pbtxt\n",
    "```\n",
    "\n",
    "Execute the cell below to prepare model package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40fa3e-2eb5-4de9-bc06-dab72df696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf 3_src/resnet50.tar.gz 3_src/resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db37d15",
   "metadata": {},
   "source": [
    "Finally, we upload model archive to Amazon S3. For this, we instantiate SageMaker Session object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1feddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'triton'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "model_data = sagemaker_session.upload_data(\"resnet50.tar.gz\",\n",
    "                                           bucket,\n",
    "                                           prefix)\n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b0993",
   "metadata": {},
   "source": [
    "## Deploying Triton Endpoint\n",
    "\n",
    "The Triton inference container is not supported by the SageMaker Python SDK. Hence, we will need to use the boto3 SageMaker client to deploy the model.\n",
    "\n",
    "1. First, we need to identify the correct Triton image. Use the following code to find the Triton container URI based on your version of the Triton server (we used `22.05` for both model compilation and serving) and your AWS region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c57bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:21.08-py3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8445c78-72f8-4df7-9119-5936782acb28",
   "metadata": {},
   "source": [
    "2. Next, we create the model, which defines the model data and serving container, as well as other parameters, such as environment variables. Note, that we use `sagemaker_client` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "sm_model_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"resnet50\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c584214",
   "metadata": {},
   "source": [
    "3. After that, we can define the endpoint configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b8035",
   "metadata": {},
   "source": [
    "4. Now, we are ready to deploy our endpoint. We added waiter method below to wait for endpoint to be fully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78894fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b24d85",
   "metadata": {},
   "source": [
    "## Running Inference\n",
    "\n",
    "To run inference, we first download and pre-preprocess sample image to match expected model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9054f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(\n",
    "    \"sagemaker-sample-files\",\n",
    "    \"datasets/image/pets/shiba_inu_dog.jpg\",\n",
    "    \"shiba_inu_dog.jpg\"\n",
    ")\n",
    "\n",
    "def get_sample_image():\n",
    "    image_path = \"./shiba_inu_dog.jpg\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((224, 224))\n",
    "    img = (np.array(img).astype(np.float32) / 255) - np.array(\n",
    "        [0.485, 0.456, 0.406], dtype=np.float32\n",
    "    ).reshape(1, 1, 3)\n",
    "    img = img / np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return img.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbb001",
   "metadata": {},
   "source": [
    "Next, we construct a payload according to the model signature defined in `config.pbtxt`. Take a look at the following inference call. The response will follow a defined output signature as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [1, 3, 224, 224],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": get_sample_image(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9a899",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Execute cell below to delete endpoints and model artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ecada",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
