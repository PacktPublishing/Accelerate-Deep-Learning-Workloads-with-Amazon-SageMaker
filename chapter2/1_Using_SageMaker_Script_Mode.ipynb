{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f299ab1d",
   "metadata": {},
   "source": [
    "# Develop training and inference scripts for Script Mode\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will learn how to develop training and inference scripts using HuggingFace framework. We will leverage SageMaker pre-build containers for HuggingFace (with PyTorch backend).\n",
    "\n",
    "We chose to solve a typical NLP task - text classification. We will use `20 Newsgroups` dataset which assembles ~ 20,000 newsgroup documents across 20 different newsgroups (categories).\n",
    "\n",
    "By the end of this notebook you will learn how to:\n",
    "- prepare text corpus for training and inference using Amazon SageMaker;\n",
    "- develop training script to run in pre-build HugginFace container;\n",
    "- configure and schedule training job;\n",
    "- develop inference code;\n",
    "- configure and deploy real-time inference endpoint;\n",
    "- test SageMaker endpoint.\n",
    "\n",
    "Please note, that this notebook was tested on SageMaker Notebook instance with latest PyTorch dependencies installed (conda environment `conda_pytorch_latest_p36`). If you are using different environment, please make sure to install following Python dependencies via PIP or Conda installers:\n",
    "- `scikit-learn`.\n",
    "- `sagemaker`.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Selecting Model Architecture\n",
    "Our study task is to train model which can categorize newsgroup article based on its content into one of categories.\n",
    "\n",
    "There are number of model architecture which can address this task. Existing State-of-the-art (SOTA) models are usually based on Transformer architecture. Autoregressive models like BERT and its various derivatives are suitable for this task. We will use concept known as `Transfer learning` where pre-trained model on one task is used for a new task with minimal modifications. \n",
    "\n",
    "As a baseline model we will use model architecture known as `DistilBERT` which provides high accuracy on wide variety of tasks and is considerably smaller than other models (for instance, original BERT model). To adapt model for classification task, we would need to add a classification layer which will be trained during our training to recognize articles.\n",
    "\n",
    "![title](static/finetuning.png)\n",
    "\n",
    "`HuggingFace Transformers` simplifies model selection and modification for fine-tuning:\n",
    "- provides rich model zoo with number pre-trained models and tokenizers.\n",
    "- has simple model API to modify baseline model for finetuning for specific task.\n",
    "- implements inference pipelines, combining data preprocessing and actual inference together.\n",
    "\n",
    "### Selecting SageMaker Training Containers\n",
    "\n",
    "Amazon SageMaker supports HuggingFace Transformer framework for inference and trainining. Hence, we won't need to develop any custom container. Instead we will use `Script Mode` feature to provide our custom training and inference scripts and execute them in pre-build containers. In this example we will develop intution how to develop these scripts.\n",
    "\n",
    "## Preparing Dataset\n",
    "First of, we need to acquire `20 Newsgroups` dataset. For this, we can use `sklearn` module utility. To shorten training cycle, let's choose 6 newsgroup categories (original dataset contains 20). The datasets will be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We select 6 out of 20 diverse newsgroups\n",
    "categories = [\n",
    "    \"comp.windows.x\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.electronics\",\n",
    "    \"misc.forsale\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"alt.atheism\"\n",
    "]\n",
    "\n",
    "train_dataset = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "test_dataset = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "\n",
    "n=6 # arbitrary sample index\n",
    "print(f\"Number of training samples: {len(train_dataset['data'])}\")\n",
    "print(f\"Number of test samples: {len(test_dataset['data'])}\")\n",
    "\n",
    "print(f\"\\n=========== Sample article for category {train_dataset['target'][n]} ============== \\n\")\n",
    "print(f\"{train_dataset['data'][n]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e2c05",
   "metadata": {},
   "source": [
    "Now, we need to save selected datasets into files and upload resulting files to Amazon S3 storage.\n",
    "SageMaker will download them to training container at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b4e35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for file in ['train_dataset.csv', 'test_dataset.csv']:\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.DictWriter(f, ['data', 'category_id'])\n",
    "        w.writeheader()\n",
    "        for i in range(len(train_dataset[\"data\"])):\n",
    "            w.writerow({\"data\":train_dataset[\"data\"][i], \"category_id\":train_dataset[\"target\"][i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a64ed",
   "metadata": {},
   "source": [
    "`sagemaker.Session()` object provides a set of utilizities to manage interaction with Sagemaker and AWS services in general. Let's use it to upload our data files in dedicated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "session = sagemaker.Session()\n",
    "train_dataset_uri=session.upload_data(\"train_dataset.csv\", key_prefix=\"newsgroups\")\n",
    "test_dataset_uri=session.upload_data(\"test_dataset.csv\", key_prefix=\"newsgroups\")\n",
    "\n",
    "print(f\"Datasets are available in following locations: {train_dataset_uri} and {test_dataset_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd5ba2",
   "metadata": {},
   "source": [
    "## Developing training script\n",
    "\n",
    "When running SageMaker training job we need to provide training script. Additionally, we may provide any other dependencies. We can also install or modify Python packages installed in pre-built containers via `requirements.txt` file.\n",
    "\n",
    "In this sample, we will use fairly new feature of HuggingFace framework to fine-tune multicategorical classifiers using Trainer API. Let's make sure that training container has newer HuggingFace Transformer library installed. For this, we create `requirements.txt` and specify minimal compatible version. We will provide this file to our SageMaker training job later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0e30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers >= 4.10\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 1_sources/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a11a97",
   "metadata": {},
   "source": [
    "Next, we need to actually develop training script. See it's content prepared script `1_sources/trian.py` below. Here are several highlights of this script:\n",
    "* At training time, SageMaker starts training by calling `user_training_script --arg1 value1 --arg2 value2 ...`. Here, arg1..N are hyperparameters provided by users as part of training job configuration. To correctly kick off training process in our script we need to include `a main guard` into our script (see line #100)\n",
    "- To correctly capture hyperparameters, training script need to be able to parse command line arguments. We use Python argpars library to do it (see code snippet #104-#112)\n",
    "* `train()` method is resposible for running end-to-end training job. It includes following components:\n",
    "    - calling `_get_tokenized_dataset` to load and tokenize dataset using pretrained DistilBERT tokenizer from HuggingFace library;\n",
    "    - loading and configuring DistilBERT model from HuggingFace model Zoo. Please note that we update default config for classification task to adjust for our chosen number of categories (line #80);\n",
    "    - configure HuggingFace Trainer and start training process (lines #86-#93);\n",
    "    - once training is done, we save trained model (line #97)\n",
    "    \n",
    "\n",
    "\n",
    "SageMaker Training toolkit setups up several environmental variables which comes handy when writing your training script:\n",
    "- `\"SM_CHANNEL_TRAIN\"` and `\"SM_CHANNEL_TEST\"` are locations where data files are download before training begins;\n",
    "- `\"SM_OUTPUT_DIR\"` is a directory for any output artifacts, SageMaker will upload this directory to S3 whether training job succeeds or failes;\n",
    "- `\"SM_MODEL_DIR\"`is a directory to store resulting model artifacts, SageMaker will also upload the model to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6bb2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "0002: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "0003: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "0004: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "0005: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "0006: \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "0007:     DistilBertTokenizerFast,\n",
      "0008:     DistilBertForSequenceClassification,\n",
      "0009:     DistilBertConfig,\n",
      "0010:     TrainingArguments,\n",
      "0011:     Trainer,\n",
      "0012: )\n",
      "0013: \n",
      "0014: MODEL_NAME = \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "0015: NUM_LABELS = \u001b[34m6\u001b[39;49;00m\n",
      "0016: \n",
      "0017: \n",
      "0018: \u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mCustomDataset\u001b[39;49;00m(torch.utils.data.Dataset):\n",
      "0019:     \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, encodings, labels):\n",
      "0020:         \u001b[36mself\u001b[39;49;00m.encodings = encodings\n",
      "0021:         \u001b[36mself\u001b[39;49;00m.labels = labels\n",
      "0022: \n",
      "0023:     \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\n",
      "0024:         item = {key: torch.tensor(val[idx]) \u001b[34mfor\u001b[39;49;00m key, val \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.encodings.items()}\n",
      "0025:         item[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = torch.tensor(\u001b[36mself\u001b[39;49;00m.labels[idx])\n",
      "0026:         \u001b[34mreturn\u001b[39;49;00m item\n",
      "0027: \n",
      "0028:     \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "0029:         \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\n",
      "0030: \n",
      "0031: \n",
      "0032: \u001b[34mdef\u001b[39;49;00m \u001b[32m_get_tokenized_data\u001b[39;49;00m():\n",
      "0033:     \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "0034: \u001b[33m    Read data from CSV files and tokenize datasets for training\u001b[39;49;00m\n",
      "0035: \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "0036:     train_dataset = pd.read_csv(\n",
      "0037:         os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_dataset.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[34m0\u001b[39;49;00m\n",
      "0038:     )\n",
      "0039:     test_dataset = pd.read_csv(\n",
      "0040:         os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mtest_dataset.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[34m0\u001b[39;49;00m\n",
      "0041:     )\n",
      "0042: \n",
      "0043:     tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
      "0044:     train_encodings = tokenizer(\n",
      "0045:         train_dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to_list(), truncation=\u001b[34mTrue\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m\n",
      "0046:     )\n",
      "0047:     test_encodings = tokenizer(\n",
      "0048:         test_dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to_list(), truncation=\u001b[34mTrue\u001b[39;49;00m, padding=\u001b[34mTrue\u001b[39;49;00m\n",
      "0049:     )\n",
      "0050: \n",
      "0051:     train_enc_dataset = CustomDataset(train_encodings, train_dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mcategory_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "0052:     test_enc_dataset = CustomDataset(test_encodings, test_dataset[\u001b[33m\"\u001b[39;49;00m\u001b[33mcategory_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "0053: \n",
      "0054:     \u001b[34mreturn\u001b[39;49;00m train_enc_dataset, test_enc_dataset\n",
      "0055: \n",
      "0056: \n",
      "0057: \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "0058:     \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "0059: \u001b[33m    Instantiate tokenizer, model config, and download pretrained model.\u001b[39;49;00m\n",
      "0060: \u001b[33m    After that run training using hyperparameters defined in SageMaker Training job config.\u001b[39;49;00m\n",
      "0061: \u001b[33m    If training is succesfull, save trained model.\u001b[39;49;00m\n",
      "0062: \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "0063: \n",
      "0064:     train_enc_dataset, test_enc_dataset = _get_tokenized_data()\n",
      "0065: \n",
      "0066:     training_args = TrainingArguments(\n",
      "0067:         output_dir=os.getenv(\n",
      "0068:             \u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m./\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "0069:         ),  \u001b[37m# output directory, if runtime is not\u001b[39;49;00m\n",
      "0070:         num_train_epochs=args.epochs,\n",
      "0071:         per_device_train_batch_size=args.per_device_train_batch_size,\n",
      "0072:         per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
      "0073:         warmup_steps=args.warmup_steps,\n",
      "0074:         weight_decay=args.weight_decay,\n",
      "0075:         logging_steps=args.logging_steps,\n",
      "0076:     )\n",
      "0077: \n",
      "0078:     \u001b[37m# Update config for multicategorical task (default is binary classification)\u001b[39;49;00m\n",
      "0079:     config = DistilBertConfig()\n",
      "0080:     config.num_labels = NUM_LABELS\n",
      "0081: \n",
      "0082:     model = DistilBertForSequenceClassification.from_pretrained(\n",
      "0083:         MODEL_NAME, config=config\n",
      "0084:     )\n",
      "0085: \n",
      "0086:     trainer = Trainer(\n",
      "0087:         model=model,  \u001b[37m# model to be trained\u001b[39;49;00m\n",
      "0088:         args=training_args,  \u001b[37m# training arguments, defined above\u001b[39;49;00m\n",
      "0089:         train_dataset=train_enc_dataset,  \u001b[37m# training dataset\u001b[39;49;00m\n",
      "0090:         eval_dataset=test_enc_dataset,  \u001b[37m# evaluation dataset\u001b[39;49;00m\n",
      "0091:     )\n",
      "0092: \n",
      "0093:     trainer.train()\n",
      "0094: \n",
      "0095:     \u001b[37m# if training is successfuly completed, we save model to SM_MODEL_DIR directory\u001b[39;49;00m\n",
      "0096:     \u001b[37m# SageMaker will automatically upload any artifacts in this directory to S3\u001b[39;49;00m\n",
      "0097:     model.save_pretrained(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "0098: \n",
      "0099: \n",
      "0100: \u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "0101: \n",
      "0102:     \u001b[37m# SageMaker passes hyperparameters  as command-line arguments to the script\u001b[39;49;00m\n",
      "0103:     \u001b[37m# Parsing them below...\u001b[39;49;00m\n",
      "0104:     parser = argparse.ArgumentParser()\n",
      "0105:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m)\n",
      "0106:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per-device-train-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m)\n",
      "0107:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per-device-eval-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "0108:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup-steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n",
      "0109:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging-steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n",
      "0110:     parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight-decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m)\n",
      "0111: \n",
      "0112:     args, _ = parser.parse_known_args()\n",
      "0113: \n",
      "0114:     train(args)\n",
      "0115: \n"
     ]
    }
   ],
   "source": [
    "! pygmentize -O linenos=1 1_sources/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7ae9f",
   "metadata": {},
   "source": [
    "## Running training job\n",
    "\n",
    "Once we have training script and dependencies ready, we can proceed and schedule training job via SageMaker Python SDK.\n",
    "\n",
    "We start with import of HuggingFace Estimator object and getting IAM execution role for our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85731552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role=get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b76ebb",
   "metadata": {},
   "source": [
    "Next, we need to define our hyperparameters of our model and training process. These variables will be passed to our script at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf7cd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\":1,\n",
    "    # 2 params below may need to updated if non-GPU instances is used for training\n",
    "    \"per-device-train-batch-size\":16, \n",
    "    \"per-device-eval-batch-size\":64,\n",
    "    \"warmup-steps\":100,\n",
    "    \"logging-steps\":100,\n",
    "    \"weight-decay\":0.01    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    py_version=\"py36\",\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"1_sources\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    transformers_version=\"4.6.1\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\":train_dataset_uri,\n",
    "    \"test\":test_dataset_uri\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1672b4",
   "metadata": {},
   "source": [
    "## Developing Inference Code\n",
    "\n",
    "Now that we have trained model, let's deploy it as SageMaker real-time endpoint. Similar to training job, we will use SageMaker pre-build HuggingFace container and will only provide our inference script. The inference requests will be handled by [Multi-Model Server](https://github.com/awslabs/multi-model-server) which exposes HTTP endpoint. \n",
    "\n",
    "When using pre-build inference containers, SageMaker automatically recognizes our inference script. According to SageMaker convention, inference script has to contain following methods:\n",
    "- `model_fn(model_dir)` (lines #16-#45) is executed at container start time to load model in the memory. This method takes model directory as an input argument. You can use `model_fn()` to initiatilize other components of your inference pipeline, such as tokenizer in our case. Note, that HuggingFace Transformers has a convenient Pipeline API which allows to combine data pre-processing (in our case, text tokenization) and actual inference in a single object. Hence, instead of loaded model, we return inference pipeline (line #45).\n",
    "- `transform_fn(inference_pipeline, data, content_type, accept_type)` is responsible for running actual inference (line #). Since we are communicating with end-client via HTTP, we also need to do payload deserialization and response serialization. In our sample example we expect JSON payload and return back JSON payload, however, this can be extended to any other formats based on the requirements (e.g. CSV, Protobuf).\n",
    "\n",
    "\n",
    "Sometimes combining deserialization, inference, and serialization in a single method can be inconvenient. Alternatively, SageMaker supports more granular API:\n",
    "- `input_fn(request_body, request_content_type)` runs deserialization.\n",
    "- `predict_fn(deser_input, model)` performs predictions.\n",
    "- `output_fn(prediction, response_content_type)` run serialization of predictions.\n",
    "\n",
    "Note, that `transform_fn()` and `input_fn(); predict_fn(); output_fn()` are mutually exclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d6d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001: \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "0002:     pipeline,\n",
      "0003:     DistilBertTokenizerFast,\n",
      "0004:     DistilBertConfig,\n",
      "0005:     DistilBertForSequenceClassification,\n",
      "0006: )\n",
      "0007: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "0008: \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "0009: \n",
      "0010: \n",
      "0011: MODEL_NAME = \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "0012: NUM_LABELS = \u001b[34m6\u001b[39;49;00m \u001b[37m# number of categories\u001b[39;49;00m\n",
      "0013: MAX_LENGTH = \u001b[34m512\u001b[39;49;00m \u001b[37m# max number of tokens model can handle\u001b[39;49;00m\n",
      "0014: \n",
      "0015: \n",
      "0016: \u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "0017:     \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "0018: \u001b[33m    Load required components (model, config and tokenizer) to constuct inference pipeline.\u001b[39;49;00m\n",
      "0019: \u001b[33m\u001b[39;49;00m\n",
      "0020: \u001b[33m    This method is executed only once when SageMaker starts model server.\u001b[39;49;00m\n",
      "0021: \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "0022: \n",
      "0023:     \u001b[37m# If CUDA device is present, then use it for inference\u001b[39;49;00m\n",
      "0024:     \u001b[37m# otherwise fallback to CPU\u001b[39;49;00m\n",
      "0025:     device_id = \u001b[34m0\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m -\u001b[34m1\u001b[39;49;00m\n",
      "0026: \n",
      "0027:     tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
      "0028:     config = DistilBertConfig()\n",
      "0029:     config.num_labels = NUM_LABELS\n",
      "0030: \n",
      "0031:     model = DistilBertForSequenceClassification.from_pretrained(\n",
      "0032:         model_dir, config=config\n",
      "0033:     )\n",
      "0034: \n",
      "0035:     inference_pipeline = pipeline(\n",
      "0036:         model=model,\n",
      "0037:         task=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext-classification\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "0038:         tokenizer=tokenizer,\n",
      "0039:         framework=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "0040:         device=device_id,\n",
      "0041:         max_length=MAX_LENGTH,\n",
      "0042:         truncation=\u001b[34mTrue\u001b[39;49;00m\n",
      "0043:     )\n",
      "0044: \n",
      "0045:     \u001b[34mreturn\u001b[39;49;00m inference_pipeline\n",
      "0046: \n",
      "0047: \n",
      "0048: \u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_fn\u001b[39;49;00m(inference_pipeline, data, content_type, accept_type):\n",
      "0049:     \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "0050: \u001b[33m    Deserialize inference request payload, run inferenece, and return back serialized response.\u001b[39;49;00m\n",
      "0051: \u001b[33m    Note, that currently only JSON is supported, however, this can be extended further as needed.\u001b[39;49;00m\n",
      "0052: \u001b[33m\u001b[39;49;00m\n",
      "0053: \u001b[33m    This method is executed on every request to SageMaker endpoint.\u001b[39;49;00m\n",
      "0054: \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "0055: \n",
      "0056:     \u001b[37m# Deserialize payload\u001b[39;49;00m\n",
      "0057:     \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m content_type:\n",
      "0058:         deser_data = json.loads(data)\n",
      "0059:     \u001b[34melse\u001b[39;49;00m:\n",
      "0060:         \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplemented\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mOnly \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m content type is supported.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "0061:     \n",
      "0062:     \u001b[37m# Run inference\u001b[39;49;00m\n",
      "0063:     predictions = inference_pipeline(deser_data)\n",
      "0064:     \n",
      "0065:     \u001b[37m# Serialize response\u001b[39;49;00m\n",
      "0066:     \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m accept_type:\n",
      "0067:         \u001b[34mreturn\u001b[39;49;00m json.dumps(predictions)\n",
      "0068:     \u001b[34melse\u001b[39;49;00m:\n",
      "0069:         \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplemented\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mOnly \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m accept type is supported.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "0070: \n"
     ]
    }
   ],
   "source": [
    "! pygmentize -O linenos=1 1_sources/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b95497",
   "metadata": {},
   "source": [
    "## Deploying Inference Endpoint\n",
    "\n",
    "Now we are ready to deploy and test our Newsgroup Classification endpoint. We can use method `estimator.create_model()` to configure our model deployment parameters, specifically:\n",
    "- define inference script and other dependencies which will be uploaded by SageMaker to endpoint;\n",
    "- identify inference container. If you provide `transformers_version`, `pytorch_version` and `py_version` parameters, SageMaker will automatically find appropriate pre-built inference container (if it exists). Alternatively, you can provide `image_uri` to directly specify container image you wish to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f6bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFaceModel\n",
    "\n",
    "model = estimator.create_model(role=role, \n",
    "                               entry_point=\"inference.py\", \n",
    "                               source_dir=\"1_sources\",\n",
    "                               py_version=\"py36\",\n",
    "                               transformers_version=\"4.6.1\",\n",
    "                               pytorch_version=\"1.7.1\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f651d1",
   "metadata": {},
   "source": [
    "Next, we define parameters of our endpoint such as number and type of instances behind it. Remember, SageMaker supports horizontal scaling of your inference endpoints! `model.deploy()` method starts inference deployment (which usually takes several minutes) and returns `Predictor` object to run inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bde344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4f32f",
   "metadata": {},
   "source": [
    "Now that endpoint is deployed, let's test it out! Note that we don't expect stellar performance, since model is likely undertrained because we only trained for single epoch to shorten training cycle. However, we expect that model will get most predictions right. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e61025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    sample_id = random.randint(0, len(test_dataset['data']))\n",
    "    prediction = predictor.predict([test_dataset['data'][sample_id]])\n",
    "    print(f\"Sample index: {sample_id}; predicted newsgroup: {prediction[0]['label']}; actual newsgroup: {test_dataset['target'][sample_id]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972410e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to train and deploy custom HuggingFace model using **SageMaker Script mode**. Script mode provide a lot of flexibility for developers when it comes to development of training and inference scripts (as long as it's inline SageMaker conventions which we discussed in this notebook). You can also modify container runtime via `requirements.txt` if you need to install additional Python packages or upload custom code dependencies. \n",
    "\n",
    "However, there are scenarios when you need to have more control over your runtime environments. SageMaker allows you to extend pre-built containers or BYO containers. In next notebooks of this chapter we will learn when you need to consider modifying your containers and how to do it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
