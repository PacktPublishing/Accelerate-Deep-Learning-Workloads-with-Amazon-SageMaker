{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25f0a21",
   "metadata": {},
   "source": [
    "# Develop training and inference scripts for Script Mode\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will learn how to develop training and inference scripts using HuggingFace framework. We will leverage SageMaker pre-build containers for HuggingFace (with PyTorch backend).\n",
    "\n",
    "We chose to solve a typical NLP task - text classification. We will use `20 Newsgroups` dataset which assembles ~ 20,000 newsgroup documents across 20 different newsgroups.\n",
    "\n",
    "By the end of this notebook you will learn how to:\n",
    "- prepare text corpus for training and inference using Amazon SageMaker;\n",
    "- develop training script to run in pre-build HugginFace container;\n",
    "- configure and schedule training job;\n",
    "- develop inference code;\n",
    "- configure and deploy real-time inference endpoint;\n",
    "- test SageMaker endpoint.\n",
    "\n",
    "## Preparing Dataset\n",
    "First, we will download dataset using `sklearn` module facility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91f7d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3308\n",
      "Number of test samples: 2203\n",
      "=========================\n",
      "Sample news article: From: sjp@hpuerca.atl.hp.com (Steve Phillips)\n",
      "Subject: Re: SUPER MEGA AUTOMOBILE SIGHTING(s)!!!!! Exotics together!\n",
      "Organization: Hewlett-Packard NARC Atlanta\n",
      "X-Newsreader: Tin 1.1.3 PL5\n",
      "Lines: 8\n",
      "\n",
      "Give out the address, I'll drive by and take a look myself, then post.\n",
      "\n",
      "\n",
      "--\n",
      "Stephen Phillips\n",
      "Atlanta Response Center\n",
      "Atlanta, Ga.\n",
      "Home of the Braves!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We select 6 out of 20 diverse newsgroups\n",
    "categories = [\n",
    "    \"comp.windows.x\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.electronics\",\n",
    "    \"misc.forsale\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"alt.atheism\"\n",
    "]\n",
    "\n",
    "train_dataset = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "test_dataset = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset['data'])}\")\n",
    "print(f\"Number of test samples: {len(test_dataset['data'])}\")\n",
    "\n",
    "print(\"=========================\")\n",
    "print(f\"Sample news article: {train_dataset['data'][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568fb76",
   "metadata": {},
   "source": [
    "Now, we need to save selected datasets into file and upload resulting files to Amazon S3 storage. SageMaker will download them to training container at training time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95ca81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "dump(train_dataset, open(\"train.pkl\",\"wb\"))\n",
    "dump(test_dataset, open(\"train.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f2655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sjp@hpuerca.atl.hp.com (Steve Phillips)\n",
      "Subject: Re: SUPER MEGA AUTOMOBILE SIGHTING(s)!!!!! Exotics together!\n",
      "Organization: Hewlett-Packard NARC Atlanta\n",
      "X-Newsreader: Tin 1.1.3 PL5\n",
      "Lines: 8\n",
      "\n",
      "Give out the address, I'll drive by and take a look myself, then post.\n",
      "\n",
      "\n",
      "--\n",
      "Stephen Phillips\n",
      "Atlanta Response Center\n",
      "Atlanta, Ga.\n",
      "Home of the Braves!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "re_train=pickle.load(open(\"train.pkl\",\"rb\"))\n",
    "print(re_train.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4168d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 81.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (4.6.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 62.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 75.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers) (4.62.0)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 368 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 77.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 81.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets) (1.3.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tqdm, fsspec, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.0\n",
      "    Uninstalling tqdm-4.62.0:\n",
      "      Successfully uninstalled tqdm-4.62.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.4.0\n",
      "    Uninstalling fsspec-2021.4.0:\n",
      "      Successfully uninstalled fsspec-2021.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2021.4.0 requires fsspec==2021.04.0, but you have fsspec 2021.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed datasets-1.12.1 fsspec-2021.9.0 huggingface-hub-0.0.17 sacremoses-0.0.46 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.10.3 xxhash-2.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"newsgroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018982e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p37",
   "language": "python",
   "name": "conda_pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
