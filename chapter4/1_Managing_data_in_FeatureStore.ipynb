{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# region = boto3.session.Session().region_name\n",
    "role = \"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\"\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# sagemaker_local_session = LocalSession()\n",
    "s3_bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# sklearn_processor = SKLearnProcessor(\n",
    "#     framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "current_time_sec = int(round(time.time()))\n",
    "dataset_df[\"EventTime\"] = pd.Series([current_time_sec]*len(dataset_df), dtype=\"float64\")\n",
    "dataset_df[\"ID\"] = dataset_df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"text\"] = dataset_df[\"text\"].astype('string')\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].str.encode(\"utf8\")\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_df['text'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove in future\n",
    "tok_dataset = tokenizer(dataset_df[\"text\"].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"tokenized-text\"] = tokenizer(dataset_df[\"text\"].tolist(), truncation=True, padding=True)[\"input_ids\"]\n",
    "# dataset_df[\"tokenized-text\"] = tokenizer(dataset_df[\"text\"].tolist(), truncation=True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"tokenized-text\"] = dataset_df[\"tokenized-text\"].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "imdb_feature_group_name = \"imdb-reviews-tokenized-4\"\n",
    "\n",
    "imdb_feature_group = FeatureGroup(\n",
    "    name=imdb_feature_group_name, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_feature_group.load_feature_definitions(data_frame=dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_feature_group.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{imdb_feature_group_name}\",\n",
    "    record_identifier_name=\"ID\",\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# Waiter for FeatureGroup creation\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        print(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    print(f'FeatureGroup {feature_group.name} was successfully created.')\n",
    "\n",
    "wait_for_feature_group_creation_complete(imdb_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# To disable Tokenizer warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "imdb_feature_group.ingest(data_frame=dataset_df, max_processes=16, wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take several minutes to populate the tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting labels in dataset: \n",
      "    label  Count\n",
      "0      0  12500\n",
      "1      1  12500\n"
     ]
    }
   ],
   "source": [
    "athena_query = imdb_feature_group.athena_query()\n",
    "imdb_table_name = athena_query.table_name\n",
    "# (catalog=\"AwsDataCatalog\", database=\"sagemaker_featurestore\", table_name=imdb_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "result = athena_query.run(f'SELECT \"label\", COUNT(\"label\") as \"Count\" FROM \"sagemaker_featurestore\".\"{imdb_table_name}\" group by \"label\";', output_location=f\"s3://{s3_bucket_name}/athena_output\")\n",
    "athena_query.wait()\n",
    "print(f\"Counting labels in dataset: \\n {athena_query.as_dataframe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting labels in dataset: \n",
      "                                                 text  label     eventtime  \\\n",
      "0  b'b\\'This is one of my favorites along with th...      1  1.636321e+09   \n",
      "1  b'b\"Enchanted April is a tone poem, an impress...      1  1.636321e+09   \n",
      "2  b'b\\'Have never understood why the MacDonald-E...      1  1.636321e+09   \n",
      "3  b'b\"I\\'m going to say first off that I have gi...      0  1.636321e+09   \n",
      "4  b'b\"I am a big fan of Stephen King. I loved Th...      0  1.636321e+09   \n",
      "5  b'b\"Philo Vance (William Powell) helps solve m...      1  1.636321e+09   \n",
      "6  b'b\\'The first half of this movie is a pure de...      0  1.636321e+09   \n",
      "7  b'b\"This was filmed back-to-back with the 1992...      0  1.636321e+09   \n",
      "8  b'b\\'I read a small ad in some horror magazine...      1  1.636321e+09   \n",
      "9  b'b\"A disappointing film.<br /><br />The story...      0  1.636321e+09   \n",
      "\n",
      "      id                                     tokenized-text  \\\n",
      "0   7819  [101, 1038, 1005, 1038, 1032, 1005, 2023, 2003...   \n",
      "1   6254  [101, 1038, 1005, 1038, 1000, 22454, 2258, 200...   \n",
      "2  10951  [101, 1038, 1005, 1038, 1032, 1005, 2031, 2196...   \n",
      "3  18762  [101, 1038, 1005, 1038, 1000, 1045, 1032, 1005...   \n",
      "4  15647  [101, 1038, 1005, 1038, 1000, 1045, 2572, 1037...   \n",
      "5   4727  [101, 1038, 1005, 1038, 1000, 6316, 2080, 1667...   \n",
      "6  14092  [101, 1038, 1005, 1038, 1032, 1005, 1996, 2034...   \n",
      "7  17217  [101, 1038, 1005, 1038, 1000, 2023, 2001, 6361...   \n",
      "8  10975  [101, 1038, 1005, 1038, 1032, 1005, 1045, 3191...   \n",
      "9  23465  [101, 1038, 1005, 1038, 1000, 1037, 15640, 214...   \n",
      "\n",
      "                write_time      api_invocation_time  is_deleted  \n",
      "0  2021-11-08 17:06:56.940  2021-11-08 17:01:27.000       False  \n",
      "1  2021-11-08 17:06:56.940  2021-11-08 17:01:27.000       False  \n",
      "2  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "3  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "4  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "5  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "6  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "7  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "8  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n",
      "9  2021-11-08 17:06:56.940  2021-11-08 17:01:28.000       False  \n"
     ]
    }
   ],
   "source": [
    "athena_query = imdb_feature_group.athena_query()\n",
    "imdb_table_name = athena_query.table_name\n",
    "# (catalog=\"AwsDataCatalog\", database=\"sagemaker_featurestore\", table_name=imdb_feature_group_name, sagemaker_session=sagemaker_session)\n",
    "result = athena_query.run(f'SELECT * FROM \"sagemaker_featurestore\".\"{imdb_table_name}\" limit 10;', output_location=f\"s3://{s3_bucket_name}/athena_output\")\n",
    "athena_query.wait()\n",
    "print(f\"Counting labels in dataset: \\n {athena_query.as_dataframe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_uri = imdb_feature_group.describe()['OfflineStoreConfig'][\"S3StorageConfig\"][\"ResolvedOutputS3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "local_data_dir = \"./fs_data/\"\n",
    "\n",
    "df = pd.read_parquet(local_data_dir)\n",
    "df[\"text\"] = df[\"text\"].astype('string')\n",
    "df[\"input_ids\"] = df[\"tokenized-text\"].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_dict({\"input_ids\" : df[\"tokenized-text\"].tolist()})\n",
    "\n",
    "# dataset = Dataset.from_dict({\"input_ids\" : df[\"tokenized-text\"].tolist()})\n",
    "dataset = Dataset.from_pandas(df[[\"input_ids\", \"label\"]])\n",
    "# dataset = dataset.rename_column(\"tokenized-text\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:04<00:00, 5828.24ex/s]\n"
     ]
    }
   ],
   "source": [
    "def string_to_list(example):\n",
    "    list_of_str = example[\"input_ids\"].strip(\"][\").split(\", \")\n",
    "    example[\"input_ids\"] = [int(el) for el in list_of_str]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [101,\n",
       "  1038,\n",
       "  1005,\n",
       "  1038,\n",
       "  1000,\n",
       "  19957,\n",
       "  1998,\n",
       "  9414,\n",
       "  2135,\n",
       "  2209,\n",
       "  2011,\n",
       "  1996,\n",
       "  2048,\n",
       "  2402,\n",
       "  3057,\n",
       "  1010,\n",
       "  28616,\n",
       "  7507,\n",
       "  12975,\n",
       "  2004,\n",
       "  12784,\n",
       "  1010,\n",
       "  1998,\n",
       "  22093,\n",
       "  24471,\n",
       "  20755,\n",
       "  2004,\n",
       "  14015,\n",
       "  1010,\n",
       "  2348,\n",
       "  1996,\n",
       "  5436,\n",
       "  2003,\n",
       "  2738,\n",
       "  1037,\n",
       "  7683,\n",
       "  1997,\n",
       "  1996,\n",
       "  9647,\n",
       "  1012,\n",
       "  2402,\n",
       "  14015,\n",
       "  2770,\n",
       "  2005,\n",
       "  3664,\n",
       "  3849,\n",
       "  2041,\n",
       "  1997,\n",
       "  2173,\n",
       "  1010,\n",
       "  2000,\n",
       "  2022,\n",
       "  7481,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2096,\n",
       "  1996,\n",
       "  3772,\n",
       "  2003,\n",
       "  2092,\n",
       "  2589,\n",
       "  2011,\n",
       "  2035,\n",
       "  4986,\n",
       "  1996,\n",
       "  3185,\n",
       "  12102,\n",
       "  2000,\n",
       "  3768,\n",
       "  1037,\n",
       "  10218,\n",
       "  7224,\n",
       "  1997,\n",
       "  3689,\n",
       "  1012,\n",
       "  3383,\n",
       "  2057,\n",
       "  1032,\n",
       "  1005,\n",
       "  2310,\n",
       "  4961,\n",
       "  2000,\n",
       "  5987,\n",
       "  24842,\n",
       "  3723,\n",
       "  4507,\n",
       "  1999,\n",
       "  5691,\n",
       "  1010,\n",
       "  2738,\n",
       "  2066,\n",
       "  13599,\n",
       "  17543,\n",
       "  25789,\n",
       "  2000,\n",
       "  2129,\n",
       "  2665,\n",
       "  2001,\n",
       "  2026,\n",
       "  3028,\n",
       "  999,\n",
       "  2196,\n",
       "  2568,\n",
       "  1010,\n",
       "  2169,\n",
       "  1997,\n",
       "  2068,\n",
       "  2024,\n",
       "  2204,\n",
       "  1999,\n",
       "  2037,\n",
       "  2219,\n",
       "  2126,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1045,\n",
       "  2079,\n",
       "  19837,\n",
       "  7437,\n",
       "  20228,\n",
       "  5004,\n",
       "  15950,\n",
       "  2130,\n",
       "  2065,\n",
       "  2014,\n",
       "  2535,\n",
       "  2003,\n",
       "  5399,\n",
       "  20442,\n",
       "  2182,\n",
       "  1012,\n",
       "  2690,\n",
       "  1997,\n",
       "  1996,\n",
       "  2346,\n",
       "  4024,\n",
       "  2092,\n",
       "  10897,\n",
       "  2005,\n",
       "  3920,\n",
       "  7193,\n",
       "  1010,\n",
       "  1998,\n",
       "  2129,\n",
       "  3835,\n",
       "  2012,\n",
       "  2335,\n",
       "  2000,\n",
       "  2022,\n",
       "  6086,\n",
       "  2000,\n",
       "  2986,\n",
       "  4556,\n",
       "  2189,\n",
       "  2029,\n",
       "  2003,\n",
       "  2471,\n",
       "  1037,\n",
       "  10958,\n",
       "  15780,\n",
       "  999,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1045,\n",
       "  2424,\n",
       "  2023,\n",
       "  3185,\n",
       "  2000,\n",
       "  2022,\n",
       "  1037,\n",
       "  10979,\n",
       "  2689,\n",
       "  2004,\n",
       "  2009,\n",
       "  11138,\n",
       "  27486,\n",
       "  1010,\n",
       "  16465,\n",
       "  5300,\n",
       "  2005,\n",
       "  1996,\n",
       "  3652,\n",
       "  2039,\n",
       "  2086,\n",
       "  1010,\n",
       "  1998,\n",
       "  2053,\n",
       "  4808,\n",
       "  4067,\n",
       "  15003,\n",
       "  1012,\n",
       "  1037,\n",
       "  4010,\n",
       "  2155,\n",
       "  2143,\n",
       "  2000,\n",
       "  5959,\n",
       "  1012,\n",
       "  1000,\n",
       "  1005,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 2\n",
    "VOCAB_SIZE = 30522\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "config = DistilBertConfig()\n",
    "config.num_labels=NUM_LABELS\n",
    "config.vocab_size=VOCAB_SIZE\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./drqs_distilbert/output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_gpu_train_batch_size=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    max_steps=100,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=dataset,  # training dataset\n",
    "#     eval_dataset=test_enc_dataset,  # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "100%|██████████| 100/100 [04:54<00:00,  3.10s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [04:54<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 294.8451, 'train_samples_per_second': 0.339, 'train_steps_per_second': 0.339, 'train_loss': 0.7964998626708985, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.7964998626708985, metrics={'train_runtime': 294.8451, 'train_samples_per_second': 0.339, 'train_steps_per_second': 0.339, 'train_loss': 0.7964998626708985, 'epoch': 0.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-27 14:54:01 Starting - Starting the training job...\n",
      "2021-11-27 14:54:04 Starting - Launching requested ML instances......\n",
      "2021-11-27 14:55:23 Starting - Preparing the instances for training.........\n",
      "2021-11-27 14:56:48 Downloading - Downloading input data...\n",
      "2021-11-27 14:57:10 Training - Downloading the training image........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-11-27 15:01:20,741 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-11-27 15:01:20,767 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-11-27 15:01:23,802 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-11-27 15:01:24,518 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 16,\n",
      "        \"max_steps\": 100,\n",
      "        \"model_name\": \"distilbert-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-11-27-14-54-01-237\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/huggingface-pytorch-training-2021-11-27-14-54-01-237/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max_steps\":100,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/huggingface-pytorch-training-2021-11-27-14-54-01-237/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_steps\":100,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-11-27-14-54-01-237\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/huggingface-pytorch-training-2021-11-27-14-54-01-237/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max_steps\",\"100\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --max_steps 100 --model_name distilbert-base-uncased --train_batch_size 16\u001b[0m\n",
      "\u001b[34m{'input_ids': [101, 1038, 1005, 1038, 1000, 19957, 1998, 9414, 2135, 2209, 2011, 1996, 2048, 2402, 3057, 1010, 28616, 7507, 12975, 2004, 12784, 1010, 1998, 22093, 24471, 20755, 2004, 14015, 1010, 2348, 1996, 5436, 2003, 2738, 1037, 7683, 1997, 1996, 9647, 1012, 2402, 14015, 2770, 2005, 3664, 3849, 2041, 1997, 2173, 1010, 2000, 2022, 7481, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2096, 1996, 3772, 2003, 2092, 2589, 2011, 2035, 4986, 1996, 3185, 12102, 2000, 3768, 1037, 10218, 7224, 1997, 3689, 1012, 3383, 2057, 1032, 1005, 2310, 4961, 2000, 5987, 24842, 3723, 4507, 1999, 5691, 1010, 2738, 2066, 13599, 17543, 25789, 2000, 2129, 2665, 2001, 2026, 3028, 999, 2196, 2568, 1010, 2169, 1997, 2068, 2024, 2204, 1999, 2037, 2219, 2126, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 19837, 7437, 20228, 5004, 15950, 2130, 2065, 2014, 2535, 2003, 5399, 20442, 2182, 1012, 2690, 1997, 1996, 2346, 4024, 2092, 10897, 2005, 3920, 7193, 1010, 1998, 2129, 3835, 2012, 2335, 2000, 2022, 6086, 2000, 2986, 4556, 2189, 2029, 2003, 2471, 1037, 10958, 15780, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2424, 2023, 3185, 2000, 2022, 1037, 10979, 2689, 2004, 2009, 11138, 27486, 1010, 16465, 5300, 2005, 1996, 3652, 2039, 2086, 1010, 1998, 2053, 4808, 4067, 15003, 1012, 1037, 4010, 2155, 2143, 2000, 5959, 1012, 1000, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1}\u001b[0m\n",
      "\u001b[34mCompleted loading data\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-11-27 15:01:37,836 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmoh8r48n\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-11-27 15:01:43,984 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-11-27 15:01:43,984 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2021-11-27 15:01:43,985 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2021-11-27 15:01:44,756 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2021-11-27 15:01:44,756 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|training_args.py:710] 2021-11-27 15:01:44,757 >> PyTorch: setting up devices\u001b[0m\n",
      "\u001b[34m[INFO|training_args.py:616] 2021-11-27 15:01:45,169 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:398] 2021-11-27 15:01:48,803 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2021-11-27 15:01:48,951 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2021-11-27 15:01:48,952 >>   Num examples = 25000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2021-11-27 15:01:48,952 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2021-11-27 15:01:48,952 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2021-11-27 15:01:48,952 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2021-11-27 15:01:48,953 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2021-11-27 15:01:48,953 >>   Total optimization steps = 100\u001b[0m\n",
      "\n",
      "2021-11-27 15:02:12 Training - Training image download completed. Training in progress.\u001b[34m{'loss': 0.6915, 'learning_rate': 1e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1352] 2021-11-27 15:04:24,023 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 155.0707, 'train_samples_per_second': 0.645, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2021-11-27 15:04:24,700 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2021-11-27 15:04:25,606 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mINFO:__main__:Args: Namespace(epochs=3, eval_batch_size=64, learning_rate=5e-05, logging_steps=100, max_steps=100, model_dir='/opt/ml/model', model_name='distilbert-base-uncased', n_gpus='1', output_data_dir='/opt/ml/output/data', seed=42, train_batch_size=16, training_dir='/opt/ml/input/data/training', warmup_steps=500, weight_decay=0.01)\u001b[0m\n",
      "\u001b[34mINFO:__main__:remainder: []\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/25000 [00:00<?, ?ex/s]#015  2%|▏         | 405/25000 [00:00<00:06, 4048.53ex/s]#015  3%|▎         | 796/25000 [00:00<00:06, 4003.22ex/s]#015  4%|▍         | 1000/25000 [00:00<00:07, 3033.38ex/s]#015  6%|▌         | 1402/25000 [00:00<00:07, 3272.87ex/s]#015  7%|▋         | 1733/25000 [00:00<00:07, 3281.39ex/s]#015  8%|▊         | 2015/25000 [00:00<00:07, 3042.94ex/s]#015 10%|▉         | 2425/25000 [00:00<00:06, 3294.71ex/s]#015 11%|█▏        | 2831/25000 [00:00<00:06, 3490.19ex/s]#015 13%|█▎        | 3173/25000 [00:00<00:06, 3262.48ex/s]#015 14%|█▍        | 3584/25000 [00:01<00:06, 3476.80ex/s]#015 16%|█▌        | 3996/25000 [00:01<00:05, 3646.83ex/s]#015 17%|█▋        | 4365/25000 [00:01<00:05, 3466.06ex/s]#015 19%|█▉        | 4763/25000 [00:01<00:05, 3605.72ex/s]#015 21%|██        | 5129/25000 [00:01<00:06, 3171.25ex/s]#015 22%|██▏       | 5511/25000 [00:01<00:05, 3340.44ex/s]#015 24%|██▎       | 5916/25000 [00:01<00:05, 3524.55ex/s]#015 25%|██▌       | 6280/25000 [00:01<00:05, 3381.84ex/s]#015 27%|██▋       | 6690/25000 [00:01<00:05, 3568.11ex/s]#015 28%|██▊       | 7056/25000 [00:02<00:06, 2835.69ex/s]#015 30%|██▉       | 7474/25000 [00:02<00:05, 3136.85ex/s]#015 32%|███▏      | 7881/25000 [00:02<00:05, 3367.60ex/s]#015 33%|███▎      | 8245/25000 [00:02<00:05, 3251.54ex/s]#015 34%|███▍      | 8595/25000 [00:02<00:04, 3320.97ex/s]#015 36%|███▌      | 9000/25000 [00:02<00:05, 3050.63ex/s]#015 38%|███▊      | 9425/25000 [00:02<00:04, 3315.51ex/s]#015 39%|███▉      | 9834/25000 [00:02<00:04, 3514.38ex/s]#015 41%|████      | 10201/25000 [00:03<00:04, 3357.07ex/s]#015 42%|████▏     | 10604/25000 [00:03<00:04, 3532.42ex/s]#015 44%|████▍     | 11000/25000 [00:03<00:04, 3381.16ex/s]#015 46%|████▌     | 11425/25000 [00:03<00:03, 3587.42ex/s]#015 47%|████▋     | 11817/25000 [00:03<00:03, 3680.64ex/s]#015 49%|████▉     | 12193/25000 [00:03<00:03, 3394.59ex/s]#015 50%|█████     | 12605/25000 [00:03<00:03, 3582.16ex/s]#015 52%|█████▏    | 13000/25000 [00:03<00:03, 3374.23ex/s]#015 54%|█████▎    | 13424/25000 [00:03<00:03, 3592.74ex/s]#015 55%|█████▌    | 13820/25000 [00:04<00:03, 3693.66ex/s]#015 57%|█████▋    | 14198/25000 [00:04<00:03, 3511.10ex/s]#015 58%|█████▊    | 14613/25000 [00:04<00:02, 3679.76ex/s]#015 60%|██████    | 15000/25000 [00:04<00:02, 3446.26ex/s]#015 62%|██████▏   | 15425/25000 [00:04<00:02, 3639.38ex/s]#015 63%|██████▎   | 15798/25000 [00:04<00:02, 3607.70ex/s]#015 65%|██████▍   | 16165/25000 [00:04<00:02, 3404.85ex/s]#015 66%|██████▋   | 16584/25000 [00:04<00:02, 3607.29ex/s]#015 68%|██████▊   | 16992/25000 [00:04<00:02, 3735.90ex/s]#015 69%|██████▉   | 17373/25000 [00:05<00:02, 3506.94ex/s]#015 71%|███████   | 17777/25000 [00:05<00:01, 3651.11ex/s]#015 73%|███████▎  | 18149/25000 [00:05<00:01, 3453.32ex/s]#015 74%|███████▍  | 18560/25000 [00:05<00:01, 3625.90ex/s]#015 76%|███████▌  | 18962/25000 [00:05<00:01, 3733.55ex/s]#015 77%|███████▋  | 19342/25000 [00:05<00:01, 3451.13ex/s]#015 79%|███████▉  | 19734/25000 [00:05<00:01, 3579.39ex/s]#015 80%|████████  | 20099/25000 [00:05<00:01, 3389.37ex/s]#015 82%|████████▏ | 20515/25000 [00:05<00:01, 3586.87ex/s]#015 84%|████████▎ | 20921/25000 [00:05<00:01, 3715.24ex/s]#015 85%|████████▌ | 21300/25000 [00:06<00:01, 3471.53ex/s]#015 87%|████████▋ | 21706/25000 [00:06<00:00, 3627.41ex/s]#015 88%|████████▊ | 22077/25000 [00:06<00:00, 3399.02ex/s]#015 90%|████████▉ | 22497/25000 [00:06<00:00, 3603.79ex/s]#015 92%|█████████▏| 22888/25000 [00:06<00:00, 3690.40ex/s]#015 93%|█████████▎| 23264/25000 [00:06<00:00, 3366.13ex/s]#015 95%|█████████▍| 23664/25000 [00:06<00:00, 3532.30ex/s]#015 96%|█████████▌| 24027/25000 [00:06<00:00, 3347.88ex/s]#015 98%|█████████▊| 24440/25000 [00:06<00:00, 3548.93ex/s]#015 99%|█████████▉| 24836/25000 [00:07<00:00, 3662.48ex/s]#015100%|██████████| 25000/25000 [00:07<00:00, 3487.92ex/s]\u001b[0m\n",
      "\u001b[34mINFO:datasets.arrow_writer:Done writing 25000 examples in 102700100 bytes .\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 169.254.170.2:80\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:http://169.254.170.2:80 \"GET /v3/c3c6f332-63b4-4cc5-967c-c53cd6514f36 HTTP/1.1\" 200 1950\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\u001b[0m\n",
      "\u001b[34mDEBUG:filelock:Attempting to acquire lock 140680501837720 on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 140680501837720 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1532] 2021-11-27 15:01:37,836 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmoh8r48n\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://cdn-lfs.huggingface.co:443 \"GET /distilbert-base-uncased/e60d71610916da4787c5513c81bc026d415708528295502fb3e1a6fe1485ea7c HTTP/1.1\" 200 267967963\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.77M/268M [00:00<00:05, 47.7MB/s]#015Downloading:   4%|▎         | 9.42M/268M [00:00<00:05, 47.3MB/s]#015Downloading:   5%|▌         | 14.3M/268M [00:00<00:05, 47.8MB/s]#015Downloading:   7%|▋         | 19.4M/268M [00:00<00:05, 48.7MB/s]#015Downloading:   9%|▉         | 24.5M/268M [00:00<00:04, 49.4MB/s]#015Downloading:  11%|█         | 29.7M/268M [00:00<00:04, 50.1MB/s]#015Downloading:  13%|█▎        | 34.9M/268M [00:00<00:04, 50.5MB/s]#015Downloading:  15%|█▍        | 40.1M/268M [00:00<00:04, 50.9MB/s]#015Downloading:  17%|█▋        | 45.2M/268M [00:00<00:04, 51.2MB/s]#015Downloading:  19%|█▉        | 50.4M/268M [00:01<00:04, 51.4MB/s]#015Downloading:  21%|██        | 55.7M/268M [00:01<00:04, 51.7MB/s]#015Downloading:  23%|██▎       | 60.9M/268M [00:01<00:03, 52.0MB/s]#015Downloading:  25%|██▍       | 66.1M/268M [00:01<00:03, 50.9MB/s]#015Downloading:  27%|██▋       | 71.2M/268M [00:01<00:03, 51.1MB/s]#015Downloading:  28%|██▊       | 76.4M/268M [00:01<00:03, 51.2MB/s]#015Downloading:  30%|███       | 81.5M/268M [00:01<00:03, 47.3MB/s]#015Downloading:  32%|███▏      | 86.3M/268M [00:01<00:03, 47.5MB/s]#015Downloading:  34%|███▍      | 91.6M/268M [00:01<00:03, 49.1MB/s]#015Downloading:  36%|███▌      | 96.8M/268M [00:01<00:03, 50.0MB/s]#015Downloading:  38%|███▊      | 102M/268M [00:02<00:04, 35.2MB/s] #015Downloading:  40%|███▉      | 106M/268M [00:02<00:04, 37.9MB/s]#015Downloading:  41%|████▏     | 111M/268M [00:02<00:04, 36.3MB/s]#015Downloading:  43%|████▎     | 115M/268M [00:02<00:04, 37.8MB/s]#015Downloading:  45%|████▍     | 119M/268M [00:02<00:03, 39.4MB/s]#015Downloading:  46%|████▋     | 124M/268M [00:02<00:03, 42.2MB/s]#015Downloading:  48%|████▊     | 130M/268M [00:02<00:03, 45.4MB/s]#015Downloading:  50%|█████     | 135M/268M [00:02<00:02, 47.7MB/s]#015Downloading:  52%|█████▏    | 141M/268M [00:03<00:02, 49.4MB/s]#015Downloading:  55%|█████▍    | 146M/268M [00:03<00:02, 50.9MB/s]#015Downloading:  57%|█████▋    | 152M/268M [00:03<00:02, 52.1MB/s]#015Downloading:  59%|█████▊    | 157M/268M [00:03<00:02, 51.1MB/s]#015Downloading:  61%|██████    | 162M/268M [00:03<00:02, 51.2MB/s]#015Downloading:  63%|██████▎   | 168M/268M [00:03<00:02, 49.8MB/s]#015Downloading:  65%|██████▍   | 173M/268M [00:03<00:01, 51.1MB/s]#015Downloading:  67%|██████▋   | 178M/268M [00:03<00:02, 34.0MB/s]#015Downloading:  68%|██████▊   | 184M/268M [00:04<00:02, 37.9MB/s]#015Downloading:  70%|███████   | 188M/268M [00:04<00:01, 40.6MB/s]#015Downloading:  72%|███████▏  | 193M/268M [00:04<00:02, 35.9MB/s]#015Downloading:  74%|███████▍  | 198M/268M [00:04<00:01, 38.8MB/s]#015Downloading:  76%|███████▌  | 203M/268M [00:04<00:01, 42.0MB/s]#015Downloading:  77%|███████▋  | 208M/268M [00:04<00:01, 40.6MB/s]#015Downloading:  79%|███████▉  | 212M/268M [00:04<00:01, 34.4MB/s]#015Downloading:  81%|████████  | 217M/268M [00:04<00:01, 38.2MB/s]#015Downloading:  83%|████████▎ | 222M/268M [00:04<00:01, 41.1MB/s]#015Downloading:  85%|████████▍ | 226M/268M [00:05<00:01, 30.7MB/s]#015Downloading:  86%|████████▋ | 231M/268M [00:05<00:01, 34.3MB/s]#015Downloading:  88%|████████▊ | 236M/268M [00:05<00:00, 37.5MB/s]#015Downloading:  90%|████████▉ | 241M/268M [00:05<00:00, 40.3MB/s]#015Downloading:  92%|█████████▏| 246M/268M [00:05<00:00, 42.4MB/s]#015Downloading:  94%|█████████▎| 251M/268M [00:05<00:00, 44.4MB/s]#015Downloading:  95%|█████████▌| 256M/268M [00:05<00:00, 45.5MB/s]#015Downloading:  97%|█████████▋| 261M/268M [00:05<00:00, 48.5MB/s]#015Downloading: 100%|█████████▉| 267M/268M [00:05<00:00, 50.1MB/s]#015Downloading: 100%|██████████| 268M/268M [00:06<00:00, 44.6MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1536] 2021-11-27 15:01:43,984 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1544] 2021-11-27 15:01:43,984 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34mDEBUG:filelock:Attempting to release lock 140680501837720 on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 140680501837720 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1155] 2021-11-27 15:01:43,985 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1331] 2021-11-27 15:01:44,756 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1342] 2021-11-27 15:01:44,756 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[INFO|training_args.py:710] 2021-11-27 15:01:44,757 >> PyTorch: setting up devices\u001b[0m\n",
      "\u001b[34m[INFO|training_args.py:616] 2021-11-27 15:01:45,169 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:398] 2021-11-27 15:01:48,803 >> max_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1156] 2021-11-27 15:01:48,951 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1157] 2021-11-27 15:01:48,952 >>   Num examples = 25000\u001b[0m\n",
      "\u001b[34m2021-11-27 15:04:26,480 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1158] 2021-11-27 15:01:48,952 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1159] 2021-11-27 15:01:48,952 >>   Instantaneous batch size per device = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1160] 2021-11-27 15:01:48,952 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1161] 2021-11-27 15:01:48,953 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1162] 2021-11-27 15:01:48,953 >>   Total optimization steps = 100\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/100 [00:00<?, ?it/s]#015  1%|          | 1/100 [00:02<04:40,  2.83s/it]#015  2%|▏         | 2/100 [00:04<03:58,  2.43s/it]#015  3%|▎         | 3/100 [00:05<03:29,  2.16s/it]#015  4%|▍         | 4/100 [00:07<03:08,  1.96s/it]#015  5%|▌         | 5/100 [00:08<02:53,  1.83s/it]#015  6%|▌         | 6/100 [00:10<02:43,  1.74s/it]#015  7%|▋         | 7/100 [00:11<02:36,  1.68s/it]#015  8%|▊         | 8/100 [00:13<02:30,  1.63s/it]#015  9%|▉         | 9/100 [00:15<02:26,  1.61s/it]#015 10%|█         | 10/100 [00:16<02:21,  1.58s/it]#015 11%|█         | 11/100 [00:18<02:19,  1.56s/it]#015 12%|█▏        | 12/100 [00:19<02:16,  1.55s/it]#015 13%|█▎        | 13/100 [00:21<02:14,  1.55s/it]#015 14%|█▍        | 14/100 [00:22<02:12,  1.54s/it]#015 15%|█▌        | 15/100 [00:24<02:12,  1.55s/it]#015 16%|█▌        | 16/100 [00:25<02:10,  1.55s/it]#015 17%|█▋        | 17/100 [00:27<02:08,  1.55s/it]#015 18%|█▊        | 18/100 [00:28<02:07,  1.55s/it]#015 19%|█▉        | 19/100 [00:30<02:04,  1.54s/it]#015 20%|██        | 20/100 [00:31<02:02,  1.53s/it]#015 21%|██        | 21/100 [00:33<02:00,  1.53s/it]#015 22%|██▏       | 22/100 [00:34<01:59,  1.53s/it]#015 23%|██▎       | 23/100 [00:36<01:58,  1.54s/it]#015 24%|██▍       | 24/100 [00:38<01:56,  1.53s/it]#015 25%|██▌       | 25/100 [00:39<01:54,  1.53s/it]#015 26%|██▌       | 26/100 [00:41<01:53,  1.53s/it]#015 27%|██▋       | 27/100 [00:42<01:52,  1.54s/it]#015 28%|██▊       | 28/100 [00:44<01:50,  1.53s/it]#015 29%|██▉       | 29/100 [00:45<01:48,  1.53s/it]#015 30%|███       | 30/100 [00:47<01:47,  1.54s/it]#015 31%|███       | 31/100 [00:48<01:46,  1.54s/it]#015 32%|███▏      | 32/100 [00:50<01:44,  1.54s/it]#015 33%|███▎      | 33/100 [00:51<01:43,  1.54s/it]#015 34%|███▍      | 34/100 [00:53<01:41,  1.54s/it]#015 35%|███▌      | 35/100 [00:54<01:39,  1.54s/it]#015 36%|███▌      | 36/100 [00:56<01:38,  1.54s/it]#015 37%|███▋      | 37/100 [00:58<01:37,  1.55s/it]#015 38%|███▊      | 38/100 [00:59<01:35,  1.55s/it]#015 39%|███▉      | 39/100 [01:01<01:34,  1.55s/it]#015 40%|████      | 40/100 [01:02<01:32,  1.55s/it]#015 41%|████      | 41/100 [01:04<01:31,  1.54s/it]#015 42%|████▏     | 42/100 [01:05<01:29,  1.54s/it]#015 43%|████▎     | 43/100 [01:07<01:27,  1.54s/it]#015 44%|████▍     | 44/100 [01:08<01:26,  1.55s/it]#015 45%|████▌     | 45/100 [01:10<01:25,  1.55s/it]#015 46%|████▌     | 46/100 [01:11<01:23,  1.54s/it]#015 47%|████▋     | 47/100 [01:13<01:21,  1.54s/it]#015 48%|████▊     | 48/100 [01:15<01:20,  1.54s/it]#015 49%|████▉     | 49/100 [01:16<01:18,  1.54s/it]#015 50%|█████     | 50/100 [01:18<01:17,  1.54s/it]#015 51%|█████     | 51/100 [01:19<01:15,  1.54s/it]#015 52%|█████▏    | 52/100 [01:21<01:14,  1.55s/it]#015 53%|█████▎    | 53/100 [01:22<01:12,  1.54s/it]#015 54%|█████▍    | 54/100 [01:24<01:10,  1.54s/it]#015 55%|█████▌    | 55/100 [01:25<01:09,  1.54s/it]#015 56%|█████▌    | 56/100 [01:27<01:07,  1.53s/it]#015 57%|█████▋    | 57/100 [01:28<01:05,  1.53s/it]#015 58%|█████▊    | 58/100 [01:30<01:04,  1.53s/it]#015 59%|█████▉    | 59/100 [01:31<01:02,  1.53s/it]#015 60%|██████    | 60/100 [01:33<01:01,  1.54s/it]#015 61%|██████    | 61/100 [01:35<00:59,  1.54s/it]#015 62%|██████▏   | 62/100 [01:36<00:57,  1.52s/it]#015 63%|██████▎   | 63/100 [01:38<00:56,  1.53s/it]#015 64%|██████▍   | 64/100 [01:39<00:55,  1.53s/it]#015 65%|██████▌   | 65/100 [01:41<00:53,  1.54s/it]#015 66%|██████▌   | 66/100 [01:42<00:52,  1.54s/it]#015 67%|██████▋   | 67/100 [01:44<00:50,  1.54s/it]#015 68%|██████▊   | 68/100 [01:45<00:49,  1.53s/it]#015 69%|██████▉   | 69/100 [01:47<00:47,  1.54s/it]#015 70%|███████   | 70/100 [01:48<00:46,  1.54s/it]#015 71%|███████   | 71/100 [01:50<00:44,  1.54s/it]#015 72%|███████▏  | 72/100 [01:51<00:43,  1.54s/it]#015 73%|███████▎  | 73/100 [01:53<00:41,  1.54s/it]#015 74%|███████▍  | 74/100 [01:55<00:40,  1.54s/it]#015 75%|███████▌  | 75/100 [01:56<00:38,  1.54s/it]#015 76%|███████▌  | 76/100 [01:58<00:36,  1.54s/it]#015 77%|███████▋  | 77/100 [01:59<00:35,  1.54s/it]#015 78%|███████▊  | 78/100 [02:01<00:33,  1.53s/it]#015 79%|███████▉  | 79/100 [02:02<00:32,  1.53s/it]#015 80%|████████  | 80/100 [02:04<00:30,  1.53s/it]#015 81%|████████  | 81/100 [02:05<00:29,  1.53s/it]#015 82%|████████▏ | 82/100 [02:07<00:27,  1.53s/it]#015 83%|████████▎ | 83/100 [02:08<00:26,  1.54s/it]#015 84%|████████▍ | 84/100 [02:10<00:24,  1.54s/it]#015 85%|████████▌ | 85/100 [02:11<00:23,  1.55s/it]#015 86%|████████▌ | 86/100 [02:13<00:21,  1.55s/it]#015 87%|████████▋ | 87/100 [02:15<00:20,  1.55s/it]#015 88%|████████▊ | 88/100 [02:16<00:18,  1.54s/it]#015 89%|████████▉ | 89/100 [02:18<00:16,  1.54s/it]#015 90%|█████████ | 90/100 [02:19<00:15,  1.54s/it]#015 91%|█████████ | 91/100 [02:21<00:13,  1.54s/it]#015 92%|█████████▏| 92/100 [02:22<00:12,  1.53s/it]#015 93%|█████████▎| 93/100 [02:24<00:10,  1.54s/it]#015 94%|█████████▍| 94/100 [02:25<00:09,  1.53s/it]#015 95%|█████████▌| 95/100 [02:27<00:07,  1.54s/it]#015 96%|█████████▌| 96/100 [02:28<00:06,  1.54s/it]#015 97%|█████████▋| 97/100 [02:30<00:04,  1.55s/it]#015 98%|█████████▊| 98/100 [02:31<00:03,  1.54s/it]#015 99%|█████████▉| 99/100 [02:33<00:01,  1.55s/it]#015100%|██████████| 100/100 [02:35<00:00,  1.55s/it]#015                                                 #015#015100%|██████████| 100/100 [02:35<00:00,  1.55s/it][INFO|trainer.py:1352] 2021-11-27 15:04:24,023 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m#015                                                 #015#015100%|██████████| 100/100 [02:35<00:00,  1.55s/it]#015100%|██████████| 100/100 [02:35<00:00,  1.55s/it]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:351] 2021-11-27 15:04:24,700 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:889] 2021-11-27 15:04:25,606 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\n",
      "2021-11-27 15:04:27 Uploading - Uploading generated training model\n",
      "2021-11-27 15:05:09 Completed - Training job completed\n",
      "Training seconds: 501\n",
      "Billable seconds: 501\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFace\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    py_version=\"py36\",\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"1_sources\",\n",
    "    pytorch_version=\"1.7.1\",\n",
    "    transformers_version=\"4.6.1\",\n",
    "    hyperparameters={\n",
    "        \"model_name\":\"distilbert-base-uncased\",\n",
    "        \"train_batch_size\": 16,\n",
    "        \"epochs\": 3\n",
    "        # \"max_steps\": 100 # to shorten training cycle, remove in real scenario\n",
    "    },\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(train_dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label value: 1\n",
      "Sample list of token ids:\n",
      "[101, 1038, 1005, 1038, 1032, 1005, 8235, 2058, 1011, 3772, 2011, 23920, 5754, 6031, 1012, 2190, 6918, 7570, 5092, 3203, 1045, 2031, 2412, 2464, 1010, 1998, 2293, 5019, 1999, 4253, 9746, 2024, 2117, 2000, 3904, 1012, 1996, 9781, 2006, 2227, 2003, 1037, 4438, 1010, 2004, 2204, 2004, 2505, 1999, 17162, 12279, 2015, 1012, 1996, 2202, 2006, 9559, 2003, 2036, 21688, 1012, 2044, 2108, 5496, 1997, 2108, 1037, 2735, 16531, 1010, 4855, 2041, 2010, 5795, 1010, 1998, 2108, 9841, 21821, 2102, 1996, 5160, 1997, 27233, 3406, 10053, 23822, 24436, 2135, 1000, 1045, 1032, 1032, 1032, 1005, 1049, 1037, 5160, 1000, 2002, 2758, 1012, 2093, 6057, 2616, 1012, 10799, 17214, 12821, 1010, 1037, 5440, 2013, 1996, 2101, 6554, 12055, 2265, 1010, 2003, 10392, 2182, 2205, 2004, 1037, 5506, 19965, 2040, 4122, 2000, 10188, 1996, 17276, 1012, 2010, 2839, 2003, 2062, 3287, 6767, 16136, 2084, 5156, 1012, 1996, 2902, 3496, 1010, 1998, 1996, 3496, 2073, 1996, 11573, 18445, 1037, 12451, 2609, 1010, 2024, 2035, 1011, 2051, 10002, 1012, 2298, 2005, 1996, 3456, 3496, 1998, 1996, 2048, 2502, 28661, 2015, 3554, 1006, 2028, 19501, 2015, 1007, 1012, 2023, 3185, 4152, 2488, 2169, 2051, 1045, 2156, 2009, 1006, 2029, 2003, 3243, 2411, 1007, 1012, 1032, 1005, 1005, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample list of token ids:\n",
      "b'b\\'Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I\\\\\\'m a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\\''\n"
     ]
    }
   ],
   "source": [
    "response = client.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName':imdb_feature_group.name,\n",
    "            'RecordIdentifiersValueAsString': [\"0\", \"1\", \"2\"], # picking several records to run inference.\n",
    "            'FeatureNames': [\n",
    "                'tokenized-text', \"label\", 'text'\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# preparing the inference payload\n",
    "labels = []\n",
    "input_ids = []\n",
    "texts = []\n",
    "\n",
    "for record in response[\"Records\"]:\n",
    "    for feature in record[\"Record\"]:\n",
    "        if feature[\"FeatureName\"]==\"label\":\n",
    "            labels.append(feature[\"ValueAsString\"])\n",
    "        if feature[\"FeatureName\"]==\"tokenized-text\":\n",
    "            list_of_str = feature[\"ValueAsString\"].strip(\"][\").split(\", \")\n",
    "            input_ids.append([int(el) for el in list_of_str])\n",
    "        if feature[\"FeatureName\"]==\"text\":\n",
    "            # list_of_str = feature[\"ValueAsString\"].strip(\"][\").split(\", \")\n",
    "            texts.append(feature[\"ValueAsString\"])    \n",
    "\n",
    "print(f\"Sample label value: {labels[0]}\")\n",
    "print(f\"Sample list of token ids:\\n{input_ids[0]}\")\n",
    "print(f\"Sample list of token ids:\\n{texts[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFaceModel\n",
    "\n",
    "model = estimator.create_model(role=role, \n",
    "                               entry_point=\"inference.py\", \n",
    "                               source_dir=\"1_sources\",\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample index: 0; predicted label: LABEL_1; confidence score: 0.5366891026496887\n",
      "Sample index: 1; predicted label: LABEL_0; confidence score: 0.5006090998649597\n",
      "Sample index: 2; predicted label: LABEL_1; confidence score: 0.516234815120697\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    prediction = predictor.predict([texts[i]])\n",
    "    print(f\"Sample index: {i}; predicted label: {prediction[0]['label']}; confidence score: {prediction[0]['score']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
