{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto scaling\n",
    "\n",
    "In this example we will apply simple tracking policy to TF HuggingFace endppoint and load it with some synthethic traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifacts for DistilBert model for Question-Answering task\n",
    "\n",
    "! mkdir distilbert-base-uncased-distilled-squad\n",
    "! mkdir distilbert-base-uncased-distilled-squad/1\n",
    "! mkdir distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "! wget https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/saved_model.tar.gz\n",
    "! tar -zxvf saved_model.tar.gz -C distilbert-base-uncased-distilled-squad/1\n",
    "\n",
    "! cp 1_src/inference.py distilbert-base-uncased-distilled-squad/code\n",
    "! cp 1_src/requirements.txt distilbert-base-uncased-distilled-squad/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os \n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#role = get_execution_role()  # TODO: replace it\n",
    "role=\"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\" # TODO: this has to be replaced\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'auto-scaling'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-941656036254/auto-scaling/model-artifacts/distilbert-base-uncased-distilled-squad.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))     \n",
    "\n",
    "print(model_data)                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "env = { \"NLP_TASK\":\"question-answering\"\n",
    "    }\n",
    "\n",
    "# The \"Model\" object doesn't create a SageMaker Model until a Transform Job or Endpoint is created.\n",
    "tensorflow_serving_model = TensorFlowModel(model_data=model_data,\n",
    "                                 name=\"qa-tensorflow\",\n",
    "                                 role=role,\n",
    "                                 framework_version='2.8',\n",
    "                                 env=env,\n",
    "                                 sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Using already existing model: qa-tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "instance = \"ml.c5.2xlarge\"\n",
    "\n",
    "predictor = tensorflow_serving_model.deploy(initial_instance_count=1, instance_type=instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'qa-tensorflow-2022-08-18-11-38-23-027',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:941656036254:endpoint/qa-tensorflow-2022-08-18-11-38-23-027',\n",
       " 'EndpointConfigName': 'qa-tensorflow-2022-08-18-11-38-23-027',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.8-cpu',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference@sha256:d72f9623bab06fcf97cef4cad7a5748926f002a62503fc06c89fb29f09a2beaf',\n",
       "     'ResolutionTime': datetime.datetime(2022, 8, 18, 7, 38, 24, 597000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2022, 8, 18, 7, 38, 23, 581000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 8, 18, 7, 40, 1, 793000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '2bda7ef4-c3a0-4023-8863-3b0619fec216',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2bda7ef4-c3a0-4023-8863-3b0619fec216',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '764',\n",
       "   'date': 'Thu, 18 Aug 2022 11:45:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multi Container Endpoint\n",
    "\n",
    "This has to be replaced with locust of some sort: https://github.com/arunprsh/SageMaker-Load-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "article = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#  preparing data for TF Serving format\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "max_length = 384\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "encoded_input = tokenizer(question, article, padding='max_length', max_length=max_length)\n",
    "encoded_input = dict(encoded_input)\n",
    "qa_inputs = [{\"input_ids\": np.array(encoded_input[\"input_ids\"]).tolist(), \"attention_mask\":np.array(encoded_input[\"attention_mask\"]).tolist()}]\n",
    "#qa_inputs = {\"input_ids\": np.array(encoded_input[\"input_ids\"]).tolist(), \"attention_mask\":np.array(encoded_input[\"attention_mask\"]).tolist()}\n",
    "qa_inputs = {\"instances\" : qa_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(qa_inputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(tf_response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What kind of forest is Amazon?, answer: moist broadleaf forest\n"
     ]
    }
   ],
   "source": [
    "answer_start_index = int(tf.math.argmax(predictions['predictions'][0]['output_0']))\n",
    "answer_end_index = int(tf.math.argmax(predictions['predictions'][0]['output_1']))\n",
    "\n",
    "predict_answer_tokens = encoded_input[\"input_ids\"][answer_start_index : answer_end_index + 1]\n",
    "tf_response = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}, answer: {tf_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_file = \"3_src/payload.json\"\n",
    "json.dump(qa_inputs, open(payload_file, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Scaling Policies\n",
    "\n",
    "We start from simple tracking policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "as_client = boto3.client('application-autoscaling') # Common class representing Application Auto Scaling for SageMaker amongst other services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource type is variant and the unique identifier is the resource ID.\n",
    "resource_id=f\"endpoint/{predictor.endpoint_name}/variant/AllTraffic\"\n",
    "policy_name = f'Request-ScalingPolicy-{predictor.endpoint_name}'\n",
    "scalable_dimension = 'sagemaker:variant:DesiredInstanceCount'\n",
    "\n",
    "# scaling configuration\n",
    "response = as_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")\n",
    "\n",
    "\n",
    "#Target Scaling\n",
    "response = as_client.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # Threshold\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # duration until scale in\n",
    "        'ScaleOutCooldown': 60 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running load tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r \"../utils/load_testing/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../utils/load_testing/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../utils/load_testing/config.py\n",
    "\n",
    "# provide configuration parameters\n",
    "# TODO: clean up config from personal data\n",
    "\n",
    "HOST = 'runtime.sagemaker.us-east-1.amazonaws.com'\n",
    "REGION = 'us-east-1'\n",
    "# replace the url below with the sagemaker endpoint you are load testing\n",
    "ENDPOINT_NAME = \"qa-tensorflow-2022-08-16-12-55-04-479\"\n",
    "SAGEMAKER_ENDPOINT_URL = f'https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{ENDPOINT_NAME}/invocations'\n",
    "ACCESS_KEY = '<USE YOUR AWS ACCESS KEY HERE>'\n",
    "SECRET_KEY = '<USE YOUR AWS SECRET KEY HERE>'\n",
    "# replace the context type below as per your requirements\n",
    "CONTENT_TYPE = 'application/json'\n",
    "METHOD = 'POST'\n",
    "SERVICE = 'sagemaker'\n",
    "SIGNED_HEADERS = 'content-type;host;x-amz-date'\n",
    "CANONICAL_QUERY_STRING = ''\n",
    "ALGORITHM = 'AWS4-HMAC-SHA256'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start locust\n",
    "\n",
    "Beloew run in console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! locust -f ../utils/load_testing/locustfile.py --headless -u 20 -r 1 --run-time 5m\n",
    "# u - number of concurrent users\n",
    "# r - spawn rate (users per sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "status = endpoint_description['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    status = endpoint_description['EndpointStatus']\n",
    "    instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update manually endpoint\n",
    "\n",
    "Details are described here: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to delete scaling policy\n",
    "\n",
    "response = as_client.delete_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '4c2d0f4b-d565-4be7-9ad0-00870c477c23', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '4c2d0f4b-d565-4be7-9ad0-00870c477c23', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'date': 'Thu, 18 Aug 2022 12:06:20 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = as_client.deregister_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current instance count: 4\n"
     ]
    }
   ],
   "source": [
    "# get current instance count\n",
    "\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "print(f\"Current instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:941656036254:endpoint/qa-tensorflow-2022-08-18-11-38-23-027',\n",
       " 'ResponseMetadata': {'RequestId': '41729f43-28ca-49bc-9471-d70a15a1d3bf',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '41729f43-28ca-49bc-9471-d70a15a1d3bf',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '105',\n",
       "   'date': 'Thu, 18 Aug 2022 12:11:49 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.update_endpoint_weights_and_capacities(EndpointName=predictor.endpoint_name,\n",
    "                            DesiredWeightsAndCapacities=[\n",
    "                                {\n",
    "                                    'VariantName': 'AllTraffic',\n",
    "                                    'DesiredInstanceCount': 1\n",
    "                                }\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "status = endpoint_description['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    status = endpoint_description['EndpointStatus']\n",
    "    instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current instance count: 1\n"
     ]
    }
   ],
   "source": [
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "print(f\"Current instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(predictor.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
