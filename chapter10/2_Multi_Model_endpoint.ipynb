{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Container endpoints\n",
    "\n",
    "In this example we will deploy two different models for summarization and Q&A tasks.\n",
    "Please note that loading and packaging models may take several minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#role = get_execution_role()  # TODO: replace it\n",
    "role=\"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\" # TODO: this has to be replaced\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'multi-model'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "region = sagemaker_session.boto_region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Multi Model Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "GER_MODEL = \"oliverguhr/german-sentiment-bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/english_sentiment/tokenizer_config.json',\n",
       " 'models/english_sentiment/special_tokens_map.json',\n",
       " 'models/english_sentiment/vocab.txt',\n",
       " 'models/english_sentiment/added_tokens.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve english model\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "en_tokenizer = DistilBertTokenizer.from_pretrained(EN_MODEL)\n",
    "en_model = DistilBertForSequenceClassification.from_pretrained(EN_MODEL)\n",
    "\n",
    "inputs = en_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = en_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predictions = en_model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "en_model_path = \"models/english_sentiment\"\n",
    "os.makedirs(en_model_path, exist_ok=True)\n",
    "\n",
    "en_model.save_pretrained(save_directory=en_model_path)\n",
    "en_tokenizer.save_pretrained(save_directory=en_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/german_sentiment/tokenizer_config.json',\n",
       " 'models/german_sentiment/special_tokens_map.json',\n",
       " 'models/german_sentiment/vocab.txt',\n",
       " 'models/german_sentiment/added_tokens.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve German model\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "ger_tokenizer = BertTokenizer.from_pretrained(GER_MODEL)\n",
    "ger_model = BertForSequenceClassification.from_pretrained(GER_MODEL)\n",
    "\n",
    "inputs = ger_tokenizer(\"Das ist gar nicht mal so gut\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = ger_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predictions = ger_model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "ger_model_path = \"models/german_sentiment\"\n",
    "os.makedirs(ger_model_path, exist_ok=True)\n",
    "\n",
    "en_model.save_pretrained(save_directory=ger_model_path)\n",
    "en_tokenizer.save_pretrained(save_directory=ger_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inference Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 2_src/en_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 2_src/get_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models/english_sentiment/code: File exists\n",
      "a .\n",
      "a ./tokenizer_config.json\n",
      "a ./special_tokens_map.json\n",
      "a ./config.json\n",
      "a ./code\n",
      "a ./vocab.txt\n",
      "a ./pytorch_model.bin\n",
      "a ./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "! mkdir models/english_sentiment/code\n",
    "! cp 2_src/en_inference.py models/english_sentiment/code/inference.py\n",
    "! tar -czvf models/english_sentiment.tar.gz -C models/english_sentiment/ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models/german_sentiment/code: File exists\n",
      "a .\n",
      "a ./tokenizer_config.json\n",
      "a ./special_tokens_map.json\n",
      "a ./config.json\n",
      "a ./code\n",
      "a ./vocab.txt\n",
      "a ./pytorch_model.bin\n",
      "a ./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "! mkdir models/german_sentiment/code\n",
    "! cp 2_src/ger_inference.py models/german_sentiment/code/inference.py\n",
    "! tar -czvf models/german_sentiment.tar.gz -C models/german_sentiment/ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model_data = sagemaker_session.upload_data('models/english_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\n",
    "ger_model_data = sagemaker_session.upload_data('models/german_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-941656036254/multi-model/english_sentiment.tar.gz\n",
      "s3://sagemaker-us-east-1-941656036254/multi-model/german_sentiment.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(en_model_data)\n",
    "print(ger_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "HF_VERSION = '4.17.0'\n",
    "PT_VERSION = 'pytorch1.10.2'\n",
    "TF_VERSION = \"tensorflow2.6.3\"\n",
    "\n",
    "pt_container_uri = image_uris.retrieve(framework='huggingface',\n",
    "                                region=region,\n",
    "                                version=HF_VERSION,\n",
    "                                image_scope='inference',\n",
    "                                base_framework_version=PT_VERSION,\n",
    "                                #py_version='py38',\n",
    "                                #container_version='ubuntu20.04',\n",
    "                                instance_type='ml.c5.xlarge')\n",
    "\n",
    "print(pt_container_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_data_path = f\"s3://{bucket}/{prefix}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "container  = {\n",
    "    'Image': pt_container_uri,\n",
    "    'ContainerHostname': 'MultiModel',\n",
    "    'Mode': 'MultiModel',\n",
    "    'ModelDataUrl': mm_data_path,\n",
    "    'Environment': {\n",
    "\t    'SAGEMAKER_PROGRAM':'inference.py',\n",
    "\t    'SAGEMAKER_SUBMIT_DIRECTORY':mm_data_path\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.sagemaker_client\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n",
    "\n",
    "model_name = f\"mme-sentiment-model-{unique_id}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container,\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-ep-config\"\n",
    "\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mce-nlp-model-2022-08-2511-01-19-ep\n",
      "mce-nlp-model-2022-08-2511-01-19\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_name)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multi Model Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ger_input = \"Der Test verlief positiv.\"\n",
    "en_input = \"Test results are positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from MultiModel with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Can\\u0027t load tokenizer for \\u0027/opt/ml/models/d3d3623c9ea3827636e7fb90658814c3/model/model\\u0027. If you were trying to load it from \\u0027https://huggingface.co/models\\u0027, make sure you don\\u0027t have a local directory with the same name. Otherwise, make sure \\u0027/opt/ml/models/d3d3623c9ea3827636e7fb90658814c3/model/model\\u0027 is the correct path to a directory containing all relevant files for a DistilBertTokenizer tokenizer.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/mce-nlp-model-2022-08-2513-15-49-ep in account 941656036254 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m en_response \u001b[39m=\u001b[39m runtime_sm_client\u001b[39m.\u001b[39;49minvoke_endpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     EndpointName\u001b[39m=\u001b[39;49mendpoint_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     ContentType\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     Accept\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     TargetModel\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39menglish_sentiment.tar.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     Body\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps(en_input),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter10/2_Multi_Model_endpoint.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=503'>504</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=504'>505</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=505'>506</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=506'>507</a>\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=507'>508</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py:911\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=908'>909</a>\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=909'>910</a>\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=910'>911</a>\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=911'>912</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/botocore/client.py?line=912'>913</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from MultiModel with message \"{\n  \"code\": 400,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Can\\u0027t load tokenizer for \\u0027/opt/ml/models/d3d3623c9ea3827636e7fb90658814c3/model/model\\u0027. If you were trying to load it from \\u0027https://huggingface.co/models\\u0027, make sure you don\\u0027t have a local directory with the same name. Otherwise, make sure \\u0027/opt/ml/models/d3d3623c9ea3827636e7fb90658814c3/model/model\\u0027 is the correct path to a directory containing all relevant files for a DistilBertTokenizer tokenizer.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/mce-nlp-model-2022-08-2513-15-49-ep in account 941656036254 for more information."
     ]
    }
   ],
   "source": [
    "en_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetModel=\"english_sentiment.tar.gz\",\n",
    "    Body=json.dumps(en_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(en_response[\"Body\"].read().decode())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7555540800094604, 'start': 77, 'end': 92, 'answer': 'Selva Amazónica'}\n"
     ]
    }
   ],
   "source": [
    "ger_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetModel=\"german_sentiment.tar.gz\",\n",
    "    Body=json.dumps(ger_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(ger_response[\"Body\"].read().decode())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a4628488-6003-4618-8569-e9c6541d79c2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a4628488-6003-4618-8569-e9c6541d79c2',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 25 Aug 2022 17:34:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_model(ModelName = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
