{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Container endpoints\n",
    "\n",
    "In this example we will deploy two different models for summarization and Q&A tasks.\n",
    "Please note that loading and packaging models may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:43:23--  https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/saved_model.tar.gz\n",
      "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 34.231.117.252\n",
      "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/distilbert-base-cased-distilled-squad/f7e26fe22fdeb23462ae6423fc04b7e4929212a49aa033c3a7b8f30c937c943f?response-content-disposition=attachment%3B%20filename%3D%22saved_model.tar.gz%22 [following]\n",
      "--2022-08-13 18:43:23--  https://cdn-lfs.huggingface.co/distilbert-base-cased-distilled-squad/f7e26fe22fdeb23462ae6423fc04b7e4929212a49aa033c3a7b8f30c937c943f?response-content-disposition=attachment%3B%20filename%3D%22saved_model.tar.gz%22\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 99.84.37.94, 99.84.37.69, 99.84.37.114, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|99.84.37.94|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 241487391 (230M) [application/x-tar]\n",
      "Saving to: ‘saved_model.tar.gz’\n",
      "\n",
      "saved_model.tar.gz  100%[===================>] 230.30M  34.4MB/s    in 6.7s    \n",
      "\n",
      "2022-08-13 18:43:30 (34.2 MB/s) - ‘saved_model.tar.gz’ saved [241487391/241487391]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "x ./\n",
      "x ./variables/\n",
      "x ./saved_model.pb\n",
      "x ./assets/\n",
      "x ./variables/variables.data-00000-of-00001\n",
      "x ./variables/variables.index\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download artifacts for DistilBert model for Question-Answering task\n",
    "\n",
    "! mkdir distilbert-base-uncased-distilled-squad\n",
    "! mkdir distilbert-base-uncased-distilled-squad/1\n",
    "! mkdir distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "! wget https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/saved_model.tar.gz\n",
    "! tar -zxvf saved_model.tar.gz -C distilbert-base-uncased-distilled-squad/1\n",
    "\n",
    "! cp 1_src/inference.py distilbert-base-uncased-distilled-squad/code\n",
    "! cp 1_src/requirements.txt distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "# Old model fetch. to be deleted.\n",
    "#! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tf_model.h5 -P distilbert-base-uncased-distilled-squad\n",
    "#! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json -P distilbert-base-uncased-distilled-squad\n",
    "#! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json -P distilbert-base-uncased-distilled-squad\n",
    "#! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt -P distilbert-base-uncased-distilled-squad\n",
    "#! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json -P distilbert-base-uncased-distilled-squad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:48:20--  https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 34.231.117.252\n",
      "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/facebook/bart-large-cnn/2ac2745c02ac987d82c78a14b426de58d5e4178ae8039ba1c6881eccff3e82f1?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22 [following]\n",
      "--2022-08-13 18:48:20--  https://cdn-lfs.huggingface.co/facebook/bart-large-cnn/2ac2745c02ac987d82c78a14b426de58d5e4178ae8039ba1c6881eccff3e82f1?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.139.29.117, 108.139.29.77, 108.139.29.38, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.139.29.117|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1625270765 (1.5G) [application/octet-stream]\n",
      "Saving to: ‘bart-large-cnn/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>]   1.51G  33.0MB/s    in 50s     \n",
      "\n",
      "2022-08-13 18:49:10 (31.2 MB/s) - ‘bart-large-cnn/pytorch_model.bin’ saved [1625270765/1625270765]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:49:11--  https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json\n",
      "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 34.231.117.252\n",
      "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1355863 (1.3M) [text/plain]\n",
      "Saving to: ‘bart-large-cnn/tokenizer.json’\n",
      "\n",
      "tokenizer.json      100%[===================>]   1.29M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-13 18:49:11 (12.8 MB/s) - ‘bart-large-cnn/tokenizer.json’ saved [1355863/1355863]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:49:12--  https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json\n",
      "Resolving huggingface.co (huggingface.co)... 34.231.117.252, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|34.231.117.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 898823 (878K) [text/plain]\n",
      "Saving to: ‘bart-large-cnn/vocab.json’\n",
      "\n",
      "vocab.json          100%[===================>] 877.76K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-08-13 18:49:12 (9.76 MB/s) - ‘bart-large-cnn/vocab.json’ saved [898823/898823]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:49:13--  https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json\n",
      "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 34.231.117.252\n",
      "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1585 (1.5K) [text/plain]\n",
      "Saving to: ‘bart-large-cnn/config.json’\n",
      "\n",
      "config.json         100%[===================>]   1.55K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-13 18:49:13 (168 MB/s) - ‘bart-large-cnn/config.json’ saved [1585/1585]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-08-13 18:49:14--  https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt\n",
      "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 34.231.117.252\n",
      "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘bart-large-cnn/merges.txt’\n",
      "\n",
      "merges.txt          100%[===================>] 445.62K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-08-13 18:49:14 (6.18 MB/s) - ‘bart-large-cnn/merges.txt’ saved [456318/456318]\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download artifacts for Bart model for Summarization task\n",
    "\n",
    "! wget https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin -P bart-large-cnn\n",
    "! wget https://huggingface.co/facebook/bart-large-cnn/resolve/main/tokenizer.json -P bart-large-cnn\n",
    "! wget https://huggingface.co/facebook/bart-large-cnn/resolve/main/vocab.json -P bart-large-cnn\n",
    "! wget https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json -P bart-large-cnn\n",
    "! wget https://huggingface.co/facebook/bart-large-cnn/resolve/main/merges.txt -P bart-large-cnn\n",
    "! cp 1_src/inference.py bart-large-cnn\n",
    "! cp 1_src/requirements.txt bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tar -C \"$PWD\" -czf bart-large-cnn.tar.gz bart-large-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#role = get_execution_role()  # TODO: replace it\n",
    "role=\"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\" # TODO: this has to be replaced\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'multi-container'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))\n",
    "\n",
    "summarization_model_data = sagemaker_session.upload_data('bart-large-cnn.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_src/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['attention_mask'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 384)\n",
      "      name: serving_default_attention_mask:0\n",
      "  inputs['input_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 384)\n",
      "      name: serving_default_input_ids:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_0'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 384)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 384)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "### For TensorFlow we need to check signature of the model\n",
    "\n",
    "! saved_model_cli show --dir distilbert-base-uncased-distilled-squad/1 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local mode\n",
    "\n",
    "TODO: delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env = { \"NLP_TASK\":\"summarization\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Note: You can update the 'torchserve-predictor.py' file as needed according to the model you want to use (ie BERT) \n",
    "model = PyTorchModel(model_data=summarization_model_data,\n",
    "                   role=role, \n",
    "                   entry_point='pipeline_predictor.py',\n",
    "                   source_dir='1_src',\n",
    "                   framework_version='1.9.0',\n",
    "                   py_version='py38',\n",
    "                   env=env,\n",
    "                   sagemaker_session=sagemaker_local_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "local_predictor = model.deploy(initial_instance_count=1, instance_type=instance_type, serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "context = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n",
    "\n",
    "\n",
    "data = {\"article\":context, \"max_length\":100}\n",
    "print(data)\n",
    "#local_predictor.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Multi Container Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "instance_type = \"ml.c5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_env = {\n",
    "    \"NLP_TASK\" : \"summarization\",\n",
    "    \"SAGEMAKER_PROGRAM\" : \"inference.py\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": summarization_model_data,\n",
    "}\n",
    "\n",
    "pt_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "pytorch_container = {\n",
    "    \"ContainerHostname\": \"pytorch-bart-summarizer\",\n",
    "    \"Image\": pt_inference_image_uri,\n",
    "    \"ModelDataUrl\": summarization_model_data,\n",
    "    \"Environment\" : summarization_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_env = {\n",
    "    \"NLP_TASK\" : \"question-answering\"\n",
    "\n",
    "}\n",
    "\n",
    "tf_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.8\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "tensorflow_container = {\n",
    "    \"ContainerHostname\": \"tensorflow-distilbert-qa\",\n",
    "    \"Image\": tf_inference_image_uri,\n",
    "    \"ModelDataUrl\": qa_model_data,\n",
    "    \"Environment\" : qa_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ContainerHostname': 'pytorch-bart-summarizer', 'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.9-cpu-py38', 'ModelDataUrl': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/bart-large-cnn.tar.gz', 'Environment': {'NLP_TASK': 'summarization', 'SAGEMAKER_PROGRAM': 'inference.py', 'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/bart-large-cnn.tar.gz'}}\n",
      "{'ContainerHostname': 'tensorflow-distilbert-qa', 'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.8-cpu', 'ModelDataUrl': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/distilbert-base-uncased-distilled-squad.tar.gz', 'Environment': {'NLP_TASK': 'question-answering'}}\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_container)\n",
    "print(tensorflow_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.sagemaker_client\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n",
    "\n",
    "model_name = f\"nlp-multi-container-{unique_id}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[pytorch_container, tensorflow_container],\n",
    "    InferenceExecutionConfig={\"Mode\": \"Direct\"},\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"nlp-multi-container-ep-config-{unique_id}\"\n",
    "\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": \"nlp-multi-container\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"nlp-multi-container-ep-{unique_id}\"\n",
    "\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multi Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "article = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n",
    "\n",
    "#qa_input = {\"question\":question, \"context\":article}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_input = {\"article\":article, \"max_length\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_179']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#  preparing data for TF Serving format\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "max_length = 384\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "encoded_input = tokenizer(question, article, padding='max_length', max_length=max_length)\n",
    "encoded_input = dict(encoded_input)\n",
    "qa_inputs = [{\"input_ids\": np.array(encoded_input[\"input_ids\"]).tolist(), \"attention_mask\":np.array(encoded_input[\"attention_mask\"]).tolist()}]\n",
    "qa_inputs = {\"instances\" : qa_inputs}\n",
    "#outputs = model(**inputs)\n",
    "\n",
    "#answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "#answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "#predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "#tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [{'input_ids': [101, 1327, 1912, 1104, 3304, 1110, 9786, 136, 102, 1109, 9786, 24369, 113, 4269, 131, 18827, 1777, 7277, 10961, 28206, 13109, 1137, 7277, 10961, 28206, 5813, 132, 2124, 131, 22087, 1233, 2497, 7277, 10961, 4722, 4578, 117, 9786, 7171, 1137, 1932, 9786, 1465, 132, 1497, 131, 1370, 24559, 1204, 1821, 10961, 11153, 26042, 132, 2954, 131, 9786, 9014, 4915, 12821, 4867, 114, 117, 1145, 1227, 1107, 1483, 1112, 9786, 1465, 1137, 1103, 9786, 17009, 117, 1110, 170, 11758, 4728, 21407, 3304, 1115, 3662, 1211, 1104, 1103, 9786, 8434, 1104, 1375, 1738, 119, 1188, 8434, 14474, 128, 117, 1288, 117, 1288, 1961, 2850, 113, 123, 117, 5689, 117, 1288, 4816, 1940, 114, 117, 1104, 1134, 126, 117, 2260, 117, 1288, 1961, 2850, 113, 123, 117, 1620, 117, 1288, 4816, 1940, 114, 1132, 2262, 1118, 1103, 24369, 119, 1188, 1805, 2075, 3441, 7078, 1106, 2551, 6015, 119, 1109, 2656, 1104, 1103, 3304, 1110, 4049, 1439, 3524, 117, 1114, 2539, 110, 1104, 1103, 24369, 117, 1723, 1118, 7022, 1114, 1492, 110, 117, 6855, 1114, 1275, 110, 117, 1105, 1114, 3137, 7919, 1107, 7917, 117, 10244, 117, 11686, 117, 20345, 117, 17078, 28085, 1105, 1497, 26354, 119, 1311, 1137, 7844, 1107, 1300, 6015, 4651, 107, 9786, 2225, 107, 1107, 1147, 2666, 119, 1109, 9786, 5149, 1166, 1544, 1104, 1103, 5015, 112, 188, 2735, 24369, 1116, 117, 1105, 8302, 1103, 2026, 1105, 1211, 25128, 3309, 10840, 14441, 1104, 5065, 24369, 1107, 1103, 1362, 117, 1114, 1126, 3555, 21830, 3775, 2510, 2863, 3233, 1154, 1479, 117, 1288, 1530, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]}\n"
     ]
    }
   ],
   "source": [
    "print(qa_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"tensorflow-distilbert-qa\",\n",
    "    Body=json.dumps(qa_inputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(tf_response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.81213641, -6.33242798, -7.15034676, -7.79426241, -6.90369606, -8.3878746, -5.81328297, -6.05798721, -6.61176205, 1.79013884, 4.34852314, -0.252203882, -4.39401484, -4.04632378, -7.97626305, -3.93852258, -7.52109814, -5.39915323, -7.81546879, -8.23505402, -7.76532936, -7.82198906, -4.54771233, -8.06315, -8.57569695, -7.88697243, -7.86777449, -4.8922739, -7.9327507, -4.11743212, -7.51967812, -7.50865, -6.01840925, -7.89440441, -7.91009808, -7.60474825, -7.92939043, -3.12175298, -7.23803806, -8.49301, -5.9343586, -2.62836814, -7.25240135, -7.82351112, -5.01983881, -7.87251902, -5.06236935, -7.2889185, -7.07985878, -6.05922174, -7.75251818, -7.95574093, -7.72836256, -7.57692575, -4.79824066, -7.64635849, -2.57327676, -7.0719595, -7.66239166, -7.45005035, -7.40450716, -4.3102684, -5.45500469, -5.99215889, -6.43772, -5.71161842, -5.73724747, -7.01345539, 0.741526544, -6.04399204, -7.55634308, -0.873561263, 2.05227709, -3.75576329, -4.57667065, -0.241587117, 5.76309681, 9.03434181, 8.13376, 0.771087229, 1.25531423, -3.85686636, -4.15995216, -4.32733917, -7.54844475, -4.98027706, -1.28696, -5.11139107, -7.93242455, -4.41302252, -5.49494, -5.48473835, -3.96387792, -5.06771803, -6.75283909, -4.37581396, -7.80547619, -6.60562134, -7.82924652, -6.95212269, -7.03249598, -6.76940918, -6.9891367, -5.34932423, -8.69425774, -7.42443943, -8.61603165, -7.91028309, -7.79712677, -8.02691078, -7.05909252, -7.24580288, -7.48053932, -8.30043793, -4.98818684, -8.30913544, -7.11565256, -8.3144989, -7.60046244, -7.69764423, -7.69803429, -7.48148727, -5.86716, -8.76695061, -7.79868317, -8.73648262, -8.13766098, -8.00272083, -8.25079823, -8.20362282, -8.39399052, -7.19475269, -8.00591946, -3.56994438, -0.980433881, -8.02768326, -6.28064823, -7.28628159, -8.06825256, -6.97283411, -7.73038, -8.77413845, -6.66213751, -7.9391489, -7.43259048, -4.28970289, -4.99137688, -7.99466372, -5.43764114, -3.78769922, -7.7356329, -6.54243803, -7.25778723, -3.05337548, -8.45275879, -8.02404594, -5.27561474, -7.5755558, -8.60756779, -6.74052238, -3.63666511, -8.76453781, -8.04810619, -9.57406425, -5.46448517, -8.58606148, -6.61782, -8.39124489, -8.99166107, -6.30476952, -8.49706459, -6.91443, -8.18516, -8.86192513, -8.98947906, -7.79736042, -6.84502029, -8.46944427, -8.99796295, -6.16195107, -9.52871704, -6.52558, -9.46899, -6.78101444, -9.45220947, -7.028162, -9.38282776, -7.3543334, -8.74966717, -9.19105721, -5.81213093, -6.92670727, -8.08358097, -6.31352663, -9.16589451, -7.77503681, -8.75734234, -6.97373581, -8.57383, -8.32263756, -4.86810875, -2.1225543, -6.92626333, -8.88569927, -9.32845879, -8.85915947, -8.26963615, -7.88832951, -3.87696028, -2.02321672, -8.07951355, -6.7788415, -7.26549768, -9.15803051, -8.27331, -7.93912792, -9.06726837, -9.18499756, -7.04615211, -4.96468353, -8.38026142, -8.63596916, -8.35536289, -6.61975861, -6.03975725, -5.13371, -8.03698158, -5.31400394, -3.42678475, -7.59170914, -7.08288956, -5.84828568, -7.2967453, 0.572648, -3.24494052, -8.26244926, -7.78577518, -7.42305231, -8.56930447, -8.12800789, -7.07125, -7.26705313, -5.61346722, -8.15735245, -7.07504559, -5.63343716, -8.01972103, -9.08974266, -6.89504433, -9.166852, -8.61495304, -8.56857491, -8.52835274, -6.61171055, -9.82754135, -9.81749535, -9.84056854, -9.85566902, -9.86833382, -9.83841801, -9.88336945, -9.90420723, -9.89540482, -9.88846684, -9.8735733, -9.88055706, -9.89042473, -9.90811539, -9.87286282, -9.85545, -9.88644, -9.87854576, -9.83482552, -9.82344627, -9.82147408, -9.81250381, -9.89245605, -9.8747406, -9.8501, -9.89487457, -9.85798073, -9.890028, -9.91657734, -9.89454842, -9.9401741, -9.84879875, -9.86658478, -9.88515186, -9.85644531, -9.88802338, -9.84669685, -9.83959198, -9.85834789, -9.86245346, -9.85692406, -9.85430241, -9.85203362, -9.88093758, -9.87693405, -9.87422657, -9.87897682, -9.90232277, -9.90541363, -9.87522507, -9.88200092, -9.87968063, -9.86494255, -9.89153862, -9.85433865, -9.86069298, -9.87878, -9.86727619, -9.88844395, -9.85716915, -9.86111259, -9.8813324, -9.87519741, -9.86639595, -9.86506557, -9.88430786, -9.87337303, -9.88530159, -9.89596558, -9.88547134, -9.91033173, -9.91765, -9.89911079, -9.91065407, -9.8828373, -9.89782143, -9.88716316, -9.89300728, -9.91529465, -9.86116219, -9.88715, -9.87607193, -9.86278343, -9.86279488, -9.84458542, -9.84894753, -9.84836292, -9.86423111, -9.83943844, -9.8472662, -9.87596512, -9.84338951, -9.85189056, -9.84482, -9.85204315, -9.86971855, -9.8314476, -9.84900856, -9.85155678, -9.89135075, -9.85535908, -9.84664536, -9.86989212, -9.82916832, -9.85092, -9.83228111, -9.81796, -9.82949829, -9.83345604, -9.7982769, -9.80912209, -9.83535194, -9.81528282, -9.83336639, -9.83778, -9.84375381, -9.83512, -9.82902241, -9.85537529, -9.8947258, -9.87397194, -9.85754299, -9.83698463, -9.84357452, -9.89635944, -9.8655, -9.87566566, -9.86089325, -9.84945679, -9.85029602]\n",
      "tf.Tensor(77, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(predictions['predictions'][0]['output_0'])\n",
    "print(tf.math.argmax(predictions['predictions'][0]['output_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What kind of forest is Amazon?, answer: moist broadleaf forest\n"
     ]
    }
   ],
   "source": [
    "answer_start_index = int(tf.math.argmax(predictions['predictions'][0]['output_0']))\n",
    "answer_end_index = int(tf.math.argmax(predictions['predictions'][0]['output_1']))\n",
    "\n",
    "predict_answer_tokens = encoded_input[\"input_ids\"][answer_start_index : answer_end_index + 1]\n",
    "tf_response = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}, answer: {tf_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_result = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"pytorch-bart-summarizer\",\n",
    "    Body=json.dumps(summarization_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are initial experiments.\n",
    "\n",
    "To be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env = {\n",
    "    \"NLP_TASK\": \"summarization\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Note: You can update the 'torchserve-predictor.py' file as needed according to the model you want to use (ie BERT) \n",
    "model = PyTorchModel(model_data=summarization_model_data,\n",
    "                   role=role, \n",
    "                   entry_point='pipeline_predictor.py',\n",
    "                   source_dir='1_src',\n",
    "                   framework_version='1.9.0',\n",
    "                   py_version='py38',\n",
    "                   env=env,\n",
    "                   sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.4xlarge\", serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def timer(f,*args):   \n",
    "    \n",
    "    start = perf_counter()\n",
    "    f(*args)\n",
    "    return (1000 * (perf_counter() - start))\n",
    "\n",
    "\n",
    "stats = np.array([timer(predict_f, local_predictor) for _ in range(10)])\n",
    "print(stats)\n",
    "\n",
    "for i in range(100):\n",
    "    local_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
