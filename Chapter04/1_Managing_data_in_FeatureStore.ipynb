{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker Feature Store for Training and Inference\n",
    "\n",
    "In this practical example, we will develop skills how to use SageMaker Feature Store to ingest, process, and use at training and at inference times. For this we will use `IMDB movie reviews` dataset. We will tokenize movie reviews and store tokenized data in Feature Store, so you don’t have to re-tokenize the dataset next time we want to use it. After that, we will train our model to categorize positive and negative reviews using data saved in Feature Store. Then we will deploy trained model and use data from Feature Store for model inference. \n",
    "\n",
    "### Prerequisites\n",
    "We use SageMaker Feature Store SDK to interact with Feature Store APIs. We use HuggingFace [Datasets](https://huggingface.co/docs/datasets/) and [Transformers](https://huggingface.co/docs/transformers/index) libraries to tokenize the text and run training and inference. Please make sure that these libraries are installed. You can also install required dependencies by running following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset \n",
    "\n",
    "Before we begin, let's start with SageMaker imports and initiatiating SageMaker session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role() # or replace with role ARN in case of local enviornment\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_bucket_name = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to acquire initial dataset with IMDB reviews. For this we use HuggingFace `dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the dataset to Pandas `DataFrame` instance which is supported by Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dataset_df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we cast data types into supported ones by Feature StoreI. Note, that we also add metadata fields EventTime and ID. Both are required by Feature Store to support fast retrieval and feature versioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time_sec = int(round(time.time()))\n",
    "dataset_df[\"EventTime\"] = pd.Series([current_time_sec]*len(dataset_df), dtype=\"float64\")\n",
    "dataset_df[\"ID\"] = dataset_df.index\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].astype('string')\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].str.encode(\"utf8\")\n",
    "dataset_df[\"text\"] = dataset_df[\"text\"].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s download pre-trained tokenizer for Distilbert model and add new feature `tokenized-text` to our dataset. Note, that we cast tokenized text to string, as SageMaker Feature Store doesn’t support collection data types such as arrays or maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',proxies = {})\n",
    "\n",
    "dataset_df[\"tokenized-text\"] = tokenizer(dataset_df[\"text\"].tolist(), truncation=True, padding=True)[\"input_ids\"]\n",
    "dataset_df[\"tokenized-text\"] = dataset_df[\"tokenized-text\"].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we have a Pandas DataFrame object with the features we are looking to ingest into FeatureStore. Execute the cell below to preview resulting dataset with tokenized inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Data\n",
    "\n",
    "Next step is to provision Feature Store resources and prepare them for ingestion.  We start by configuring `feature group` and preparing `feature definitions`. Note, that since we stored our dataset in Pandas DataFrame, Feature Store can infer features definitions automatically based on dataframe data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "imdb_feature_group_name = \"imdb-reviews-tokenized\"\n",
    "\n",
    "imdb_feature_group = FeatureGroup(\n",
    "    name=imdb_feature_group_name, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "imdb_feature_group.load_feature_definitions(data_frame=dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now ready to create our  `feature group` - a logic unit which includes multiple data records and associated features. It may take several minutes to do so, hence, we add a waiter method. Since we are planning to use both online and offline storage, we set flag `enable_online_store=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_feature_group.create(\n",
    "    s3_uri=f\"s3://{s3_bucket_name}/{imdb_feature_group_name}\",\n",
    "    record_identifier_name=\"ID\",\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# A function to wait for FeatureGroup creation\n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    print(f'Initial status: {status}')\n",
    "    while status == 'Creating':\n",
    "        print(f'Waiting for feature group: {feature_group.name} to be created ...')\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get('FeatureGroupStatus')\n",
    "    if status != 'Created':\n",
    "        raise SystemExit(f'Failed to create feature group {feature_group.name}: {status}')\n",
    "    print(f'FeatureGroup {feature_group.name} was successfully created.')\n",
    "\n",
    "wait_for_feature_group_creation_complete(imdb_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have feature group created, we can ingest our dataset using `.ingest()` method. You can choose what's the maximum number of ingest processes can run in parallel at the same time via `max_processes`. Ingesting processes may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# To disable Tokenizer warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "imdb_feature_group.ingest(data_frame=dataset_df, max_processes=16, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is ingested, we can explore our dataset using SQL queries. Feature Store supports querying data using Amazon Athena SQL engine. See more details on Athena Quering capabilities [here](https://docs.aws.amazon.com/athena/latest/ug/ddl-sql-reference.html). For instance, we can run a query to understand if we are working with balanced dataset (in other words, dataset where number of classes for target label is approximately equal). The query below takes a moment to run, but in the end, you should get a count of labels in our dataset. Feel free to experiment with other queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_query = imdb_feature_group.athena_query()\n",
    "imdb_table_name = athena_query.table_name\n",
    "result = athena_query.run(f'SELECT \"label\", COUNT(\"label\") as \"Count\" FROM \"sagemaker_featurestore\".\"{imdb_table_name}\" group by \"label\";', output_location=f\"s3://{s3_bucket_name}/athena_output\")\n",
    "athena_query.wait()\n",
    "print(f\"Counting labels in dataset: \\n {athena_query.as_dataframe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's remember a S3 location where data is stored in Feature Store Offline Storage. We will use this as an input to our training job later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_uri = imdb_feature_group.describe()['OfflineStoreConfig'][\"S3StorageConfig\"][\"ResolvedOutputS3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Feature Store for Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data available in Feature Store, let’s train our binary classification model using data from Feature Store. Note, that for training job we will use `Feature Store Offline Storage`.\n",
    "\n",
    "### Preparing Training Script\n",
    "We already prepared a training script for our job. Run the cell below to review it. There are several key blocks in our training script:\n",
    "- as data is stored as parquet files in Offline Storage, we read data using Pandas `.from_parquet()` method (line #49). We then use resulting dataframe instance to instantiate `dataset` instance (line #51) which will be used during model training. \n",
    "- we instantiate DistilBert model for binary classification (lines #67-#73). HuggingFace handles downloading of model weights from its public Model Hub.\n",
    "- We use HuggingFace `Trainer` class to configure and execute model training (lines #75-#96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_sources/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training Job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have training script ready, we can configure and execute training job. We start by pinning versions of used frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "TRANSFORMER_VERSION = \"4.17.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to start training job. Feel free to experiment with hyperparameters of training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFace\n",
    "\n",
    "estimator = HuggingFace(\n",
    "    py_version=PYTHON_VERSION,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"1_sources\",\n",
    "    pytorch_version=PYTORCH_VERSION,\n",
    "    transformers_version=TRANSFORMER_VERSION,\n",
    "    hyperparameters={\n",
    "        \"model_name\":\"distilbert-base-uncased\",\n",
    "        \"train_batch_size\": 16,\n",
    "        \"epochs\": 3\n",
    "        # \"max_steps\": 100 # to shorten training cycle, remove in real scenario\n",
    "    },\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit(train_dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on hyperparameters selected (specifically, number of training steps or epochs) training may take some time. Feel free to set these parameters to lower values to expedite training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Feature Store for Inference\n",
    "\n",
    "Now let's see how we can use data stored in Feature Store at inference time. We will use for this `Feature Store Online Storage` to fetch records of our dataset and send tokenized text as an input to inference endpoint. We start by deploying trained model to SageMaker Real-Time endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimator.create_model(role=role, \n",
    "                               entry_point=\"inference.py\", \n",
    "                               source_dir=\"1_sources\",\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, let's fetch records from Online Storage and use these records as inputs for model inference. For this, AWS provides Feature Store Runtime client as part of `boto3` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fetch records using their record IDs. For test purposes, we retrieve first 3 records from Feature Store using their respective ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName':imdb_feature_group.name,\n",
    "            'RecordIdentifiersValueAsString': [\"0\", \"1\", \"2\"], # picking several records to run inference.\n",
    "            'FeatureNames': [\n",
    "                'tokenized-text', \"label\", 'text'\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process featured records according to model requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the inference payload\n",
    "labels = []\n",
    "input_ids = []\n",
    "texts = []\n",
    "\n",
    "for record in response[\"Records\"]:\n",
    "    for feature in record[\"Record\"]:\n",
    "        if feature[\"FeatureName\"]==\"label\":\n",
    "            labels.append(feature[\"ValueAsString\"])\n",
    "        if feature[\"FeatureName\"]==\"tokenized-text\":\n",
    "            list_of_str = feature[\"ValueAsString\"].strip(\"][\").split(\", \")\n",
    "            input_ids.append([int(el) for el in list_of_str])\n",
    "        if feature[\"FeatureName\"]==\"text\":\n",
    "            # list_of_str = feature[\"ValueAsString\"].strip(\"][\").split(\", \")\n",
    "            texts.append(feature[\"ValueAsString\"])    \n",
    "\n",
    "print(f\"Sample label value: {labels[0]}\")\n",
    "print(f\"Sample list of token ids:\\n{input_ids[0]}\")\n",
    "print(f\"Sample list of token ids:\\n{texts[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then send inference requests to SageMaker endpoint. Note, that depending how long your trained your model, inference results and model accuracy may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    prediction = predictor.predict([texts[i]])\n",
    "    print(f\"Sample index: {i}; predicted label: {prediction[0]['label']}; confidence score: {prediction[0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Execute the cell below to clean up all SageMaker resources and avoid any costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete SageMaker model and endpoint\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "model.delete_model()\n",
    "\n",
    "# Delete Feature Store resources\n",
    "imdb_feature_group.delete()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "117b28c900cff70d7dbffefa326a59f5b290ef63ff7a44883b4492d4cc48193c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('accelerate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
