{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Training Data From S3\n",
    "\n",
    "So far in the book we used Amazon S3 service to store our training datasets. By default, SageMaker downloads full dataset to each training node which can be problematic for large DL datasets (known as `FullySharded` distribution strategy). In previous example `Chapter04/2_Distributed_Data_Processing.ipynb` we also learned how to use `ShardedByKey` distribution strategy that will reduce the amount of data downloaded to each training node. However, that approach only reduces the amount of data that needs to be downloaded to your training nodes. For large datasets (100s+ gigabytes) it solves the problem only partially.\n",
    "\n",
    "Alternative approach to reduce training time is to stream data from Amazon S3 without downloading it upfront. There are several implementations of S3 data streaming provided by Amazon SageMaker: \n",
    "- Framework specific streaming implementations: TensorFlow `PipeModeDataset` and `S3 Plugin` for PyTorch \n",
    "- Framework agnostic `FastFile` mode  \n",
    "\n",
    "In this example we will learn how to use `PipeModeDataset` streaming feature to train TensorFlow model.  For this we will convert CIFAR-100 dataset into `TFRecords` format and then stream this dataset at training time using `PipeModeDataset` from SageMaker TensorFlow extyension library ([link](https://github.com/aws/sagemaker-tensorflow-extensions)).\n",
    "\n",
    "### Prerequisites \n",
    "To run this example you need to have `tensorflow` and `wget` packages installed. Feel free to run cell below to install requried dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data to TFRecords\n",
    "\n",
    "`PipeModeDataset` is an open-source implementation of TensorFlow Dataset API which allows to read SageMaker Pipe Mode channels. PipeModeDataset supports several formats of datasets, such as text line, RecordIO, and TFRecord. We chose to use TFRecord format in this example.\n",
    "\n",
    "We start by converting original dataset into TFRecord format. For this we prepared a conversion script `3_sources/generate_cifar100_records.py`. Here are several highlights from the script:\n",
    "- Method `download_and_extract()` (line #31) downloads and unarchives CIFAR-100 datasets.\n",
    "- Method `convert_to_tfrecord()` (line #62) iterates other dataset, for each a pair of images and labels into TensorFlow `Example` class, and writes batch of Example objects into single TFRecord file.\n",
    "- Methods `_int64_feature()` (line #38) and `_bytes_feature()` (line #42) convert images and class labels to expected types.\n",
    "\n",
    "Execute the cell below to review conversion script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_script = \"3_sources/generate_cifar100_records.py\"\n",
    "data_dir = \"cifar100_data\"\n",
    "\n",
    "! pygmentize -O linenos=1  $conversion_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to perform conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python  $conversion_script --data-dir $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once dataset is converted to TF Record format, we upload it to Amazon S3 location using SageMaker SDK `S3Uploader` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "dataset_uri = S3Uploader.upload(data_dir, \"s3://{}/tf-cifar10-example/data\".format(bucket))\n",
    "\n",
    "print(dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Training Script\n",
    "\n",
    "Next, we need to prepare training script. Execute the cell below to preview training script. Our training script largely follows a typical TensorFlow training script, however, there are several differences:\n",
    "- We import `PipeModeDataset` class from `sagemaker_tensorflow` (line #22) and use it as input dataset to our training. \n",
    "- Method `_dataset_parser()` (line #143) implements parsing logic for dataset.\n",
    "- Method `_input()` returns parsed data sample and classes and is used to instantiate training and evaluation datasets (lines #184-185)\n",
    "\n",
    "The rest of training script is similar to other Keras application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -O linenos=1  3_sources/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training Job\n",
    "\n",
    "Now, we are ready to run our training job which will stream data from S3 location. When configuring SageMaker training job, we need to explicitly specify that we want to use `PipeMode`. Prior to this we define hyperparameters and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters: {\n",
    "    \"batch-size\": 256,\n",
    "    \"epochs\": 10\n",
    "    }\n",
    "\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \".*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \".*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*\"},\n",
    "    {\n",
    "        \"Name\": \"validation:accuracy\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:loss\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"sec/steps\",\n",
    "        \"Regex\": \".* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to start the training job. Note, that we are setting `input_mode=\"Pipe\"` as part of estimator configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"train.py\", \n",
    "    source_dir=\"3_sources\", \n",
    "    metric_definitions=metric_definitions, \n",
    "    hyperparameters=hyperparameters, \n",
    "    role=role, \n",
    "    framework_version=\"1.15.2\", \n",
    "    py_version=\"py3\", \n",
    "    train_instance_count=1, \n",
    "    input_mode=\"Pipe\", \n",
    "    train_instance_type=\"ml.p2.xlarge\", \n",
    "    base_job_name=\"cifar100-tf\", \n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"train\": \"{}/train\".format(dataset_uri),\n",
    "    \"validation\": \"{}/validation\".format(dataset_uri),\n",
    "    \"eval\": \"{}/eval\".format(dataset_uri),\n",
    "}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now observe training job performance in AWS Console. You may notice that training job started faster as we avoided time on initial data download. Note, that since CIFAF100 dataset is relatively small, you may be not able to see any considerable decrease of training start time. However, with bigger datasets like COCO2017 you can expect to training time reduced by at least several of minutes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notice": "Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
