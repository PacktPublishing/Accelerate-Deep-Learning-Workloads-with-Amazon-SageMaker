{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to preprocess data in distributed fashion using SageMaker Processing.\n",
    "\n",
    "Download dataset from Kaggle (requires free account): https://www.kaggle.com/gpiosenka/100-bird-species/ and unzip it to local directory next to this notebook.\n",
    "\n",
    "To keep costs and timing of execution manageable, we will use only \"test\" split to produce augmented images. First, we start by uploading test split dataset to S3. It's convenient to use SageMaker Session upload functionality for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# role = get_execution_role() # TODO: uncomment it for final version\n",
    "role = \"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\"\n",
    "sess = sagemaker.Session()\n",
    "account_id = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "original_data_dir = \"315_birds\"\n",
    "split = \"test\"\n",
    "\n",
    "dataset_uri = S3Uploader.upload(f\"./{original_data_dir}/{split}\", f\"s3://{sess.default_bucket()}/{original_data_dir}/{split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_file = \"class_dict.csv\"\n",
    "\n",
    "class_dict_uri = S3Uploader.upload(f\"./{original_data_dir}/{class_dict_file}\", f\"s3://{sess.default_bucket()}/{original_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test split data has been  uploaded to s3://sagemaker-us-east-1-941656036254/315_birds/test/\n",
      "class dictionary has been  uploaded to s3://sagemaker-us-east-1-941656036254/315_birds/class_dict.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"{split} split data has been  uploaded to {dataset_uri}\")\n",
    "print(f\"class dictionary has been  uploaded to {class_dict_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling a processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in region us-east-1\n",
      "Image URI 941656036254.dkr.ecr.us-east-1.amazonaws.com/keras-processing:latest\n"
     ]
    }
   ],
   "source": [
    "! chmod +x build_and_push.sh\n",
    "! ./build_and_push.sh \"keras-processing\" \"latest\" \"2_dockerfile.processor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"keras-processing\"\n",
    "container_tag = \"latest\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{container_name}:{container_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  image-augmentation-2021-11-27-23-44-06-455\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-941656036254/315_birds/test/', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-941656036254/315_birds/class_dict.csv', 'LocalPath': '/opt/ml/processing/lookup', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-941656036254/image-augmentation-2021-11-27-23-44-06-455/output/output-1', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "............................\u001b[34m2021-11-27 23:48:32.160111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2021-11-27 23:48:32.160147: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34mNamespace(batch_size=32, data_location='/opt/ml/processing/input', lookup_location='/opt/ml/processing/lookup', max_augmentations=5, max_samples=10, output_location='/opt/ml/processing/output')\u001b[0m\n",
      "\u001b[34menviron({'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'HOSTNAME': 'ip-10-0-107-182.ec2.internal', 'AWS_REGION': 'us-east-1', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/DwsnUVwJRdh8_Wyz7svNLof7Foy_bWwpcM95dAomFn4', 'LANG': 'C.UTF-8', 'GPG_KEY': '0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D', 'PYTHON_VERSION': '3.7.12', 'PYTHON_PIP_VERSION': '21.2.4', 'PYTHON_SETUPTOOLS_VERSION': '57.5.0', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/3cb8888cc2869620f57d5d2da64da38f516078c7/public/get-pip.py', 'PYTHON_GET_PIP_SHA256': 'c518250e91a70d7b20cceb15272209a4ded2a0c263ae5776f129e0d9b5674309', 'PYTHONUNBUFFERED': 'TRUE', 'HOME': '/root', 'TF2_BEHAVIOR': '1'})\u001b[0m\n",
      "\u001b[34mFound 787 files belonging to 315 classes.\u001b[0m\n",
      "\u001b[34m2021-11-27 23:48:33.894544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2021-11-27 23:48:33.894580: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2021-11-27 23:48:33.894608: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-107-182.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2021-11-27 23:48:33.895006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34mStart image augmentation\u001b[0m\n",
      "\u001b[34mProcessing batch with index 0 out from 25\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:32.225213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:32.225249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[35mNamespace(batch_size=32, data_location='/opt/ml/processing/input', lookup_location='/opt/ml/processing/lookup', max_augmentations=5, max_samples=10, output_location='/opt/ml/processing/output')\u001b[0m\n",
      "\u001b[35menviron({'PATH': '/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'HOSTNAME': 'ip-10-0-89-137.ec2.internal', 'AWS_REGION': 'us-east-1', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/DwsnUVwJRdh8_Wyz7svNLof7Foy_bWwpcM95dAomFn4', 'LANG': 'C.UTF-8', 'GPG_KEY': '0D96DF4D4110E5C43FBFB17F2D347EA6AA65421D', 'PYTHON_VERSION': '3.7.12', 'PYTHON_PIP_VERSION': '21.2.4', 'PYTHON_SETUPTOOLS_VERSION': '57.5.0', 'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/3cb8888cc2869620f57d5d2da64da38f516078c7/public/get-pip.py', 'PYTHON_GET_PIP_SHA256': 'c518250e91a70d7b20cceb15272209a4ded2a0c263ae5776f129e0d9b5674309', 'PYTHONUNBUFFERED': 'TRUE', 'HOME': '/root', 'TF2_BEHAVIOR': '1'})\u001b[0m\n",
      "\u001b[35mFound 788 files belonging to 315 classes.\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:34.026527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:34.026567: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:34.026592: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-89-137.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[35m2021-11-27 23:48:34.026920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[35mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35mStart image augmentation\u001b[0m\n",
      "\u001b[35mProcessing batch with index 0 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 1 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 1 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 2 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 2 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 3 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 3 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 4 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 4 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 5 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 6 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 7 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 5 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 6 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 7 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 8 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 8 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 9 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 9 out from 25\u001b[0m\n",
      "\u001b[34mProcessing batch with index 10 out from 25\u001b[0m\n",
      "\u001b[35mProcessing batch with index 10 out from 25\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "lookup_location = f\"/opt/ml/processing/lookup\"\n",
    "data_location = \"/opt/ml/processing/input\"\n",
    "output_location = '/opt/ml/processing/output'\n",
    "\n",
    "sklearn_processor = Processor(image_uri=image_uri,\n",
    "                      role=role,\n",
    "                      instance_count=2,\n",
    "                      base_job_name=\"image-augmentation\",\n",
    "                      sagemaker_session=sess, \n",
    "                      instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "sklearn_processor.run(\n",
    "                      inputs=[\n",
    "                        ProcessingInput(\n",
    "                          source=dataset_uri,\n",
    "                          destination=data_location,\n",
    "                          s3_data_distribution_type=\"ShardedByS3Key\"),\n",
    "                        ProcessingInput(\n",
    "                          source=class_dict_uri,\n",
    "                          destination=lookup_location),\n",
    "\n",
    "                        ],\n",
    "                      outputs=[ProcessingOutput(source=output_location)],\n",
    "                      arguments = [\n",
    "                        \"--data_location\", data_location, \n",
    "                        \"--lookup_location\", lookup_location,\n",
    "                        \"--output_location\", output_location,\n",
    "                        \"--batch_size\", \"32\",\n",
    "                        \"--max_samples\", \"10\",\n",
    "                        \"--max_augmentations\", \"5\"\n",
    "                        ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
