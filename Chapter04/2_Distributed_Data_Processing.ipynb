{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Dataset Augmentation using SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to preprocess data in distributed fashion using SageMaker Processing capability.\n",
    "\n",
    "We will download CV dataset `450 Bird Species` which contains multiple images for each bird species. We then augmented original dataset with modified versions of images (rotated, cropped, resized) to increase dataset size and image variability. For image transformation we will use `Keras` module (a part of TensorFlow library). We will then run our processing job on multiple SageMaker compute nodes.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "In this example we will build processing container from scratch. Make sure that you have `docker` installed.\n",
    "\n",
    "## Getting Data\n",
    "Download dataset from Kaggle (requires free account): https://www.kaggle.com/gpiosenka/100-bird-species/ and unzip it to local directory next to this notebook. To keep costs and timing of execution manageable, we will use only `test` split to produce augmented images. First, we start by uploading test split dataset to S3. It's convenient to use SageMaker `S3 uploader` class for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "account_id = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we upload test split of original dataset and associated class dictionary to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "original_data_dir = \"450_birds\"\n",
    "split = \"test\"\n",
    "\n",
    "dataset_uri = S3Uploader.upload(f\"./{original_data_dir}/{split}\", f\"s3://{sess.default_bucket()}/{original_data_dir}/{split}\")\n",
    "\n",
    "class_dict_file = \"class_dict.csv\"\n",
    "\n",
    "class_dict_uri = S3Uploader.upload(f\"./{original_data_dir}/{class_dict_file}\", f\"s3://{sess.default_bucket()}/{original_data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{split} split data has been  uploaded to {dataset_uri}\")\n",
    "print(f\"class dictionary has been  uploaded to {class_dict_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Processing Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Processing provides two pre-built containers:\n",
    "- PySpark container with dependencies to run Spark computations \n",
    "- Scikit-learn container\n",
    "\n",
    "You can also provide BYO processing container with virtually any runtime configuration to run SageMaker Processing. In our example we will use TensorFlow image augmentation functionality, specifically `Keras` module. So we will build our processing container from scratch using `slim-buster` Python image as a base. We then install all required  Python dependencies and copy code processing code inside our container. Note, that SageMaker starts processing containers using `docker container name` command, hence, we need to specify entrypoint in our Dockerfile.\n",
    "\n",
    "Run the cell below to familiarize with processing container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -l docker 2_dockerfile.processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build our container and push it to Amazon ECR (a managed container registry from AWS). Execute cell below to authenticate your docker client in ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"keras-processing\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}\"\n",
    "\n",
    "! ./build_and_push.sh {image_name} 2_dockerfile.processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Processing Script\n",
    "\n",
    "As part of processing container we also need to provide script with processing logic. Execute the cell below to review processing code.\n",
    "\n",
    "Here are key highlights of processing script:\n",
    "- We use Keras `Dataset` class to load dataset from directory.\n",
    "- We use `ImageDataGenerator` class to generate modified versions of original images.\n",
    "- We then iterate over batches of data and save generated on the fly batches to disk according to expected dataset directory hiearachy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -O linenos=1  2_sources/processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Processing Jobs\n",
    "Once we have BYO container and processing code, we are ready to schedule processing job. First, we need to instantiate `Processor` object with basic job configuration, such as number and type of instances and container image. In our example, we want to run distirbute our processing task across several compute nodes. We set number of instances to `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "\n",
    "sklearn_processor = Processor(image_uri=image_uri,\n",
    "                      role=role,\n",
    "                      instance_count=2,\n",
    "                      base_job_name=\"image-augmentation\",\n",
    "                      sagemaker_session=sess, \n",
    "                      instance_type=\"ml.m5.xlarge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure locations for input and output data using `ProcessingInput` class. Note, that in this example we have two types of input data which require slightly different configuration:\n",
    "- Image dataset. Since we want to use multiple nodes in our processing tasks, we need to split evently input dataset images between our processing nodes. To achieve this, we set `s3_data_distribution_type=\"ShardedByS3Key\"`. SageMaker will attempt to evenly split S3 objects (in case of our dataset - images) between processing nodes.\n",
    "- Class lookup set. We need to have this file on each node. For this, we set `s3_data_distribution_type=\"FullyReplicated\"`, so SageMaker automatically downloads full copy to each compute node.\n",
    "\n",
    "Execute cell below to start SageMaker Processing Job. It will take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_location = f\"/opt/ml/processing/lookup\"\n",
    "data_location = \"/opt/ml/processing/input\"\n",
    "output_location = '/opt/ml/processing/output'\n",
    "\n",
    "\n",
    "sklearn_processor.run(\n",
    "                      inputs=[\n",
    "                        ProcessingInput(\n",
    "                          source=dataset_uri,\n",
    "                          destination=data_location,\n",
    "                          s3_data_distribution_type=\"ShardedByS3Key\"),\n",
    "                        ProcessingInput(\n",
    "                          source=class_dict_uri,\n",
    "                          destination=lookup_location,\n",
    "                          s3_data_distribution_type=\"FullyReplicated\"\n",
    "                          ),\n",
    "\n",
    "                        ],\n",
    "                      outputs=[ProcessingOutput(source=output_location)],\n",
    "                      arguments = [\n",
    "                        \"--data_location\", data_location, \n",
    "                        \"--lookup_location\", lookup_location,\n",
    "                        \"--output_location\", output_location,\n",
    "                        \"--batch_size\", \"32\",\n",
    "                        \"--max_samples\", \"10\",\n",
    "                        \"--max_augmentations\", \"5\"\n",
    "                        ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example should give you an intuition how SageMaker Processing can be used for your data processing needs. At the same time SageMaker Processing is flexible enough to run any arbitrary tasks, such as batch inference, data aggregation and analytics, and others.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "117b28c900cff70d7dbffefa326a59f5b290ef63ff7a44883b4492d4cc48193c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('accelerate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
