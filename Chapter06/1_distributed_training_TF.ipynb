{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel Training with TensorFlow2\n",
    "\n",
    "In this notebook we will learn how to engineer distributed data parallel training job using TensorFlow2 `Allreduce` implementation as well as framework-agnostic `Horovod` package. By the end of this practical example, you will be able to compare both implementations. As a test task we choose everyone's favorite MNIST dataset and train small computer vision model to solve image classification task. We will use convenient `Keras` API to build and train model and evaluate results.\n",
    "\n",
    "Note, that we use simple small-scale problem in this example, and you won't unlikely be used to draw conclusions about training efficiencies of both approaches.\n",
    "\n",
    "## TensorFlow MultiWorkerMirroredStrategy Training\n",
    "TensorFlow2 provides several native implementations of data parallel training known as `strategies`. In this examples we will use synchronous multi-GPU multi-node Allreduce implementation called `MultiWorkerMirroredStrategy` (`\"MWMS\"`). Refer to [this overview](https://www.tensorflow.org/guide/distributed_training) of data parallel strategies if you want to learn about others. \n",
    "\n",
    "As always, we start with necessary imports and basic SageMaker training configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/tf-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopting Training Script for MWMS\n",
    "\n",
    "Using MWMS strategy requries certain changes in your training script. We highlight key modifications below. Full sources are available here: `1_sources/train_ms.py`\n",
    "\n",
    "**1. Training Cluster Configuration**\n",
    "\n",
    "MWMS is not natively supported by Amazon SageMaker, so we need to correctly configure MWMS environment in SageMaker. TF2 uses environment variable called `TF_CONFIG` to represent cluster configuration. This configuration is then used to start training processes. You can read about building `TF_CONFIG` variable [here](https://www.tensorflow.org/guide/distributed_training#TF_CONFIG). We use `_build_tf_config()` method below to setup this variable. Note, that we are using SageMaker environment variables `SM_HOSTS` and `SM_CURRENT_HOST` for it.\n",
    "\n",
    "```python\n",
    "def _build_tf_config():\n",
    "\n",
    "    hosts = json.loads(os.getenv(\"SM_HOSTS\"))\n",
    "    current_host = os.getenv(\"SM_CURRENT_HOST\")\n",
    "\n",
    "    workers = hosts\n",
    "\n",
    "    def host_addresses(hosts, port=7777):\n",
    "        return [\"{}:{}\".format(host, port) for host in hosts]\n",
    "\n",
    "    tf_config = {\"cluster\": {}, \"task\": {}}\n",
    "    tf_config[\"cluster\"][\"worker\"] = host_addresses(workers)\n",
    "    tf_config[\"task\"] = {\"index\": workers.index(current_host), \"type\": \"worker\"}\n",
    "\n",
    "    os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\n",
    "```\n",
    "\n",
    "\n",
    "By default in this we use sample two `p2.xlarge` instances with total world size of just 2 training processes. So `_build_tf_config()` will produce following `TF_CONFIG` on rank=0 node:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"cluster\": \n",
    "    {\n",
    "        \"worker\": [\"algo-1:7777\", \"algo-2:7777\"]},\n",
    "        \"task\": {\"index\": 0, \"type\": \"worker\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Once TF config is properly configured, TensorFlow2 should be able to start training processes on all nodes and utilize all available GPU devices (this is a default setting but you can provide a list of specific GPU devices to use too).\n",
    "\n",
    "To complete cluster setup we also need to make sure that NCCL environment is properly configurate (see `_set_nccl_environment()` method) and that all nodes in cluster can communicate with each other (see `_dns_lookup()` method). Note, that these methods are required because TensorFlow2 strategies are not officially supported by SageMaker. For supported data parallel implementations, SageMaker provides these utilities out of the box and run them as part of training cluster initiation.\n",
    "\n",
    "\n",
    "**2. Enabling MWMS Strategy**\n",
    "\n",
    "To use MWMS we start by initiating strategy object like below. Please note, that here we explicitly set communication backend to `AUTO` which means that TF2 will identify which backend to use. You can also define a specific backend manually. `NCCL` and custom `RING` backends are available for GPU devices.\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation=tf.distribute.experimental.CollectiveCommunication.AUTO\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "Once strategy is correctly initiated, you can confirm your cluster configuration by checking properly `strategy.num_replicas_in_sync` which will return your world size. It should match with number of GPUs per node *multiplied by number of nodes.\n",
    "\n",
    "In this example we are using Keras API which fully supports MWMS, simplifying our training script. For instance, to create model copies on all workers you just need to initiate your Keras model within strategy.scope like below:\n",
    "\n",
    "```python\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_and_compile_cnn_model()\n",
    "```\n",
    "\n",
    "MWMS also automatically shards your dataset based on world sze. You only need to setup proper global batch size like below. Note, that auto sharding can be turned out if some custom sharding logic is needed.\n",
    "\n",
    "```python\n",
    "    global_batch_size = args.batch_size_per_device * _get_world_size()\n",
    "    multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "```\n",
    "\n",
    "The rest of training script is similar to your single process Keras training script. As you can see, using MWMS is quite straighforward, and TensorFlow2 does a good job abstracting complexities from developers but at the same time giving flexibility to adjust default settings if needed.\n",
    "\n",
    "Run the cell below to review a full training script. Additionally, we also have `mnist_setup.py` script which handles dataset download and model compilations. Feel free to review it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_sources/train_ms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Running SageMaker job\n",
    "\n",
    "Now we are ready to run Data Parallel training on SageMaker. \n",
    "\n",
    "In cell  below we define TF version (2.8), Python version (3.9), instance type and number of instances. Additionally, we also pass several training hyperparameters. Since MNIST dataset is downloaded from internet as part of our training script, no data is passed to `estimator_ms.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "TENSORFLOW_VERSION = \"2.8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ps_instance_type = 'ml.p2.xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "hyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\n",
    "\n",
    "estimator_ms = TensorFlow(\n",
    "                       source_dir='1_sources',\n",
    "                       entry_point='train_ms.py', \n",
    "                       role=role,\n",
    "                       framework_version=TENSORFLOW_VERSION,\n",
    "                       py_version=PYTHON_VERSION,\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       )\n",
    "\n",
    "estimator_ms.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 10-12 minutes with default settings. Feel free to experiment with number of nodes in cluster and instasnce types and observe changes `TF_CONFIG`, training speed and convergence. \n",
    "\n",
    "## TensorFlow Horovod Training\n",
    "\n",
    "Horovod is an open-source framework for data parallel training whic supports TensorFlow1, TensorFlow2, PyTorch, and MXNet. Given its popularity, Amazon SageMaker provides native support for Horovod as well, making using Horovod even simplier than native TensorFlow2 strategies. \n",
    "\n",
    "We will re-use same MNIST problem and `Keras` API.\n",
    "\n",
    "### Configuring Horovod cluster\n",
    "\n",
    "Unlike in case of TensorFlow2 MWMS, we don't have to configure and setup training cluster in training script since Horovod is supported by SageMaker. Horovod cluster configuration is done on level of `Tensorflow.Estimator` API via `distributions` object like below:\n",
    "\n",
    "```python\n",
    "distribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\n",
    "```\n",
    "Note parameter `processes_per_host` which should match number of GPUs on chosen instance type. You can also set `custom_mpi_options` as needed which SageMaker will pass to `mpirun` run utility. See list of supported MPI options [here](https://www.open-mpi.org/doc/v4.0/man1/mpirun.1.php).\n",
    "\n",
    "### Updating Training Script\n",
    "\n",
    "Horovod requries  several modifications the training script. Let's review them. You can find full training script in `1_sources/train_hvd.py`. \n",
    "\n",
    "\n",
    "\n",
    "**1. Horovod Initialization**\n",
    "\n",
    "We start by initiating Horovod in training script via `_initiate_hvd()` method.We also need to associate Horovod training processes with available GPU devices (one device per process).\n",
    "\n",
    "```python\n",
    "def _initiate_hvd():\n",
    "    # Horovod: initialize Horovod.\n",
    "    hvd.init()\n",
    "\n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "```\n",
    "\n",
    "**2. Dataset Configuration**\n",
    "\n",
    "Next we need to shard our dataset based on world size, so each process can get a slice of data based on its global rank. For this we use `shard` method of `TensorFlow.Dataset`. Note that we are getting local and global ranks of given training process using Horovod properties `size()` and `rank()`.\n",
    "\n",
    "```python\n",
    "train_dataset = train_dataset.shard(hvd.size(), hvd.rank())\n",
    "```\n",
    "\n",
    "\n",
    "**3. Adding Training Callbacks**\n",
    "\n",
    "Next we need to use Horovod `DistributedOptimizer` wrapper to enable distributed gradient update. Note, that we are wrapping instance of native TF2 optimizer.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001 * hvd.size())\n",
    "optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "```\n",
    "\n",
    "Lastly we use use special Keras callbacks:\n",
    "- `hvd.callbacks.BroadcastGlobalVariablesCallback(0)` to distribute initial variables from rank=0 process to other training processes in the cluster.\n",
    "- `hvd.callbacks.MetricAverageCallback()` to calculate global average of metrics across all training processes.\n",
    "\n",
    "These callbacks then passed to `model.fit()` method like below:\n",
    "```python\n",
    "    hvd_model.fit(\n",
    "        shareded_by_rank_dataset,\n",
    "        epochs=args.epochs,\n",
    "        steps_per_epoch=args.steps_per_epoch // hvd.size(),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "```\n",
    "\n",
    "These are minimal additions to your training script which allows to use Horovod. Execute the cell below to review full training script listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_sources/train_hvd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing SageMaker job\n",
    "\n",
    "SageMaker Training job configuration is similar to MWMS example with exception of `distributions` parameter above which we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ps_instance_type = 'ml.p2.xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "distribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\n",
    "\n",
    "hyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\n",
    "\n",
    "estimator_hvd = TensorFlow(\n",
    "                       source_dir='1_sources',\n",
    "                       entry_point='train_hvd.py', \n",
    "                       role=role,\n",
    "                       framework_version='2.8',\n",
    "                       py_version='py39',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                    #   model_dir = \"/opt/ml/model\",\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       distribution=distribution\n",
    "                       )\n",
    "\n",
    "estimator_hvd.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We implemented minimal viable examples of data parallel training jobs using TensorFlow2 MultiWorkerMirrored Strategy and TensorFlow2 Horovod. Now you should have some practical experience in developing baseline training jobs. There are certainly more knobs and capabilities to explore of both Allreduce implementations which we encorage to explore and try on your real-life use cases."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
