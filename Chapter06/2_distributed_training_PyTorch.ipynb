{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel Training with PyTorch DDP\n",
    "\n",
    "In this notebook we will learn how to engineer data parallel job using PyTorch Distributed Data Parallel (`DDP`). While SageMaker doesn’t support PyTorch DDP natively, it’s possible to run DDP training jobs on SageMaker. This notebook and associated code assets provides such implementation.\n",
    "\n",
    "As a trianing task, we will finetune pretrained Resnet18 model to classify ants and bees. We use open-source **Hymenoptera dataset**. We use data parallel to distribute task between 2 `p2.xlarge` instances with single GPU device each. Feel free to change modify number and type of instances in training cluster and observe how this change training speed. Note, that this is a small-scale training and will not be indicative of training efficiency on real-life tasks. \n",
    "\n",
    "We start with necessary imports and basic SageMaker training configs. Then we download, unzip, and upload dataset to Amazon S3 bucket. Note, it may take several minutes to complete these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket:\n",
      "sagemaker-us-east-1-941656036254\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role() # replace it with role ARN if you are not using SageMaker Notebook or Studio environments.\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/pytorch-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hymenoptera_data.zip\n",
      "   creating: hymenoptera_data/\n",
      "   creating: hymenoptera_data/train/\n",
      "   creating: hymenoptera_data/train/ants/\n",
      "  inflating: hymenoptera_data/train/ants/0013035.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1030023514_aad5c608f9.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1095476100_3906d8afde.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1099452230_d1949d3250.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/116570827_e9c126745d.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1225872729_6f0856588f.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1262877379_64fcada201.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1269756697_0bce92cdab.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1286984635_5119e80de1.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/132478121_2a430adea2.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1360291657_dc248c5eea.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1368913450_e146e2fb6d.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1473187633_63ccaacea6.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/148715752_302c84f5a4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1489674356_09d48dde0a.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/149244013_c529578289.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/150801003_3390b73135.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/150801171_cd86f17ed8.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/154124431_65460430f2.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/162603798_40b51f1654.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1660097129_384bf54490.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/167890289_dd5ba923f3.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1693954099_46d4c20605.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/175998972.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/178538489_bec7649292.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1804095607_0341701e1c.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1808777855_2a895621d7.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/188552436_605cc9b36b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1917341202_d00a7f9af5.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/1924473702_daa9aacdbe.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/196057951_63bf063b92.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/196757565_326437f5fe.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/201558278_fe4caecc76.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/201790779_527f4c0168.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2019439677_2db655d361.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/207947948_3ab29d7207.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/20935278_9190345f6b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/224655713_3956f7d39a.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2265824718_2c96f485da.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2265825502_fff99cfd2d.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/226951206_d6bf946504.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2278278459_6b99605e50.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2288450226_a6e96e8fdf.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2288481644_83ff7e4572.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/2292213964_ca51ce4bef.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/24335309_c5ea483bb8.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/245647475_9523dfd13e.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/255434217_1b2b3fe0a4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/258217966_d9d90d18d3.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/275429470_b2d7d9290b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/28847243_e79fe052cd.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/318052216_84dff3f98a.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/334167043_cbd1adaeb9.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/339670531_94b75ae47a.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/342438950_a3da61deab.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/36439863_0bec9f554f.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/374435068_7eee412ec4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/382971067_0bfd33afe0.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/384191229_5779cf591b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/386190770_672743c9a7.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/392382602_1b7bed32fa.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/403746349_71384f5b58.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/408393566_b5b694119b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/424119020_6d57481dab.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/424873399_47658a91fb.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/450057712_771b3bfc91.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/45472593_bfd624f8dc.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/459694881_ac657d3187.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/460372577_f2f6a8c9fc.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/460874319_0a45ab4d05.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/466430434_4000737de9.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/470127037_513711fd21.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/474806473_ca6caab245.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/475961153_b8c13fd405.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/484293231_e53cfc0c89.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/49375974_e28ba6f17e.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/506249802_207cd979b4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/506249836_717b73f540.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/512164029_c0a66b8498.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/512863248_43c8ce579b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/518773929_734dbc5ff4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/522163566_fec115ca66.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/522415432_2218f34bf8.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/531979952_bde12b3bc0.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/533848102_70a85ad6dd.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/535522953_308353a07c.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/540889389_48bb588b21.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/541630764_dbd285d63c.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/543417860_b14237f569.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/560966032_988f4d7bc4.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/5650366_e22b7e1065.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/6240329_72c01e663e.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/6240338_93729615ec.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/649026570_e58656104b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/662541407_ff8db781e7.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/67270775_e9fdf77e9d.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/6743948_2b8c096dda.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/684133190_35b62c0c1d.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/69639610_95e0de17aa.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/707895295_009cf23188.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/7759525_1363d24e88.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/795000156_a9900a4a71.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/822537660_caf4ba5514.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/82852639_52b7f7f5e3.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/841049277_b28e58ad05.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/886401651_f878e888cd.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/892108839_f1aad4ca46.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/938946700_ca1c669085.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/957233405_25c1d1187b.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/9715481_b3cb4114ff.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/998118368_6ac1d91f81.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/ant photos.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/Ant_1.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/army-ants-red-picture.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/formica.jpeg  \n",
      "  inflating: hymenoptera_data/train/ants/hormiga_co_por.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/imageNotFound.gif  \n",
      "  inflating: hymenoptera_data/train/ants/kurokusa.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/MehdiabadiAnt2_600.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/Nepenthes_rafflesiana_ant.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/swiss-army-ant.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/termite-vs-ant.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/trap-jaw-ant-insect-bg.jpg  \n",
      "  inflating: hymenoptera_data/train/ants/VietnameseAntMimicSpider.jpg  \n",
      "   creating: hymenoptera_data/train/bees/\n",
      "  inflating: hymenoptera_data/train/bees/1092977343_cb42b38d62.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1093831624_fb5fbe2308.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1097045929_1753d1c765.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1232245714_f862fbe385.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/129236073_0985e91c7d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1295655112_7813f37d21.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/132511197_0b86ad0fff.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/132826773_dbbcb117b9.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/150013791_969d9a968b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1508176360_2972117c9d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/154600396_53e1252e52.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/16838648_415acd9e3f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1691282715_0addfdf5e8.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/17209602_fe5a5a746f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/174142798_e5ad6d76e0.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1799726602_8580867f71.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/1807583459_4fe92b3133.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/196430254_46bd129ae7.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/196658222_3fffd79c67.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/198508668_97d818b6c4.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2031225713_50ed499635.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2037437624_2d7bce461f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2053200300_8911ef438a.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/205835650_e6f2614bee.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/208702903_42fb4d9748.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2227611847_ec72d40403.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2321139806_d73d899e66.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2330918208_8074770c20.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2345177635_caf07159b3.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2358061370_9daabbd9ac.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2364597044_3c3e3fc391.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2384149906_2cd8b0b699.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2397446847_04ef3cd3e1.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2405441001_b06c36fa72.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2445215254_51698ff797.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2452236943_255bfd9e58.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2467959963_a7831e9ff0.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2470492904_837e97800d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2477324698_3d4b1b1cab.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2477349551_e75c97cf4d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2486729079_62df0920be.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2486746709_c43cec0e42.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2493379287_4100e1dacc.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2495722465_879acf9d85.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2528444139_fa728b0f5b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2538361678_9da84b77e3.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2551813042_8a070aeb2b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2580598377_a4caecdb54.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2601176055_8464e6aa71.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2610833167_79bf0bcae5.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2610838525_fe8e3cae47.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2617161745_fa3ebe85b4.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2625499656_e3415e374d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2634617358_f32fd16bea.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2638074627_6b3ae746a0.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2645107662_b73a8595cc.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2651621464_a2fa8722eb.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2652877533_a564830cbf.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/266644509_d30bb16a1b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2683605182_9d2a0c66cf.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2704348794_eb5d5178c2.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2707440199_cd170bd512.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2710368626_cb42882dc8.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2722592222_258d473e17.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2728759455_ce9bb8cd7a.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2756397428_1d82a08807.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2765347790_da6cf6cb40.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2781170484_5d61835d63.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/279113587_b4843db199.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2792000093_e8ae0718cf.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2801728106_833798c909.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2822388965_f6dca2a275.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2861002136_52c7c6f708.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2908916142_a7ac8b57a8.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/29494643_e3410f0d37.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2959730355_416a18c63c.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/2962405283_22718d9617.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3006264892_30e9cced70.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3030189811_01d095b793.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3030772428_8578335616.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3044402684_3853071a87.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3074585407_9854eb3153.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3079610310_ac2d0ae7bc.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3090975720_71f12e6de4.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/3100226504_c0d4f1e3f1.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/342758693_c56b89b6b6.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/354167719_22dca13752.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/359928878_b3b418c728.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/365759866_b15700c59b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/36900412_92b81831ad.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/39672681_1302d204d1.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/39747887_42df2855ee.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/421515404_e87569fd8b.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/444532809_9e931e2279.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/446296270_d9e8b93ecf.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/452462677_7be43af8ff.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/452462695_40a4e5b559.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/457457145_5f86eb7e9c.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/465133211_80e0c27f60.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/469333327_358ba8fe8a.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/472288710_2abee16fa0.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/473618094_8ffdcab215.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/476347960_52edd72b06.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/478701318_bbd5e557b8.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/507288830_f46e8d4cb2.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/509247772_2db2d01374.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/513545352_fd3e7c7c5d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/522104315_5d3cb2758e.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/537309131_532bfa59ea.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/586041248_3032e277a9.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/760526046_547e8b381f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/760568592_45a52c847f.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/774440991_63a4aa0cbe.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/85112639_6e860b0469.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/873076652_eb098dab2d.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/90179376_abc234e5f4.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/92663402_37f379e57a.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/95238259_98470c5b10.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/969455125_58c797ef17.jpg  \n",
      "  inflating: hymenoptera_data/train/bees/98391118_bdb1e80cce.jpg  \n",
      "   creating: hymenoptera_data/val/\n",
      "   creating: hymenoptera_data/val/ants/\n",
      "  inflating: hymenoptera_data/val/ants/10308379_1b6c72e180.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1053149811_f62a3410d3.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1073564163_225a64f170.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1119630822_cd325ea21a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1124525276_816a07c17f.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/11381045_b352a47d8c.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/119785936_dd428e40c3.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1247887232_edcb61246c.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1262751255_c56c042b7b.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1337725712_2eb53cd742.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1358854066_5ad8015f7f.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1440002809_b268d9a66a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/147542264_79506478c2.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/152286280_411648ec27.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/153320619_2aeb5fa0ee.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/153783656_85f9c3ac70.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/157401988_d0564a9d02.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/159515240_d5981e20d1.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/161076144_124db762d6.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/161292361_c16e0bf57a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/170652283_ecdaff5d1a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/17081114_79b9a27724.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/172772109_d0a8e15fb0.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/1743840368_b5ccda82b7.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/181942028_961261ef48.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/183260961_64ab754c97.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2039585088_c6f47c592e.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/205398178_c395c5e460.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/208072188_f293096296.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/209615353_eeb38ba204.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2104709400_8831b4fc6f.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/212100470_b485e7b7b9.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2127908701_d49dc83c97.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2191997003_379df31291.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2211974567_ee4606b493.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2219621907_47bc7cc6b0.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2238242353_52c82441df.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/2255445811_dabcdf7258.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/239161491_86ac23b0a3.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/263615709_cfb28f6b8e.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/308196310_1db5ffa01b.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/319494379_648fb5a1c6.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/35558229_1fa4608a7a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/412436937_4c2378efc2.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/436944325_d4925a38c7.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/445356866_6cb3289067.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/459442412_412fecf3fe.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/470127071_8b8ee2bd74.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/477437164_bc3e6e594a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/488272201_c5aa281348.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/502717153_3e4865621a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/518746016_bcc28f8b5b.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/540543309_ddbb193ee5.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/562589509_7e55469b97.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/57264437_a19006872f.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/573151833_ebbc274b77.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/649407494_9b6bc4949f.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/751649788_78dd7d16ce.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/768870506_8f115d3d37.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/800px-Meat_eater_ant_qeen_excavating_hole.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/8124241_36b290d372.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/8398478_50ef10c47a.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/854534770_31f6156383.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/892676922_4ab37dce07.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/94999827_36895faade.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/Ant-1818.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/ants-devouring-remains-of-large-dead-insect-on-red-tile-in-Stellenbosch-South-Africa-closeup-1-DHD.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/desert_ant.jpg  \n",
      "  inflating: hymenoptera_data/val/ants/F.pergan.28(f).jpg  \n",
      "  inflating: hymenoptera_data/val/ants/Hormiga.jpg  \n",
      "   creating: hymenoptera_data/val/bees/\n",
      "  inflating: hymenoptera_data/val/bees/1032546534_06907fe3b3.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/10870992_eebeeb3a12.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1181173278_23c36fac71.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1297972485_33266a18d9.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1328423762_f7a88a8451.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1355974687_1341c1face.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/144098310_a4176fd54d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1486120850_490388f84b.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/149973093_da3c446268.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/151594775_ee7dc17b60.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/151603988_2c6f7d14c7.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1519368889_4270261ee3.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/152789693_220b003452.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/177677657_a38c97e572.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/1799729694_0c40101071.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/181171681_c5a1a82ded.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/187130242_4593a4c610.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/203868383_0fcbb48278.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2060668999_e11edb10d0.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2086294791_6f3789d8a6.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2103637821_8d26ee6b90.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2104135106_a65eede1de.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/215512424_687e1e0821.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2173503984_9c6aaaa7e2.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/220376539_20567395d8.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/224841383_d050f5f510.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2321144482_f3785ba7b2.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/238161922_55fa9a76ae.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2407809945_fb525ef54d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2415414155_1916f03b42.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2438480600_40a1249879.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2444778727_4b781ac424.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2457841282_7867f16639.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2470492902_3572c90f75.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2478216347_535c8fe6d7.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2501530886_e20952b97d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2506114833_90a41c5267.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2509402554_31821cb0b6.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2525379273_dcb26a516d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/26589803_5ba7000313.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2668391343_45e272cd07.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2670536155_c170f49cd0.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2685605303_9eed79d59d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2702408468_d9ed795f4f.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2709775832_85b4b50a57.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2717418782_bd83307d9f.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/272986700_d4d4bf8c4b.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2741763055_9a7bb00802.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2745389517_250a397f31.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2751836205_6f7b5eff30.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2782079948_8d4e94a826.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2809496124_5f25b5946a.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2815838190_0a9889d995.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2841437312_789699c740.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/2883093452_7e3a1eb53f.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/290082189_f66cb80bfc.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/296565463_d07a7bed96.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/3077452620_548c79fda0.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/348291597_ee836fbb1a.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/350436573_41f4ecb6c8.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/353266603_d3eac7e9a0.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/372228424_16da1f8884.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/400262091_701c00031c.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/416144384_961c326481.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/44105569_16720a960c.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/456097971_860949c4fc.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/464594019_1b24a28bb1.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/485743562_d8cc6b8f73.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/540976476_844950623f.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/54736755_c057723f64.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/57459255_752774f1b2.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/576452297_897023f002.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/586474709_ae436da045.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/590318879_68cf112861.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/59798110_2b6a3c8031.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/603709866_a97c7cfc72.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/603711658_4c8cd2201e.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/65038344_52a45d090d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/6a00d8341c630a53ef00e553d0beb18834-800wi.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/72100438_73de9f17af.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/759745145_e8bc776ec8.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/936182217_c4caa5222d.jpg  \n",
      "  inflating: hymenoptera_data/val/bees/abeja.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Downloading dataset and unzipping it locally\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = sagemaker_session.upload_data(path=\"./hymenoptera_data\", key_prefix=\"hymenoptera_data\")\n",
    "print(f\"S3 location of dataset {data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching Training Processes\n",
    "\n",
    "Amazon SageMaker has no out-of-the-box support for PyTorch DDP training. Specifically, it doesn’t know how to start distributed DDP processes in training cluster. Hence, we need to develop a launching utility to perform this function. This utility is quite simple and then can be re-used for any other DDP-based training jobs.\n",
    "\n",
    "In launcher script we use DDP module `torch.distributed.run` which simplifies spawning training processes in cluster.  As part of launcher script, we need to collect information about training world, specifically number of nodes and GPUs devices in cluster as well as identify node which will act as master coordinator. Then torch.distributed.run will spawn multiple training processes. \n",
    "\n",
    "\n",
    "Let’s highlight several key areas in our launcher script. First, we need to collect information about SageMaker training cluster. For this we use environmental variables set by SageMaker automatically.\n",
    "\n",
    "```python\n",
    "    nodes = json.loads(os.getenv(\"SM_HOSTS\"))\n",
    "    nnodes = len(nodes)\n",
    "    node_rank = nodes.index(os.getenv(\"SM_CURRENT_HOST\"))\n",
    "    nproc_per_node = os.getenv(\"SM_NUM_GPUS\", 1)\n",
    "Next we need to form command line to start torch.distributed.run:\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"torch.distributed.run\",\n",
    "        f\"--nproc_per_node={nproc_per_node}\",\n",
    "        f\"--nnodes={str(nnodes)}\",\n",
    "        f\"--node_rank={node_rank}\",\n",
    "        f\"--rdzv_id={os.getenv('SAGEMAKER_JOB_NAME')}\",\n",
    "        \"--rdzv_backend=c10d\",\n",
    "        f\"--rdzv_endpoint={nodes[0]}:{RDZV_PORT}\",\n",
    "        distr_args.train_script,\n",
    "    ]\n",
    "    # Adding training hyperparameters which will be then passed in training script\n",
    "    cmd.extend(training_hyperparameters)\n",
    "```\n",
    "Note, that we are adding training hyperparameters “as is” in the end of command line. These arguments are not handled by launcher, but by training script to configure training. Lastly, we use Python subprocess.Popen to start torch.distributed.run utility as a module:\n",
    "\n",
    "```python\n",
    "    process = subprocess.Popen(cmd, env=os.environ)\n",
    "    process.wait()\n",
    "    if process.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "Note, that we are copying environment variables to subprocesses to preserve all SageMaker variables. If spawned process returns non-zero code (an indication of error), we then raise exception to propagate error code to SageMaker control plane.\n",
    "```\n",
    "\n",
    "Summarizing, our launcher utility is responsible for collecting training cluster configuration and then starting torch.distributed.run on each node. The utility then takes care of starting multiple training processes.\n",
    "\n",
    "Run cell below to review full listing of launcher utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This module gathers requirements parameters of pytorch distirbuted training world\u001b[39;49;00m\n",
      "\u001b[37m# from environmental variable propagated by DSP for Pytorch Distributed job type.\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# The module is intended to be light-weight and rely exclusively on native torch distributed utility:\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/pytorch/pytorch/blob/master/torch/distributed/run.py\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser, REMAINDER\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "LOGGER = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# port for distributed DDP processes to communicate\u001b[39;49;00m\n",
      "RDZV_PORT = \u001b[33m\"\u001b[39;49;00m\u001b[33m7777\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    parser = ArgumentParser(\n",
      "        description=\u001b[33m\"\u001b[39;49;00m\u001b[33mCustom arg parser. Using it to get reference to train script.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-script\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrain script to run in distributed mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    distr_args, training_hyperparameters = parse_args()\n",
      "\n",
      "    \u001b[37m# world size in terms of number of processes across all workers\u001b[39;49;00m\n",
      "    nodes = json.loads(os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    nnodes = \u001b[36mlen\u001b[39;49;00m(nodes)\n",
      "    node_rank = nodes.index(os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    nproc_per_node = os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Construct command line to to launch training processes using torch.distributed.run\u001b[39;49;00m\n",
      "    cmd = [\n",
      "        sys.executable,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtorch.distributed.run\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--nproc_per_node=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnproc_per_node\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--nnodes=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mstr\u001b[39;49;00m(nnodes)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--node_rank=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnode_rank\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--rdzv_id=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mos.getenv(\u001b[33m'\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--rdzv_backend=c10d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--rdzv_endpoint=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnodes[\u001b[34m0\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mRDZV_PORT\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        distr_args.train_script,\n",
      "    ]\n",
      "\n",
      "    \u001b[37m# Adding training hyperparameters which will be then passed in training script\u001b[39;49;00m\n",
      "    cmd.extend(training_hyperparameters)\n",
      "\n",
      "    LOGGER.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCommand line to be executed on each node once:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcmd\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Spawning DDP launcher process which will then spawn multiple child processes (one process per GPU)\u001b[39;49;00m\n",
      "    process = subprocess.Popen(cmd, env=os.environ)\n",
      "    process.wait()\n",
      "    \u001b[34mif\u001b[39;49;00m process.returncode != \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 2_sources/launcher.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adopting Training Script For DDP\n",
    "\n",
    "To use DDP, we need to make minimal changes in our training script. First of all, we initialize training process and add it to DDP process group:\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend=\"nccl\",\n",
    "    rank=int(os.getenv(\"RANK\", 0)),\n",
    "    world_size=int(os.getenv(\"WORLD_SIZE\", 1)),\n",
    ")\n",
    "```\n",
    "\n",
    "Since we have GPU-based instances, we use `NCCL` communication backend. Also we utilize enviornment variables set but `torch.distributed.run` module: world size and global rank. \n",
    "\n",
    "Next, we need to identify which GPU device will store model and run computations. We use local_rank envvar set by `torch.distributed.run` during process spawn.\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(os.getenv(\"LOCAL_RANK\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "We then wrap our regular PyTorch model with special DDP implementation. This implementation allows us to work with PyTorch model as if it is a regular locally stored model. under the hood, DDP implements gradient synchronization between training processes in process group.\n",
    "```python\n",
    "model = DDP(model)\n",
    "```\n",
    "\n",
    "Last step we need to need to modify training data loader so tghat each training process gets a unqiue slice of data during training step. For this, we use `DistributedSampler` which samples data records based on total number of processes (`world_size` variable) and global rank (`rank` variable) of given training process:\n",
    "\n",
    "```python\n",
    "    # Note that we are passing global rank in data samples to get unique data slice\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        image_datasets[\"train\"], num_replicas=args.world_size, rank=args.rank\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        image_datasets[\"train\"],\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "    ) \n",
    "```\n",
    "You can review a full listing of training script by running cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m division, print_function\n",
      "\n",
      "\u001b[37m# Common imports\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mwebbrowser\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get\n",
      "\n",
      "\u001b[37m# Third Party imports\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparallel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistributedDataParallel \u001b[34mas\u001b[39;49;00m DDP\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, models, transforms\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "LOGGER = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# Two classes: ants and bees\u001b[39;49;00m\n",
      "NUM_CLASSES = \u001b[34m2\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# Initialize Distributed Training group\u001b[39;49;00m\n",
      "\u001b[37m# Note, that we are using env vars setup by torch.distirbuted.run utility before:\u001b[39;49;00m\n",
      "\u001b[37m# See details: https://github.com/pytorch/pytorch/blob/master/torch/distributed/run.py#L208\u001b[39;49;00m\n",
      "\u001b[37m# reference to api: https://pytorch.org/docs/stable/_modules/torch/distributed/distributed_c10d.html#init_process_group\u001b[39;49;00m\n",
      "dist.init_process_group(\n",
      "    backend=\u001b[33m\"\u001b[39;49;00m\u001b[33mnccl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    rank=\u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m)),\n",
      "    world_size=\u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)),\n",
      ")\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args, model, device, train_loader, optimizer, epoch):\n",
      "    model.train()\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader):\n",
      "        data, target = data.to(device), target.to(device)\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        loss = F.cross_entropy(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.rank == \u001b[34m0\u001b[39;49;00m:\n",
      "            LOGGER.info(\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)]\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33mLoss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                    epoch,\n",
      "                    batch_idx * \u001b[36mlen\u001b[39;49;00m(data) * args.world_size,\n",
      "                    \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "                    \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\n",
      "                    loss.item(),\n",
      "                )\n",
      "            )\n",
      "        LOGGER.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mBatch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_idx\u001b[33m}\u001b[39;49;00m\u001b[33m from rank \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.rank\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, device, test_loader):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(test_loader):\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.cross_entropy(output, target, reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33msum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).item()\n",
      "            pred = output.argmax(\n",
      "                dim=\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m\n",
      "            )  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    LOGGER.info(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            test_loss,\n",
      "            correct,\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "            \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model.module.state_dict(), f)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "    \u001b[37m# hyperparameters set by users are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m15\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m1.0\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 1.0)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.7\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLearning rate step gamma (default: 0.7)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--save-model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        action=\u001b[33m\"\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mFor Saving the current Model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    \u001b[37m# Model checkpoint location\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_resnet_model\u001b[39;49;00m():\n",
      "    model_ft = models.resnet18(pretrained=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    num_ftrs = model_ft.fc.in_features\n",
      "    model_ft.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
      "    input_size = \u001b[34m224\u001b[39;49;00m  \u001b[37m# defined by Resnet18\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model_ft, input_size\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_dataloaders\u001b[39;49;00m(args):\n",
      "    \u001b[37m# Data augmentation and normalization for training\u001b[39;49;00m\n",
      "    \u001b[37m# Just normalization for validation\u001b[39;49;00m\n",
      "    data_transforms = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: transforms.Compose(\n",
      "            [\n",
      "                transforms.RandomResizedCrop(args.input_size),\n",
      "                transforms.RandomHorizontalFlip(),\n",
      "                transforms.ToTensor(),\n",
      "                transforms.Normalize([\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m], [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]),\n",
      "            ]\n",
      "        ),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: transforms.Compose(\n",
      "            [\n",
      "                transforms.Resize(args.input_size),\n",
      "                transforms.CenterCrop(args.input_size),\n",
      "                transforms.ToTensor(),\n",
      "                transforms.Normalize([\u001b[34m0.485\u001b[39;49;00m, \u001b[34m0.456\u001b[39;49;00m, \u001b[34m0.406\u001b[39;49;00m], [\u001b[34m0.229\u001b[39;49;00m, \u001b[34m0.224\u001b[39;49;00m, \u001b[34m0.225\u001b[39;49;00m]),\n",
      "            ]\n",
      "        ),\n",
      "    }\n",
      "\n",
      "    \u001b[37m# Create training and validation datasets\u001b[39;49;00m\n",
      "    image_datasets = {\n",
      "        x: datasets.ImageFolder(\n",
      "            os.environ[\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx.upper()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], data_transforms[x]\n",
      "        )\n",
      "        \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m [\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    }\n",
      "\n",
      "    \u001b[37m# Note that we are passing global rank in data samples to get unique data slice\u001b[39;49;00m\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "        image_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], num_replicas=args.world_size, rank=args.rank\n",
      "    )\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        image_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n",
      "        batch_size=args.batch_size,\n",
      "        shuffle=\u001b[34mFalse\u001b[39;49;00m,\n",
      "        num_workers=\u001b[34m0\u001b[39;49;00m,\n",
      "        pin_memory=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        sampler=train_sampler,\n",
      "    )\n",
      "\n",
      "    test_loader = \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        test_loader = torch.utils.data.DataLoader(\n",
      "            image_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], batch_size=args.test_batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m\n",
      "        )\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_loader, test_loader\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "\n",
      "    args, unknown_args = parse_args()\n",
      "\n",
      "    \u001b[37m# Extend args with info about training cluster using world info collected by PyTorch\u001b[39;49;00m\n",
      "    args.world_size = dist.get_world_size()\n",
      "    args.rank = dist.get_rank()\n",
      "    args.local_rank = \u001b[36mint\u001b[39;49;00m(\n",
      "        os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mLOCAL_RANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    )  \u001b[37m# Note that LOCAL_RANK is auto-set by torch.distributed.run in each training process\u001b[39;49;00m\n",
      "    LOGGER.info(\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mCollected args: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs\u001b[33m}\u001b[39;49;00m\u001b[33m. Following args are not parsed and will be ignored: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00munknown_args\u001b[33m}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    LOGGER.info(\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mHello from training process with rank=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.rank\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mlocal rank=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.local_rank\u001b[33m}\u001b[39;49;00m\u001b[33m in the world of \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.world_size\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    model, args.input_size = get_resnet_model()\n",
      "\n",
      "    torch.cuda.set_device(args.local_rank)\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    model = DDP(model.to(device))\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
      "    scheduler = StepLR(optimizer, step_size=\u001b[34m1\u001b[39;49;00m, gamma=args.gamma)\n",
      "\n",
      "    train_loader, test_loader = get_dataloaders(args)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        train(args, model, device, train_loader, optimizer, epoch)\n",
      "        scheduler.step()\n",
      "    \u001b[34mif\u001b[39;49;00m args.rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        test(model, device, test_loader)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.rank == \u001b[34m0\u001b[39;49;00m:\n",
      "        LOGGER.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        save_model(model, args.model_dir)\n",
      "    dist.barrier()  \u001b[37m# syncing on barrier to avoid bug: https://github.com/pytorch/pytorch/issues/69520\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 2_sources/train_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-22 20:04:35 Starting - Starting the training job......\n",
      "2022-04-22 20:05:23 Starting - Preparing the instances for training......\n",
      "2022-04-22 20:06:36 Downloading - Downloading input data...\n",
      "2022-04-22 20:06:57 Training - Downloading the training image.......................bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-04-22 20:10:46,664 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-22 20:10:46,687 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-22 20:10:46,696 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-22 20:10:46,649 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-22 20:10:46,670 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-22 20:10:46,678 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-22 20:10:47,279 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 25,\n",
      "        \"train-script\": \"train_ddp.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-22-20-04-35-229\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launcher.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"epochs\":25,\"train-script\":\"train_ddp.py\"}\n",
      "SM_USER_ENTRY_POINT=launcher.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\",\"val\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=launcher\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":25,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-22-20-04-35-229\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}\n",
      "SM_USER_ARGS=[\"--epochs\",\"25\",\"--train-script\",\"train_ddp.py\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_CHANNEL_VAL=/opt/ml/input/data/val\n",
      "SM_HP_EPOCHS=25\n",
      "SM_HP_TRAIN-SCRIPT=train_ddp.py\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python launcher.py --epochs 25 --train-script train_ddp.py\n",
      "INFO:__main__:Command line to be executed on each node once:['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=0', '--rdzv_id=pytorch-training-2022-04-22-20-04-35-229', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--epochs', '25']\n",
      "2022-04-22 20:10:47,256 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 25,\n",
      "        \"train-script\": \"train_ddp.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2022-04-22-20-04-35-229\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launcher.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"epochs\":25,\"train-script\":\"train_ddp.py\"}\n",
      "SM_USER_ENTRY_POINT=launcher.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\",\"val\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_MODULE_NAME=launcher\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":25,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2022-04-22-20-04-35-229\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-22-20-04-35-229/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}\n",
      "SM_USER_ARGS=[\"--epochs\",\"25\",\"--train-script\",\"train_ddp.py\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_CHANNEL_VAL=/opt/ml/input/data/val\n",
      "SM_HP_EPOCHS=25\n",
      "SM_HP_TRAIN-SCRIPT=train_ddp.py\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python launcher.py --epochs 25 --train-script train_ddp.py\n",
      "INFO:__main__:Command line to be executed on each node once:['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=1', '--rdzv_id=pytorch-training-2022-04-22-20-04-35-229', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--epochs', '25']\n",
      "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for 2 nodes.\n",
      "INFO:__main__:Collected args: Namespace(batch_size=64, epochs=25, gamma=0.7, local_rank=0, log_interval=10, lr=1.0, model_dir='/opt/ml/model', rank=1, save_model=False, test_batch_size=1000, world_size=2). Following args are not parsed and will be ignored: [].\n",
      "INFO:__main__:Hello from training process with rank=1 and local rank=0 in the world of 2\n",
      "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for 2 nodes.\n",
      "INFO:__main__:Collected args: Namespace(batch_size=64, epochs=25, gamma=0.7, local_rank=0, log_interval=10, lr=1.0, model_dir='/opt/ml/model', rank=0, save_model=False, test_batch_size=1000, world_size=2). Following args are not parsed and will be ignored: [].\n",
      "INFO:__main__:Hello from training process with rank=0 and local rank=0 in the world of 2\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "34%|███▍      | 15.2M/44.7M [00:00<00:00, 159MB/s]\n",
      "68%|██████▊   | 30.3M/44.7M [00:00<00:00, 141MB/s]\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 147MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "0%|          | 0.00/44.7M [00:00<?, ?B/s]\n",
      "35%|███▌      | 15.8M/44.7M [00:00<00:00, 166MB/s]\n",
      "73%|███████▎  | 32.8M/44.7M [00:00<00:00, 173MB/s]\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 174MB/s]\n",
      "algo-1:45:45 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "NCCL version 2.7.8+cuda11.1\n",
      "\n",
      "2022-04-22 20:10:43 Training - Training image download completed. Training in progress.algo-2:44:44 [0] ofi_init:1134 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "INFO:__main__:Train Epoch: 1 [0/244 (0%)]#011Loss: 0.855305\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 2 [0/244 (0%)]#011Loss: 1.092636\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 3 [0/244 (0%)]#011Loss: 0.402528\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 4 [0/244 (0%)]#011Loss: 0.193977\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 5 [0/244 (0%)]#011Loss: 0.107819\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 6 [0/244 (0%)]#011Loss: 0.078819\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 7 [0/244 (0%)]#011Loss: 0.081951\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 8 [0/244 (0%)]#011Loss: 0.061757\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 9 [0/244 (0%)]#011Loss: 0.060025\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 10 [0/244 (0%)]#011Loss: 0.088458\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 11 [0/244 (0%)]#011Loss: 0.072356\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 12 [0/244 (0%)]#011Loss: 0.142117\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 13 [0/244 (0%)]#011Loss: 0.109312\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 14 [0/244 (0%)]#011Loss: 0.057634\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 15 [0/244 (0%)]#011Loss: 0.080968\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 16 [0/244 (0%)]#011Loss: 0.056990\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 17 [0/244 (0%)]#011Loss: 0.080120\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 18 [0/244 (0%)]#011Loss: 0.056422\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 19 [0/244 (0%)]#011Loss: 0.076485\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 20 [0/244 (0%)]#011Loss: 0.054859\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Train Epoch: 21 [0/244 (0%)]#011Loss: 0.077891\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 22 [0/244 (0%)]#011Loss: 0.099725\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Train Epoch: 23 [0/244 (0%)]#011Loss: 0.064784\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 24 [0/244 (0%)]#011Loss: 0.052897\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Train Epoch: 25 [0/244 (0%)]#011Loss: 0.068505\n",
      "INFO:__main__:Batch 0 from rank 0\n",
      "INFO:__main__:Batch 0 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 1\n",
      "INFO:__main__:Batch 1 from rank 0\n",
      "INFO:__main__:\n",
      "Test set: Average loss: 0.2211, Accuracy: 140/153 (92%)\n",
      "INFO:__main__:Saving the model...\n",
      "2022-04-22 20:11:40,685 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "WARNING:torch.distributed.elastic.rendezvous.dynamic_rendezvous:The node 'algo-2_27_0' has failed to shutdown the rendezvous 'pytorch-training-2022-04-22-20-04-35-229' due to an error of type RendezvousConnectionError.\n",
      "2022-04-22 20:11:40,706 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2022-04-22 20:12:00 Uploading - Uploading generated training model\n",
      "2022-04-22 20:12:00 Completed - Training job completed\n",
      "Training seconds: 648\n",
      "Billable seconds: 648\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "#ps_instance_type = 'ml.p3.2xlarge'\n",
    "ps_instance_type = 'ml.p3.2xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "#distribution = {'parameter_server': {\n",
    "#                    'enabled': True}\n",
    "#                }\n",
    "hyperparameters = {\n",
    "  'train-script': 'train_ddp.py',\n",
    "  'epochs': 25,\n",
    "  #'batch-size-per-device' : 16,\n",
    "  #'steps-per-epoch': 100\n",
    "  }\n",
    "\n",
    "estimator_ms = PyTorch(\n",
    "                       source_dir='2_sources',\n",
    "                       entry_point='launcher.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.9',\n",
    "                       py_version='py38',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       )\n",
    "\n",
    "estimator_ms.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
