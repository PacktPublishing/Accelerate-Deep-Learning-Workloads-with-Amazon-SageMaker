{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel Training with PyTorch DDP\n",
    "\n",
    "In this notebook we will learn how to engineer data parallel job using PyTorch Distributed Data Parallel (`DDP`). While SageMaker doesn’t support PyTorch DDP natively, it’s possible to run DDP training jobs on SageMaker. \n",
    "\n",
    "As a training task, we will finetune pretrained `Resnet18` model to classify ants and bees. We will use open-source **Hymenoptera dataset**. We will distribute our training across two nodes of `p2.xlarge` instances with single GPU device each. Feel free to modify number and/or type of instances in training cluster and observe how this change training performance. Note, that this is a small-scale training and will not be indicative of training efficiency on real-life tasks. \n",
    "\n",
    "We start with necessary imports and basic SageMaker training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role() # replace it with role ARN if you are not using SageMaker Notebook or Studio environments.\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/pytorch-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download, unzip, and upload dataset to Amazon S3 bucket. Note, it may take several minutes to complete these operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading dataset and unzipping it locally\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = sagemaker_session.upload_data(path=\"./hymenoptera_data\", key_prefix=\"hymenoptera_data\")\n",
    "print(f\"S3 location of dataset {data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching Distributed Training Processes\n",
    "\n",
    "Amazon SageMaker has no out-of-the-box support for PyTorch DDP training. Specifically, SageMaker doesn't support starting multiple distributed DDP processes in training cluster. To compensate, we need to develop a launching utility to perform this function. This utility is quite simple and then can be re-used for any other DDP-based training jobs. Launcher script is located here: `2_sources/launcher.py`.\n",
    "\n",
    "In launcher script we use DDP module `torch.distributed.run` which simplifies spawning training processes in cluster.  As part of launcher script, we need to collect information about training world: number of compute nodes and GPUs devices in cluster as well as identify node which will act as master coordinator. Then `torch.distributed.run` will spawn multiple training processes according to this configuration.\n",
    "\n",
    "Let’s review how this setup is implemented in launcher script. \n",
    "\n",
    "**1. Collecting training cluster configuration.**\n",
    "\n",
    "First, we need to collect information about SageMaker training cluster. For this, we use environmental variables - `SM_HOSTS` (list of compute hosts in training cluster), `SM_CURRENT_HOST` (hostname where given process is running), and `SM_NUM_GPUS` (number of GPU devices available on compute node). These variables are set by SageMaker automatically at the start of your training job.\n",
    "\n",
    "```python\n",
    "    nodes = json.loads(os.getenv(\"SM_HOSTS\"))\n",
    "    nnodes = len(nodes)\n",
    "    node_rank = nodes.index(os.getenv(\"SM_CURRENT_HOST\"))\n",
    "    nproc_per_node = os.getenv(\"SM_NUM_GPUS\", 1)\n",
    "```\n",
    "\n",
    "**2. Starting Training Processes.**\n",
    "Next we need to form command line for `torch.distributed.run` with instructions on how it should spawn training processes in training cluster. See code snippet below with inline comments on specific parameters. Note, that below we are using torch.distributed.run as Python module. Alternatively, you can use its script version `torchrun`. Find more details on `torchrun` in PyTorch [documentation](https://pytorch.org/docs/stable/elastic/run.html).\n",
    "\n",
    "```python\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"torch.distributed.run\",\n",
    "        f\"--nproc_per_node={nproc_per_node}\", # how many processes per compute node to start\n",
    "        f\"--nnodes={str(nnodes)}\", # how many compute nodes in training cluster\n",
    "        f\"--node_rank={node_rank}\", # rank of current compute node in training cluster\n",
    "        f\"--rdzv_id={os.getenv('SAGEMAKER_JOB_NAME')}\", # a unique job id shared by all nodes in cluster\n",
    "        \"--rdzv_backend=c10d\", # distibuted communcation backend\n",
    "        f\"--rdzv_endpoint={nodes[0]}:{RDZV_PORT}\", # master node\n",
    "        distr_args.train_script, # training script which will be executed in all training processes\n",
    "    ]\n",
    "    # Adding training hyperparameters which will be then passed in training script\n",
    "    cmd.extend(training_hyperparameters)\n",
    "```\n",
    "Note, that we are adding training hyperparameters “as is” in the end of command line. These arguments are not handled by launcher, but by training script to configure training. \n",
    "\n",
    "To actual execut the launch process we use Python `subprocess.Popen()` method:\n",
    "\n",
    "```python\n",
    "    process = subprocess.Popen(cmd, env=os.environ)\n",
    "    process.wait()\n",
    "    if process.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "```\n",
    "\n",
    "Note, that we are copying environment variables to subprocesses to preserve all SageMaker variables. If spawned process returns non-zero code (an indication of error), we then raise exception to propagate error code to SageMaker control plane.\n",
    "\n",
    "Summarizing, our launcher utility is responsible for collecting training cluster configuration and then starting torch.distributed.run on each node. The utility then takes care of starting multiple training processes. Run cell below to review full listing of launcher utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 2_sources/launcher.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopting Training Script For DDP\n",
    "\n",
    "To use DDP, we need to make minimal changes in our training script. First of all, we initialize training process and add it to DDP process group:\n",
    "\n",
    "```python\n",
    "dist.init_process_group(\n",
    "    backend=\"nccl\",\n",
    "    rank=int(os.getenv(\"RANK\", 0)),\n",
    "    world_size=int(os.getenv(\"WORLD_SIZE\", 1)),\n",
    ")\n",
    "```\n",
    "\n",
    "Since we have GPU-based instances, we use `NCCL` communication backend. Also we utilize enviornment variables set but `torch.distributed.run` module: world size and global rank. \n",
    "\n",
    "Next, we need to identify which GPU device will store model and run computations. We use `LOCAL_RANK` variable set by `torch.distributed.run` during process spawn.\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(os.getenv(\"LOCAL_RANK\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "We then wrap our regular PyTorch model with special DDP implementation. This implementation allows us to work with PyTorch model as if it is a regular locally stored model. under the hood, DDP implements gradient synchronization between training processes in process group.\n",
    "\n",
    "```python\n",
    "model = DDP(model)\n",
    "```\n",
    "\n",
    "Last step we need to need to modify training data loader so tghat each training process gets a unqiue slice of data during training step. For this, we use `DistributedSampler` which samples data records based on total number of processes (`world_size` variable) and global rank (`rank` variable) of given training process:\n",
    "\n",
    "```python\n",
    "    # Note that we are passing global rank in data samples to get unique data slice\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        image_datasets[\"train\"], num_replicas=args.world_size, rank=args.rank\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        image_datasets[\"train\"],\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "    ) \n",
    "```\n",
    "You can review a full listing of training script by running cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 2_sources/train_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training Job\n",
    "\n",
    "Once we have launcher utility and training script ready, we can start our distributed training on SageMaker. Note, that since we need to start training via launcher utilityu, we set `entry_point` parameter accordingly. We pass actual training script as part of `hyperparameters` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "ps_instance_type = 'ml.p3.2xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "hyperparameters = {\n",
    "  'train-script': 'train_ddp.py',\n",
    "  'epochs': 25,\n",
    "  }\n",
    "\n",
    "estimator_ms = PyTorch(\n",
    "                       source_dir='2_sources',\n",
    "                       entry_point='launcher.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.9',\n",
    "                       py_version='py38',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_ms.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 8–9 minutes. Feel free to review the debug messages in the training job logs. Additionally, you can experiment with other parameters such as the instance type and size, the number of epochs, the batch size, and more.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this example, we learned how to engineer data parallel distributed training using PyTorch DDP - native Allreduce implementation. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
