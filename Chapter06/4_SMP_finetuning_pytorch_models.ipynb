{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket:\n",
      "sagemaker-us-east-1-941656036254\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\"\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sm-modelparallel-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"s3://sagemaker-us-east-1-941656036254/hymenoptera_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugger configuration\n",
    "\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile, Rule, ProfilerRule, rule_configs\n",
    "\n",
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=1000,\n",
    "    framework_profile_params=None\n",
    ")\n",
    "\n",
    "rules=[ \n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "mpioptions = \"-verbose -x orte_base_help_aggregate=0 \"\n",
    "#instance_type = 'ml.p3.16xlarge'\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "instance_count = 2\n",
    "\n",
    "smd_mp_estimator = PyTorch(\n",
    "          entry_point=\"train_sm_mp.py\", # Pick your train script\n",
    "          source_dir='4_sources',\n",
    "          role=role,\n",
    "          instance_type=instance_type,\n",
    "          sagemaker_session=sagemaker_session,\n",
    "          image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
    "          #image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.0-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
    "        #  framework_version='1.10',\n",
    "        #  py_version='py38',\n",
    "          instance_count=instance_count,\n",
    "          hyperparameters={\n",
    "              \"batch-size\":64,\n",
    "              \"epochs\":30,\n",
    "              #\"model-name\":\"squeezenet\",\n",
    "              #\"num-classes\": 2,\n",
    "              #\"feature-extract\":True,\n",
    "            #  \"sync-s3-path\":f\"s3://{bucket}/distributed-training/output\"\n",
    "          },\n",
    "          disable_profiler=True,\n",
    "          debugger_hook_config=False,\n",
    "          distribution={\n",
    "              \"smdistributed\": {\n",
    "                  \"modelparallel\": {\n",
    "                      \"enabled\":True,\n",
    "                      \"parameters\": {\n",
    "                          \"microbatches\": 8, # The number of microbatches to perform pipelining over. 1 means no pipelining. Batch size must be divisible by the number of microbatches.\n",
    "                                             # A microbatch is a smaller subset of a given training mini-batch. The pipeline schedule determines which microbatch is executed by which device for every time slot.   \n",
    "                          \"placement_strategy\": \"cluster\", # more advanced topic: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#placement-strategy-with-tensor-parallelism \n",
    "                          \"pipeline\": \"interleaved\",\n",
    "                          \"optimize\": \"speed\", \n",
    "                        #  \"pipeline_parallel_degree\": 2, # The number of partitions to split the model into. this is only in newer libs\n",
    "                          \"partitions\": 2,\n",
    "                          \"auto_partition\": True,\n",
    "                          \"ddp\": True, # enables hybrid parallelism: model + data. Makes sense in multi-GPU nodes only.\n",
    "                      }\n",
    "                  }\n",
    "              },\n",
    "              \"mpi\": {\n",
    "                    \"enabled\": True, # must be enabled\n",
    "                    \"processes_per_host\": 1, # Pick your processes_per_host\n",
    "                    \"custom_mpi_options\": mpioptions \n",
    "              },\n",
    "          },\n",
    "          base_job_name=\"SMD-MP\",\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-26 02:57:14 Starting - Starting the training job......\n",
      "2022-04-26 02:58:14 Starting - Preparing the instances for training............\n",
      "2022-04-26 03:00:10 Downloading - Downloading input data\n",
      "2022-04-26 03:00:10 Training - Downloading the training image........................\n",
      "2022-04-26 03:04:16 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-04-26 03:04:19,752 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-26 03:04:19,827 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-26 03:04:19,834 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-26 03:04:20,483 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "2022-04-26 03:04:20,483 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\n",
      "2022-04-26 03:04:20,495 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-04-26 03:04:20,586 paramiko.transport INFO     Authentication (publickey) successful!\n",
      "2022-04-26 03:04:20,586 sagemaker-training-toolkit INFO     Can connect to host algo-1\n",
      "2022-04-26 03:04:20,586 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\n",
      "2022-04-26 03:04:20,586 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\n",
      "2022-04-26 03:04:20,590 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-04-26 03:04:19,631 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-26 03:04:19,706 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-26 03:04:19,712 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-26 03:04:20,364 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n",
      "2022-04-26 03:04:20,365 sagemaker-training-toolkit INFO     Creating SSH daemon.\n",
      "2022-04-26 03:04:20,368 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n",
      "2022-04-26 03:04:20,370 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\n",
      "2022-04-26 03:04:20,371 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.92.35\n",
      "2022-04-26 03:04:21,383 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "2022-04-26 03:04:21,477 paramiko.transport INFO     Authentication (publickey) successful!\n",
      "2022-04-26 03:04:21,477 sagemaker-training-toolkit INFO     Can connect to host algo-2\n",
      "2022-04-26 03:04:21,477 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\n",
      "2022-04-26 03:04:21,478 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:4', 'algo-2:4'] process_per_hosts: 4 num_processes: 8\n",
      "2022-04-26 03:04:21,481 sagemaker-training-toolkit INFO     Network interface name: eth0\n",
      "2022-04-26 03:04:21,556 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 \",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 4\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 256,\n",
      "        \"epochs\": 30,\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 8,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 2,\n",
      "            \"auto_partition\": true,\n",
      "            \"ddp\": true\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"SMD-MP-2022-04-26-02-57-13-671\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/SMD-MP-2022-04-26-02-57-13-671/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_sm_mp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_sm_mp.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch-size\":256,\"epochs\":30,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"microbatches\":8,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}}\n",
      "SM_USER_ENTRY_POINT=train_sm_mp.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\",\"val\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=train_sm_mp\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=64\n",
      "SM_NUM_GPUS=8\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/SMD-MP-2022-04-26-02-57-13-671/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":4},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size\":256,\"epochs\":30,\"mp_parameters\":{\"auto_partition\":true,\"ddp\":true,\"microbatches\":8,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"SMD-MP-2022-04-26-02-57-13-671\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/SMD-MP-2022-04-26-02-57-13-671/source/sourcedir.tar.gz\",\"module_name\":\"train_sm_mp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_sm_mp.py\"}\n",
      "SM_USER_ARGS=[\"--batch-size\",\"256\",\"--epochs\",\"30\",\"--mp_parameters\",\"auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_CHANNEL_VAL=/opt/ml/input/data/val\n",
      "SM_HP_BATCH-SIZE=256\n",
      "SM_HP_EPOCHS=30\n",
      "SM_HP_MP_PARAMETERS={\"auto_partition\":true,\"ddp\":true,\"microbatches\":8,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/urllib3-1.26.8-py3.8.egg\n",
      "Invoking script with the following command:\n",
      "mpirun --host algo-1:4,algo-2:4 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VAL -x SM_HP_BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_sm_mp.py --batch-size 256 --epochs 30 --mp_parameters auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster\n",
      "[algo-1:00041] Warning: could not find environment variable \"SM_HP_BATCH-SIZE\"\n",
      "Warning: Permanently added 'algo-2,10.0.92.35' (ECDSA) to the list of known hosts.\n",
      "Data for JOB [41167,1] offset 0 Total slots allocated 8\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 3 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41167,1] App: 0 Process rank: 7 Bound: N/A\n",
      " =============================================================\n",
      "2022-04-26 03:04:22,596 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=50, name='orted', status='sleeping', started='03:04:21')]\n",
      "2022-04-26 03:04:22,596 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=50, name='orted', status='sleeping', started='03:04:21')]\n",
      "2022-04-26 03:04:22,596 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=50, name='orted', status='sleeping', started='03:04:21')]\n",
      "[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:0,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:1,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:2,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:3,algo-1]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:4,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:4,algo-2]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:5,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:5,algo-2]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:6,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:6,algo-2]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:7,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "[1,mpirank:7,algo-2]<stderr>:  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:6,algo-2]<stdout>:[2022-04-26 03:04:28.998: I smdistributed/modelparallel/torch/state_mod.py:161] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 3, rdp_rank: 3\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:4,algo-2]<stdout>:[2022-04-26 03:04:28.999: I smdistributed/modelparallel/torch/state_mod.py:161] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 2, rdp_rank: 2\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:29.007: I smdistributed/modelparallel/torch/state_mod.py:161] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:29.007: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 8 nodes.\n",
      "[1,mpirank:1,algo-1]<stdout>:[2022-04-26 03:04:29.007: I smdistributed/modelparallel/torch/state_mod.py:161] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\n",
      "[1,mpirank:2,algo-1]<stdout>:[2022-04-26 03:04:29.007: I smdistributed/modelparallel/torch/state_mod.py:161] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "[1,mpirank:3,algo-1]<stdout>:[2022-04-26 03:04:29.007: I smdistributed/modelparallel/torch/state_mod.py:161] [3] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 1, rdp_rank: 1\n",
      "[1,mpirank:5,algo-2]<stdout>:[2022-04-26 03:04:29.009: I smdistributed/modelparallel/torch/state_mod.py:161] [5] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 2, rdp_rank: 2\n",
      "[1,mpirank:7,algo-2]<stdout>:[2022-04-26 03:04:29.009: I smdistributed/modelparallel/torch/state_mod.py:161] [7] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 3, rdp_rank: 3\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:6,algo-2]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=3, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=6).\n",
      "[1,mpirank:6,algo-2]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:6,algo-2]<stderr>:--- Logging error ---\n",
      "[1,mpirank:6,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:6,algo-2]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:6,algo-2]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:6,algo-2]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:6,algo-2]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:6,algo-2]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:6,algo-2]<stderr>:Call stack:\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:6,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:6,algo-2]<stderr>:    main()\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:6,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:6,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:6,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:6,algo-2]<stderr>:    main()\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:6,algo-2]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:6,algo-2]<stderr>:Message: 'Hello from global rank 6.'\n",
      "[1,mpirank:6,algo-2]<stderr>:Arguments: ('local rank 2 and local size 4', 'List of ranks where current model is stored [6, 7]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 3 and DP size is 4', 'Other params: 0, 1, 0, 2, 3, 4')\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:7,algo-2]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=3, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=7).\n",
      "[1,mpirank:7,algo-2]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:7,algo-2]<stderr>:--- Logging error ---\n",
      "[1,mpirank:7,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:7,algo-2]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:7,algo-2]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:7,algo-2]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:7,algo-2]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:7,algo-2]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:7,algo-2]<stderr>:Call stack:\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:7,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:7,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:7,algo-2]<stderr>:    main()\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:7,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:7,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:7,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:7,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:7,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:7,algo-2]<stderr>:    main()\n",
      "[1,mpirank:7,algo-2]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:7,algo-2]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:7,algo-2]<stderr>:Message: 'Hello from global rank 7.'\n",
      "[1,mpirank:7,algo-2]<stderr>:Arguments: ('local rank 3 and local size 4', 'List of ranks where current model is stored [6, 7]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 3 and DP size is 4', 'Other params: 0, 1, 1, 2, 3, 4')\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:5,algo-2]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=2, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=5).\n",
      "[1,mpirank:5,algo-2]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:5,algo-2]<stderr>:--- Logging error ---\n",
      "[1,mpirank:5,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:5,algo-2]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:5,algo-2]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:5,algo-2]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:5,algo-2]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:5,algo-2]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:5,algo-2]<stderr>:Call stack:\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:5,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:5,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:5,algo-2]<stderr>:    main()\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:5,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:5,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:5,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:5,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:5,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:5,algo-2]<stderr>:    main()\n",
      "[1,mpirank:5,algo-2]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:5,algo-2]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:5,algo-2]<stderr>:Message: 'Hello from global rank 5.'\n",
      "[1,mpirank:5,algo-2]<stderr>:Arguments: ('local rank 1 and local size 4', 'List of ranks where current model is stored [4, 5]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 2 and DP size is 4', 'Other params: 0, 1, 1, 2, 2, 4')\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:2,algo-1]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=1, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=2).\n",
      "[1,mpirank:2,algo-1]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:1,algo-1]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=0, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=1).\n",
      "[1,mpirank:1,algo-1]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:2,algo-1]<stderr>:--- Logging error ---\n",
      "[1,mpirank:1,algo-1]<stderr>:--- Logging error ---\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:3,algo-1]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=1, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=3).\n",
      "[1,mpirank:3,algo-1]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:3,algo-1]<stderr>:--- Logging error ---\n",
      "[1,mpirank:2,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:2,algo-1]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:2,algo-1]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:2,algo-1]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:2,algo-1]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:1,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:3,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:2,algo-1]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:2,algo-1]<stderr>:Call stack:\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:3,algo-1]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:3,algo-1]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:3,algo-1]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:3,algo-1]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:3,algo-1]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:1,algo-1]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:1,algo-1]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:1,algo-1]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:1,algo-1]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:1,algo-1]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:3,algo-1]<stderr>:Call stack:\n",
      "[1,mpirank:1,algo-1]<stderr>:Call stack:\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:2,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:2,algo-1]<stderr>:    main()\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:2,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:2,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:2,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:2,algo-1]<stderr>:    main()\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:2,algo-1]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:2,algo-1]<stderr>:Message: 'Hello from global rank 2.'\n",
      "[1,mpirank:2,algo-1]<stderr>:Arguments: ('local rank 2 and local size 4', 'List of ranks where current model is stored [2, 3]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 1 and DP size is 4', 'Other params: 0, 1, 0, 2, 1, 4')\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:1,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:1,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:1,algo-1]<stderr>:    main()\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:1,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:1,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:1,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:1,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:1,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:1,algo-1]<stderr>:    main()\n",
      "[1,mpirank:1,algo-1]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:1,algo-1]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:3,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:3,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:3,algo-1]<stderr>:    main()\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:3,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:3,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:3,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:3,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:3,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:3,algo-1]<stderr>:    main()\n",
      "[1,mpirank:3,algo-1]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:3,algo-1]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:1,algo-1]<stderr>:Message: 'Hello from global rank 1.'\n",
      "[1,mpirank:1,algo-1]<stderr>:Arguments: ('local rank 1 and local size 4', 'List of ranks where current model is stored [0, 1]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 0 and DP size is 4', 'Other params: 0, 1, 1, 2, 0, 4')\n",
      "[1,mpirank:3,algo-1]<stderr>:Message: 'Hello from global rank 3.'\n",
      "[1,mpirank:3,algo-1]<stderr>:Arguments: ('local rank 3 and local size 4', 'List of ranks where current model is stored [2, 3]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 1 and DP size is 4', 'Other params: 0, 1, 1, 2, 1, 4')\n",
      "[1,mpirank:7,algo-2]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:5,algo-2]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:6,algo-2]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:2,algo-1]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:1,algo-1]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:3,algo-1]<stderr>:Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n",
      "[1,mpirank:6,algo-2]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:2,algo-1]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:1,algo-1]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:3,algo-1]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:6,algo-2]<stderr>:#015 12%|█▏        | 584k/4.78M [00:00<00:00, 5.48MB/s]\n",
      "[1,mpirank:5,algo-2]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:7,algo-2]<stderr>:#015  0%|          | 0.00/4.78M [00:00<?, ?B/s]\n",
      "[1,mpirank:2,algo-1]<stderr>:#015 34%|███▍      | 1.65M/4.78M [00:00<00:00, 17.1MB/s]\n",
      "[1,mpirank:5,algo-2]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 56.3MB/s]\n",
      "[1,mpirank:7,algo-2]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 55.6MB/s]\n",
      "[1,mpirank:1,algo-1]<stderr>:#015 48%|████▊     | 2.30M/4.78M [00:00<00:00, 24.1MB/s][1,mpirank:6,algo-2]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 24.7MB/s]\n",
      "[1,mpirank:3,algo-1]<stderr>:#015 55%|█████▍    | 2.61M/4.78M [00:00<00:00, 27.3MB/s]\n",
      "[1,mpirank:7,algo-2]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:5,algo-2]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:6,algo-2]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:2,algo-1]<stderr>:#015 90%|████████▉ | 4.28M/4.78M [00:00<00:00, 20.5MB/s]\n",
      "[1,mpirank:2,algo-1]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 21.8MB/s]\n",
      "[1,mpirank:3,algo-1]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 26.0MB/s]\n",
      "[1,mpirank:1,algo-1]<stderr>:#015100%|█████████▉| 4.76M/4.78M [00:00<00:00, 25.0MB/s]\n",
      "[1,mpirank:2,algo-1]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:1,algo-1]<stderr>:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 24.6MB/s]\n",
      "[1,mpirank:1,algo-1]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:3,algo-1]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.534: I smdistributed/modelparallel/backend/config.py:230] Configuration parameters:\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.534: I smdistributed/modelparallel/backend/config.py:233]   pipeline_parallel_degree: 2\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.534: I smdistributed/modelparallel/backend/config.py:233]   microbatches: 8\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.534: I smdistributed/modelparallel/backend/config.py:233]   pipeline: interleaved\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.534: I smdistributed/modelparallel/backend/config.py:233]   horovod: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   ddp: True\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   tensor_parallel_degree: 1\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   ddp_port: None\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   ddp_dist_backend: nccl\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   contiguous: True\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   placement_strategy: cluster\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   optimize: speed\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   default_partition: None\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   auto_partition: True\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.535: I smdistributed/modelparallel/backend/config.py:233]   prescaled_batch: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   memory_weight: 0.8\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   active_microbatches: 4\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   fp16_params: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   tensor_parallel_seed: 0\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   offload_activations: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   shard_optimizer_state: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   skip_tracing: False\n",
      "[1,mpirank:0,algo-1]<stdout>:[2022-04-26 03:04:31.536: I smdistributed/modelparallel/backend/config.py:233]   activation_loading_horizon: 4\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:0,algo-1]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=0, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=0).\n",
      "[1,mpirank:0,algo-1]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:0,algo-1]<stderr>:--- Logging error ---\n",
      "[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:0,algo-1]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:0,algo-1]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:0,algo-1]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:0,algo-1]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:0,algo-1]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:0,algo-1]<stderr>:Call stack:\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:0,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:0,algo-1]<stderr>:    main()\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:0,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:0,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:0,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:0,algo-1]<stderr>:    main()\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:0,algo-1]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:0,algo-1]<stderr>:Message: 'Hello from global rank 0.'\n",
      "[1,mpirank:0,algo-1]<stderr>:Arguments: ('local rank 0 and local size 4', 'List of ranks where current model is stored [0, 1]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 0 and DP size is 4', 'Other params: 0, 1, 0, 2, 0, 4')\n",
      "[1,mpirank:0,algo-1]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:2,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:2,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:2,algo-1]<stderr>:    main()\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:2,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:2,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:2,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:2,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:2,algo-1]<stderr>:    main()\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"train_sm_mp.py\", line 301, in main\n",
      "[1,mpirank:2,algo-1]<stderr>:    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"train_sm_mp.py\", line 61, in train\n",
      "[1,mpirank:2,algo-1]<stderr>:    _, loss_mb = train_step(model, data, target)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__\n",
      "[1,mpirank:2,algo-1]<stderr>:    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs(\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs\n",
      "[1,mpirank:2,algo-1]<stderr>:    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args\n",
      "[1,mpirank:2,algo-1]<stderr>:    mb_args = self._split_tensors_in_list(\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list\n",
      "[1,mpirank:2,algo-1]<stderr>:    split_dict = self._split_tensors_in_dict(\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict\n",
      "[1,mpirank:2,algo-1]<stderr>:    return {\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp>\n",
      "[1,mpirank:2,algo-1]<stderr>:    k: self.map_structure(\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure\n",
      "[1,mpirank:2,algo-1]<stderr>:    return map_structure(func, structure)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure\n",
      "[1,mpirank:2,algo-1]<stderr>:    return fn(structure)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda>\n",
      "[1,mpirank:2,algo-1]<stderr>:    lambda x: self._split_util(\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util\n",
      "[1,mpirank:2,algo-1]<stderr>:    return self.slice(value, num_mb, mb, axis)\n",
      "[1,mpirank:2,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice\n",
      "[1,mpirank:2,algo-1]<stderr>:    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb))\n",
      "[1,mpirank:2,algo-1]<stderr>:ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html\n",
      "[1,mpirank:1,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:0,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:2,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:6,algo-2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:6,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:6,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:6,algo-2]<stderr>:    main()\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:6,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:6,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:6,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:6,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:6,algo-2]<stderr>:    main()\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"train_sm_mp.py\", line 301, in main\n",
      "[1,mpirank:6,algo-2]<stderr>:    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"train_sm_mp.py\", line 61, in train\n",
      "[1,mpirank:6,algo-2]<stderr>:    _, loss_mb = train_step(model, data, target)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__\n",
      "[1,mpirank:6,algo-2]<stderr>:    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs(\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs\n",
      "[1,mpirank:6,algo-2]<stderr>:    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args\n",
      "[1,mpirank:6,algo-2]<stderr>:    mb_args = self._split_tensors_in_list(\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list\n",
      "[1,mpirank:6,algo-2]<stderr>:    split_dict = self._split_tensors_in_dict(\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict\n",
      "[1,mpirank:6,algo-2]<stderr>:    return {\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp>\n",
      "[1,mpirank:6,algo-2]<stderr>:    k: self.map_structure(\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure\n",
      "[1,mpirank:6,algo-2]<stderr>:    return map_structure(func, structure)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure\n",
      "[1,mpirank:6,algo-2]<stderr>:    return fn(structure)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda>\n",
      "[1,mpirank:6,algo-2]<stderr>:    lambda x: self._split_util(\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util\n",
      "[1,mpirank:6,algo-2]<stderr>:    return self.slice(value, num_mb, mb, axis)\n",
      "[1,mpirank:6,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice\n",
      "[1,mpirank:6,algo-2]<stderr>:    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb))\n",
      "[1,mpirank:6,algo-2]<stderr>:ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html\n",
      "[1,mpirank:7,algo-2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:5,algo-2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:0,algo-1]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:0,algo-1]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:0,algo-1]<stderr>:    main()\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:0,algo-1]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:0,algo-1]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:0,algo-1]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:0,algo-1]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:0,algo-1]<stderr>:    main()\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"train_sm_mp.py\", line 301, in main\n",
      "[1,mpirank:0,algo-1]<stderr>:    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"train_sm_mp.py\", line 61, in train\n",
      "[1,mpirank:0,algo-1]<stderr>:    _, loss_mb = train_step(model, data, target)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__\n",
      "[1,mpirank:0,algo-1]<stderr>:    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs(\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs\n",
      "[1,mpirank:0,algo-1]<stderr>:    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args\n",
      "[1,mpirank:0,algo-1]<stderr>:    mb_args = self._split_tensors_in_list(\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list\n",
      "[1,mpirank:0,algo-1]<stderr>:    split_dict = self._split_tensors_in_dict(\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict\n",
      "[1,mpirank:0,algo-1]<stderr>:    return {\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp>\n",
      "[1,mpirank:0,algo-1]<stderr>:    k: self.map_structure(\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure\n",
      "[1,mpirank:0,algo-1]<stderr>:    return map_structure(func, structure)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure\n",
      "[1,mpirank:0,algo-1]<stderr>:    return fn(structure)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda>\n",
      "[1,mpirank:0,algo-1]<stderr>:    lambda x: self._split_util(\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util\n",
      "[1,mpirank:0,algo-1]<stderr>:    return self.slice(value, num_mb, mb, axis)\n",
      "[1,mpirank:0,algo-1]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice\n",
      "[1,mpirank:0,algo-1]<stderr>:    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb))\n",
      "[1,mpirank:0,algo-1]<stderr>:ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html\n",
      "[1,mpirank:3,algo-1]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:4,algo-2]<stderr>:Environment variable SAGEMAKER_INSTANCE_TYPE is not set\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000). \n",
      "[1,mpirank:4,algo-2]<stderr>: Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=2, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=4).\n",
      "[1,mpirank:4,algo-2]<stderr>:Following args are not parsed correctly and won't be used: [].\n",
      "[1,mpirank:4,algo-2]<stderr>:--- Logging error ---\n",
      "[1,mpirank:4,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "[1,mpirank:4,algo-2]<stderr>:    msg = self.format(record)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format\n",
      "[1,mpirank:4,algo-2]<stderr>:    return fmt.format(record)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format\n",
      "[1,mpirank:4,algo-2]<stderr>:    record.message = record.getMessage()\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage\n",
      "[1,mpirank:4,algo-2]<stderr>:    msg = msg % self.args\n",
      "[1,mpirank:4,algo-2]<stderr>:TypeError: not all arguments converted during string formatting\n",
      "[1,mpirank:4,algo-2]<stderr>:Call stack:\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:4,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:4,algo-2]<stderr>:    main()\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:4,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:4,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:4,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:4,algo-2]<stderr>:    main()\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"train_sm_mp.py\", line 269, in main\n",
      "[1,mpirank:4,algo-2]<stderr>:    LOGGER.info(\n",
      "[1,mpirank:4,algo-2]<stderr>:Message: 'Hello from global rank 4.'\n",
      "[1,mpirank:4,algo-2]<stderr>:Arguments: ('local rank 0 and local size 4', 'List of ranks where current model is stored [4, 5]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 2 and DP size is 4', 'Other params: 0, 1, 0, 2, 2, 4')\n",
      "[1,mpirank:4,algo-2]<stderr>:INFO:__main__:Scaled batch size from 256 to 64.\n",
      "[1,mpirank:4,algo-2]<stderr>:Traceback (most recent call last):\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "[1,mpirank:4,algo-2]<stderr>:    return _run_code(code, main_globals, None,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module>\n",
      "[1,mpirank:4,algo-2]<stderr>:    main()\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main\n",
      "[1,mpirank:4,algo-2]<stderr>:    run_command_line(args)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line\n",
      "[1,mpirank:4,algo-2]<stderr>:    run_path(sys.argv[0], run_name='__main__')\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "[1,mpirank:4,algo-2]<stderr>:    return _run_module_code(code, init_globals, run_name,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    _run_code(code, mod_globals, init_globals,\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "[1,mpirank:4,algo-2]<stderr>:    exec(code, run_globals)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"train_sm_mp.py\", line 331, in <module>\n",
      "[1,mpirank:4,algo-2]<stderr>:    main()\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"train_sm_mp.py\", line 301, in main\n",
      "[1,mpirank:4,algo-2]<stderr>:    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"train_sm_mp.py\", line 61, in train\n",
      "[1,mpirank:4,algo-2]<stderr>:    _, loss_mb = train_step(model, data, target)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__\n",
      "[1,mpirank:4,algo-2]<stderr>:    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs(\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs\n",
      "[1,mpirank:4,algo-2]<stderr>:    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args\n",
      "[1,mpirank:4,algo-2]<stderr>:    mb_args = self._split_tensors_in_list(\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list\n",
      "[1,mpirank:4,algo-2]<stderr>:    split_dict = self._split_tensors_in_dict(\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict\n",
      "[1,mpirank:4,algo-2]<stderr>:    return {\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp>\n",
      "[1,mpirank:4,algo-2]<stderr>:    k: self.map_structure(\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure\n",
      "[1,mpirank:4,algo-2]<stderr>:    return map_structure(func, structure)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure\n",
      "[1,mpirank:4,algo-2]<stderr>:    return fn(structure)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda>\n",
      "[1,mpirank:4,algo-2]<stderr>:    lambda x: self._split_util(\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util\n",
      "[1,mpirank:4,algo-2]<stderr>:    return self.slice(value, num_mb, mb, axis)\n",
      "[1,mpirank:4,algo-2]<stderr>:  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice\n",
      "[1,mpirank:4,algo-2]<stderr>:    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb))\n",
      "[1,mpirank:4,algo-2]<stderr>:ValueError[1,mpirank:4,algo-2]<stderr>:: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html\n",
      "[1,mpirank:4,algo-2]<stderr>:terminate called without an active exception\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] *** Process received signal ***\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] Signal: Aborted (6)\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] Signal code:  (-6)\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 0] /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x143c0)[0x7fa49e3743c0]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [1,mpirank:4,algo-2]<stderr>:[ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x7fa49e04b03b]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 2]\n",
      "[1,mpirank:4,algo-2]<stderr>:/usr/lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x7fa49e02a859]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 3] [1,mpirank:4,algo-2]<stderr>:/opt/conda/lib/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0xbc)[0x7fa492adc872]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 4] [1,mpirank:4,algo-2]<stderr>:/opt/conda/lib/libstdc++.so.6(+0xacf6f)[0x7fa492adaf6f]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 5] [1,mpirank:4,algo-2]<stderr>:/opt/conda/lib/libstdc++.so.6(+0xacfb1)[0x7fa492adafb1]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 6]\n",
      "[1,mpirank:4,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/smplib.cpython-38-x86_64-linux-gnu.so(+0xe4c17)[0x7fa469fe7c17]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 7] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x468d7)[0x7fa49e04e8d7]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 8] /usr/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7fa49e04ea90]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [ 9] /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xfa)[0x7fa49e02c0ba]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] [10] [1,mpirank:4,algo-2]<stderr>:/opt/conda/bin/python3.8(+0x1e8f39)[0x5558fb102f39]\n",
      "[1,mpirank:4,algo-2]<stderr>:[algo-2:00053] *** End of error message ***\n",
      "--------------------------------------------------------------------------\n",
      "Primary job  terminated normally, but 1 process returned\n",
      "a non-zero exit code. Per user-direction, the job has been aborted.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "mpirun.real noticed that process rank 4 with PID 53 on node algo-2 exited on signal 6 (Aborted).\n",
      "--------------------------------------------------------------------------\n",
      "2022-04-26 03:04:34,488 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2022-04-26 03:04:34,488 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 134\n",
      "ErrorMessage \":TypeError: not all arguments converted during string formatting\n",
      " :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 6.' :Arguments: ('local rank 2 and local size 4', 'List of ranks where current model is stored [6, 7]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 3 and DP size is 4', 'Other params: 0, 1, 0, 2, 3, 4') :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=3, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=7). :Following args are not parsed correctly and won't be used: []. :--- Logging error --- :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 7.' :Arguments: ('local rank 3 and local size 4', 'List of ranks where current model is stored [6, 7]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 3 and DP size is 4', 'Other params: 0, 1, 1, 2, 3, 4') :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=2, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=5). :Following args are not parsed correctly and won't be used: []. :--- Logging error --- :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 5.' :Arguments: ('local rank 1 and local size 4', 'List of ranks where current model is stored [4, 5]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 2 and DP size is 4', 'Other params: 0, 1, 1, 2, 2, 4') :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=1, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=2). :Following args are not parsed correctly and won't be used: []. :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=0, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=1). :Following args are not parsed correctly and won't be used: []. :--- Logging error --- :--- Logging error --- :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=1, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=3). :Following args are not parsed correctly and won't be used: []. :--- Logging error --- :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :Traceback (most recent call last): :Traceback (most recent call last): :TypeError: not all arguments converted during string formatting :Call stack: :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :Call stack: :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 2.' :Arguments: ('local rank 2 and local size 4', 'List of ranks where current model is stored [2, 3]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 1 and DP size is 4', 'Other params: 0, 1, 0, 2, 1, 4') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 1.' :Arguments: ('local rank 1 and local size 4', 'List of ranks where current model is stored [0, 1]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 0 and DP size is 4', 'Other params: 0, 1, 1, 2, 0, 4') :Message: 'Hello from global rank 3.' :Arguments: ('local rank 3 and local size 4', 'List of ranks where current model is stored [2, 3]', 'list of ranks with different replicas of the same model [1, 3, 5, 7]', 'current MP rank 1 and MP size is 2', 'current DP rank 1 and DP size is 4', 'Other params: 0, 1, 1, 2, 1, 4') :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015 12%|█▏        | 584k/4.78M [00:00<00:00, 5.48MB/s] :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015  0%|          | 0.00/4.78M [00:00<?, ?B/s] :#015 34%|███▍      | 1.65M/4.78M [00:00<00:00, 17.1MB/s] :#015100%|██████████| 4.78M/4.78M [00:00<00:00, 56.3MB/s] :#015100%|██████████| 4.78M/4.78M [00:00<00:00, 55.6MB/s] :#015 48%|████▊     | 2.30M/4.78M [00:00<00:00, 24.1MB/s]:#015100%|██████████| 4.78M/4.78M [00:00<00:00, 24.7MB/s] :#015 55%|█████▍    | 2.61M/4.78M [00:00<00:00, 27.3MB/s] :INFO:__main__:Scaled batch size from 256 to 64. :INFO:__main__:Scaled batch size from 256 to 64. :INFO:__main__:Scaled batch size from 256 to 64. :#015 90%|████████▉ | 4.28M/4.78M [00:00<00:00, 20.5MB/s] :#015100%|██████████| 4.78M/4.78M [00:00<00:00, 21.8MB/s] :#015100%|██████████| 4.78M/4.78M [00:00<00:00, 26.0MB/s] :#015100%|█████████▉| 4.76M/4.78M [00:00<00:00, 25.0MB/s] :INFO:__main__:Scaled batch size from 256 to 64. :#015100%|██████████| 4.78M/4.78M [00:00<00:00, 24.6MB/s] :INFO:__main__:Scaled batch size from 256 to 64. :INFO:__main__:Scaled batch size from 256 to 64. :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=0, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=0). :Following ar\n",
      "gs are not parsed correctly and won't be used: []. :--- Logging error --- :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 0.' :Arguments: ('local rank 0 and local size 4', 'List of ranks where current model is stored [0, 1]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 0 and DP size is 4', 'Other params: 0, 1, 0, 2, 0, 4') :INFO:__main__:Scaled batch size from 256 to 64. :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 301, in main :    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank) :  File \"train_sm_mp.py\", line 61, in train :    _, loss_mb = train_step(model, data, target) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__ :    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs :    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args :    mb_args = self._split_tensors_in_list( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list :    split_dict = self._split_tensors_in_dict( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict :    return { :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp> :    k: self.map_structure( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure :    return map_structure(func, structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure :    return fn(structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda> :    lambda x: self._split_util( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util :    return self.slice(value, num_mb, mb, axis) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice :    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb)) :ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 301, in main :    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank) :  File \"train_sm_mp.py\", line 61, in train :    _, loss_mb = train_step(model, data, target) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__ :    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs :    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args :    mb_args = self._split_tensors_in_list( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list :    split_dict = self._split_tensors_in_dict( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict :    return { :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp> :    k: self.map_structure( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure :    return map_structure(func, structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure :    return fn(structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda> :    lambda x: self._split_util( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util :    return self.slice(value, num_mb, mb, axis) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice :    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb)) :ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 301, in main :    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank) :  File \"train_sm_mp.py\", line 61, in train :    _, loss_mb = train_step(model, data, target) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__ :    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs :    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args :    mb_args = self._split_tensors_in_list( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list :    split_dict = self._split_tensors_in_dict( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict :    return { :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp> :    k: self.map_structure( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure :    return map_structure(func, structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure :    return fn(structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda> :    lambda x: self._split_util( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util :    return self.slice(value, num_mb, mb, axis) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice :    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb)) :ValueError: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :INFO:__main__:Collected hyperparameters: Namespace(batch_size=256, epochs=30, mp_parameters='auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster', test_batch_size=1000).  : Collected Model Parallel parameters: Namespace(auto_partition=True, ddp=True, dp_rank=2, dp_size=4, microbatches=8, optimize='speed', partitions=2, pipeline='interleaved', placement_strategy='cluster', rank=4). :Following args are not parsed correctly and won't be used: []. :--- Logging error --- :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1085, in emit :    msg = self.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 929, in format :    return fmt.format(record) :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 668, in format :    record.message = record.getMessage() :  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 373, in getMessage :    msg = msg % self.args :TypeError: not all arguments converted during string formatting :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 269, in main :    LOGGER.info( :Message: 'Hello from global rank 4.' :Arguments: ('local rank 0 and loca\n",
      "l size 4', 'List of ranks where current model is stored [4, 5]', 'list of ranks with different replicas of the same model [0, 2, 4, 6]', 'current MP rank 0 and MP size is 2', 'current DP rank 2 and DP size is 4', 'Other params: 0, 1, 0, 2, 2, 4') :INFO:__main__:Scaled batch size from 256 to 64. :Traceback (most recent call last): :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"train_sm_mp.py\", line 331, in <module> :    main() :  File \"train_sm_mp.py\", line 301, in main :    train(model, device, train_loader, optimizer, epoch, sdmp_args.rank) :  File \"train_sm_mp.py\", line 61, in train :    _, loss_mb = train_step(model, data, target) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 246, in __call__ :    mb_args, mb_kwargs = self._splitter.preprocess_args_all_mbs( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 60, in preprocess_args_all_mbs :    mb_args, mb_kwargs = self.preprocess_args(args, kwargs, num_mb, mb) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 92, in preprocess_args :    mb_args = self._split_tensors_in_list( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 124, in _split_tensors_in_list :    split_dict = self._split_tensors_in_dict( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 143, in _split_tensors_in_dict :    return { :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 144, in <dictcomp> :    k: self.map_structure( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 54, in map_structure :    return map_structure(func, structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/utils.py\", line 141, in map_structure :    return fn(structure) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 145, in <lambda> :    lambda x: self._split_util( :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/backend/split.py\", line 163, in _split_util :    return self.slice(value, num_mb, mb, axis) :  File \"/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/step.py\", line 59, in slice :    raise ValueError(get_divisibility_error_str(\"pytorch\", dim_size, num_mb)) :ValueError:: Batch size must be divisible by the number of microbatches. Found batch size=61, number of microbatches=8. If this is the last batch of the epoch, size of the current batch might be smaller than the regular batch size you chose. If this is the case, you can set drop_last=True in your DataLoader to skip this batch, or you can manually skip this batch. If this is a tensor with no batch dimension, you can specify its argument name in 'non_split_inputs' argument to smp.step, so that smp does not attempt to split this tensor. If the batch dimension is not 0th axis, you can specify the batch axis within 'input_split_axis' argument to smp.step function. For details, visit SageMaker distributed model parallelism documentation for smp.step: https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel.html :terminate called without an active exception :[algo-2:00053] *** Process received signal *** :[algo-2:00053] Signal: Aborted (6) :[algo-2:00053] Signal code:  (-6) :[algo-2:00053] [ 0] /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x143c0)[0x7fa49e3743c0] :[algo-2:00053] :[ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x7fa49e04b03b] :[algo-2:00053] [ 2] :/usr/lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x7fa49e02a859] :[algo-2:00053] [ 3] :/opt/conda/lib/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0xbc)[0x7fa492adc872] :[algo-2:00053] [ 4] :/opt/conda/lib/libstdc++.so.6(+0xacf6f)[0x7fa492adaf6f] :[algo-2:00053] [ 5] :/opt/conda/lib/libstdc++.so.6(+0xacfb1)[0x7fa492adafb1] :[algo-2:00053] [ 6] :/opt/conda/lib/python3.8/site-packages/smdistributed/modelparallel/torch/smplib.cpython-38-x86_64-linux-gnu.so(+0xe4c17)[0x7fa469fe7c17] :[algo-2:00053] [ 7] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x468d7)[0x7fa49e04e8d7] :[algo-2:00053] [ 8] /usr/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7fa49e04ea90] :[algo-2:00053] [ 9] /usr/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xfa)[0x7fa49e02c0ba] :[algo-2:00053] [10] :/opt/conda/bin/python3.8(+0x1e8f39)[0x5558fb102f39] :[algo-2:00053] *** End of error message *** -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real noticed that process rank 4 with PID 53 on node algo-2 exited on signal 6 (Aborted).\"\n",
      "Command \"mpirun --host algo-1:4,algo-2:4 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_CHANNEL_VAL -x SM_HP_BATCH-SIZE -x SM_HP_EPOCHS -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train_sm_mp.py --batch-size 256 --epochs 30 --mp_parameters auto_partition=True,ddp=True,microbatches=8,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=cluster\"\n",
      "2022-04-26 03:04:34,488 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "2022-04-26 03:04:34,496 sagemaker-training-toolkit INFO     Orted process exited\n",
      "\n",
      "2022-04-26 03:04:42 Uploading - Uploading generated training model2022-04-26 03:05:04,526 sagemaker-training-toolkit INFO     MPI process finished.\n",
      "2022-04-26 03:05:04,526 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2022-04-26 03:05:07 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job SMD-MP-2022-04-26-02-57-13-671: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \":TypeError: not all arguments converted during string formatting\n :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/4_SMP_finetuning_pytorch_models.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/4_SMP_finetuning_pytorch_models.ipynb#ch0000009?line=0'>1</a>\u001b[0m smd_mp_estimator\u001b[39m.\u001b[39;49mfit(inputs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_url\u001b[39m}\u001b[39;49;00m\u001b[39m/train\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_url\u001b[39m}\u001b[39;49;00m\u001b[39m/val\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py:955\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=952'>953</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=953'>954</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=954'>955</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py:1956\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1953'>1954</a>\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1954'>1955</a>\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1955'>1956</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1956'>1957</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1957'>1958</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py:3798\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3794'>3795</a>\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3796'>3797</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3797'>3798</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3798'>3799</a>\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3799'>3800</a>\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py:3336\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3329'>3330</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3330'>3331</a>\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3331'>3332</a>\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3332'>3333</a>\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3333'>3334</a>\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3334'>3335</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3335'>3336</a>\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3336'>3337</a>\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3337'>3338</a>\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3338'>3339</a>\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3339'>3340</a>\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job SMD-MP-2022-04-26-02-57-13-671: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \":TypeError: not all arguments converted during string formatting\n :Call stack: :  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main :    return _run_code(code, main_globals, None, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code :    exec(code, run_globals) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/__main__.py\", line 7, in <module> :    main() :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 196, in main :    run_command_line(args) :  File \"/opt/conda/lib/python3.8/site-packages/mpi4py/run.py\", line 47, in run_command_line :    run_path(sys.argv[0], run_name='__main__') :  File \"/opt/conda/lib/python3.8/runpy.py\", line 265, in run_path :    return _run_module_code(code, init_globals, run_name, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 97, in _run_module_code :    _run_code(code, mod_globals, init_globals, :  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in"
     ]
    }
   ],
   "source": [
    "smd_mp_estimator.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
