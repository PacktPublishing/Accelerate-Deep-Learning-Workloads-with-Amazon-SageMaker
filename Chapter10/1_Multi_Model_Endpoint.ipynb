{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Endpoints on SageMaker\n",
    "\n",
    "A multi-model endpoint (**\"MME\"**) is a special type of SageMaker model endpoint that allows you to host thousands of models behind a single endpoint simultaneously. This type of endpoint is suitable for scenarios for similarly sized models with relatively low resource requirements that can be served from the same inference container.\n",
    "\n",
    "In this code sample, we will learn how to deploy two NLP models simultaneously using an MME. One model analyzes the sentiment of German text, while the other analyzes the sentiment of English text. We will use the HuggingFace PyTorch container for this. For this task, we will use following models from HuggingFace Model Hub: `distilbert-base-uncased-finetuned-sst-2-english` and `oliverguhr/german-sentiment-bert`. \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Run cell below to install Python dependencies for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Packages for MME\n",
    "SageMaker MME requires you to create a separate package for each model and upload it to Amazon S3. Follow the steps below to prepare two packages with English and German models:\n",
    "\n",
    "1. We will start by fetching the models from the HuggingFace Model hub and saving them locally. Note, that we also run inference locally using positive English and negative German samples to test models locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import (DistilBertForSequenceClassification,\n",
    "                          DistilBertTokenizer)\n",
    "\n",
    "# Loading English model from HuggingFace Model Hub\n",
    "EN_MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "en_tokenizer = DistilBertTokenizer.from_pretrained(EN_MODEL)\n",
    "en_model = DistilBertForSequenceClassification.from_pretrained(EN_MODEL)\n",
    "\n",
    "# Running inference locally\n",
    "inputs = en_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = en_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predictions = en_model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(f\"Expected: positive, actual: {predictions}\")\n",
    "\n",
    "# Saving model locally\n",
    "en_model_path = \"models/english_sentiment\"\n",
    "os.makedirs(en_model_path, exist_ok=True)\n",
    "\n",
    "en_model.save_pretrained(save_directory=en_model_path)\n",
    "en_tokenizer.save_pretrained(save_directory=en_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Loading German model from HuggingFace Model Hub\n",
    "GER_MODEL = \"oliverguhr/german-sentiment-bert\"\n",
    "ger_tokenizer = BertTokenizer.from_pretrained(GER_MODEL)\n",
    "ger_model = BertForSequenceClassification.from_pretrained(GER_MODEL)\n",
    "\n",
    "# Running inference locally\n",
    "inputs = ger_tokenizer(\"Das ist gar nicht mal so gut\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = ger_model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predictions = ger_model.config.id2label[predicted_class_id]\n",
    "\n",
    "print(f\"Expected: negative, actual:{predictions}\")\n",
    "\n",
    "# Saving model locally\n",
    "ger_model_path = \"models/german_sentiment\"\n",
    "os.makedirs(ger_model_path, exist_ok=True)\n",
    "\n",
    "en_model.save_pretrained(save_directory=ger_model_path)\n",
    "en_tokenizer.save_pretrained(save_directory=ger_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. An MME has the same requirements as those for the inference scripts of single-model endpoints. Run the cell below to review inference script and pay attention to functions for model loading (`model_fn()`), inference(`predict_fn()`), and data pre-/post-processing (`input_fn()` and `output_fn()` respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference script for English model\n",
    "! pygmentize 1_src/en_inference.py\n",
    "\n",
    "# inference script for German model\n",
    "! pygmentize 1_src/ger_inference.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, we need to package the model and inference code for the MME. SageMaker requests a specific directory structure that varies for PyTorch and TensorFlow containers. For PyTorch containers, the model and code should be packaged into a single tar.gz archive and have the following structure:\n",
    "```python\n",
    "        model.tar.gz/\n",
    "                |- model.pth # and any other model artifacts\n",
    "                |- code/\n",
    "                        |- inference.py\n",
    "                        |- requirements.txt # optional\n",
    "```\n",
    "\n",
    "Run the code below to prepare model packages for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/english_sentiment/code\n",
    "! cp 1_src/en_inference.py models/english_sentiment/code/inference.py\n",
    "! tar -czvf models/english_sentiment.tar.gz -C models/english_sentiment/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir models/german_sentiment/code\n",
    "! cp 1_src/ger_inference.py models/german_sentiment/code/inference.py\n",
    "! tar -czvf models/german_sentiment.tar.gz -C models/german_sentiment/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finally, we upload model packages to Amazon S3 using SageMaker Session object. Note, that both model  packages are stored under the sageme S3 key (variable `mm_data_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role() \n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'multi-model'\n",
    "mm_data_path = f\"s3://{bucket}/{prefix}/\"\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "en_model_data = sagemaker_session.upload_data('models/english_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\n",
    "ger_model_data = sagemaker_session.upload_data('models/german_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Multi Model Endpoint\n",
    "Once model packages are prepared, we are ready to create MME endpoint hosting them. Follow steps below for this:\n",
    "\n",
    "1. We start by identifying appropriate SageMaker inference container. Run ell below to get container URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "HF_VERSION = '4.17.0'\n",
    "PT_VERSION = 'pytorch1.10.2'\n",
    "\n",
    "pt_container_uri = image_uris.retrieve(framework='huggingface',\n",
    "                                region=region,\n",
    "                                version=HF_VERSION,\n",
    "                                image_scope='inference',\n",
    "                                base_framework_version=PT_VERSION,\n",
    "                                instance_type='ml.c5.xlarge')\n",
    "\n",
    "print(pt_container_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, we need to configure the MME parameters. Specifically, we must define the MultiModel mode. Note that we provide two specific environment variables – `SAGEMAKER_PROGRAM` and `SAGEMAKER_SUBMIT_DIRECTORY` – so that the SageMaker inference framework knows how to register the model handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "container  = {\n",
    "    'Image': pt_container_uri,\n",
    "    'ContainerHostname': 'MultiModel',\n",
    "    'Mode': 'MultiModel',\n",
    "    'ModelDataUrl': mm_data_path,\n",
    "    'Environment': {\n",
    "\t    'SAGEMAKER_PROGRAM':'inference.py',\n",
    "\t    'SAGEMAKER_SUBMIT_DIRECTORY':mm_data_path\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The last step of configuring the MME is to create a SageMaker model instance, endpoint configuration, and the endpoint itself. When creating the model, we must provide the MultiModel-enabled container from the preceding step. Note, that to deploy MME endpoint, we are using SageMaker boto3 client (variable `sm_client`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n",
    "model_name = f\"mme-sentiment-model-{unique_id}\"\n",
    "\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container,\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-ep-config\"\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# Code to wait for MME deployment completion\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MME\n",
    "\n",
    "Once the endpoint has been created, we can run and invoke our models. For this, in the invocation request, we need to supply a special parameter called `TargetModel`. Execute cells below to get predictions from both English and German sentiment models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "ger_input = \"Der Test verlief positiv.\"\n",
    "en_input = \"Test results are positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting response from English model\n",
    "en_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetModel=\"english_sentiment.tar.gz\",\n",
    "    Body=json.dumps(en_input),\n",
    ")\n",
    "\n",
    "predictions = json.loads(en_response[\"Body\"].read().decode())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting response from German model\n",
    "ger_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetModel=\"german_sentiment.tar.gz\",\n",
    "    Body=json.dumps(ger_input),\n",
    ")\n",
    "\n",
    "predictions = json.loads(ger_response[\"Body\"].read().decode())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Clean up\n",
    "\n",
    "Run cell below to delete cloud resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName = model_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
