{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Container Endpoints on SageMaker\n",
    "\n",
    "A multi-container endpoint (**\"MCE\"**) allows you to host up to 15 inference containers simultaneously. In this case, each container would serve its own model. MCEs are a good fit for use cases where models require different runtime environments/containers but not every single model can fully utilize the available instance resources. Another scenario is when models are called at different times.\n",
    "\n",
    "In this example, we will run an inference workload with two NLP models using different runtime environments: TensorFlow and PyTorch. We will host the Q&A model in a TensorFlow container and the text summarization model in a PyTorch container.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Run cell below to install Python dependencies for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Packages for MCE\n",
    "\n",
    "To deploy MME endpoint, we need to prepare separate packages for target models. Depending on model framework (PyTorch or TensorFlow), model package sturucture will be slightly different. Follow steps below to prepare model packages.\n",
    "\n",
    "\n",
    "### Prepare TensorFlow Model Package\n",
    "SageMaker expect following package structure for TensorFlow models:\n",
    "```python\n",
    "    model.tar.gz/\n",
    "                |--[model_version_number]/\n",
    "                                        |--variables\n",
    "                                        |--saved_model.pb\n",
    "                code/\n",
    "                    |--inference.py\n",
    "                    |--requirements.txt # optional\n",
    "```\n",
    "\n",
    "Follow the steps below to prepare TensorFlow model package:\n",
    "1. We start by fetching models from HuggingFace Hub. Note, that we download model bundle `saved_model.tar.gz` which is ready to be deployed on TensorFlow serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model package local directories\n",
    "! mkdir -p distilbert-base-uncased-distilled-squad/1\n",
    "! mkdir -p distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "# Download artifacts for TensorFlow DistilBert model for Question-Answering task\n",
    "! wget https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/saved_model.tar.gz\n",
    "! tar -zxvf saved_model.tar.gz -C distilbert-base-uncased-distilled-squad/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we prepare inference script. Note, that in our case, we will use the same inference script for both PyTorch and TensorFlow models (thanks for HuggingFace robust `pipeline` API!). Execute cell below to copy inference code and requirements.txt file to model package. Then archive model package into tarball. Feel free to review inference script by running `pygmentize 2_src/inference.py` command in separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files into model package directory\n",
    "! cp 2_src/inference.py distilbert-base-uncased-distilled-squad/code\n",
    "! cp 2_src/requirements.txt distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "# Archive model package\n",
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare PyTorch Model Package\n",
    "\n",
    "SageMaker expects following package structure for PyTorch models:\n",
    "```python\n",
    "        model.tar.gz/\n",
    "                |- model.pth # and any other model artifacts\n",
    "                |- code/\n",
    "                        |- inference.py\n",
    "                        |- requirements.txt # optional\n",
    "```\n",
    "\n",
    "Follow the steps below to prepare text summarization model:\n",
    "1. We fetch model artifacts and save them locally using HuggingFace model and tokenizer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Downloading model from Model Hub\n",
    "SUM_MODEL = \"sshleifer/distilbart-cnn-6-6\"\n",
    "sum_model = AutoModelForSeq2SeqLM.from_pretrained(SUM_MODEL)\n",
    "sum_tokenizer = AutoTokenizer.from_pretrained(SUM_MODEL)\n",
    "\n",
    "# Saving model locally\n",
    "sum_model_path = \"distilbart-cnn-6-6\"\n",
    "os.makedirs(sum_model_path, exist_ok=True)\n",
    "\n",
    "sum_model.save_pretrained(save_directory=sum_model_path)\n",
    "sum_model.save_pretrained(save_directory=sum_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we copy inference code and dependencies in the model package and create single archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy inference code and dependencies\n",
    "! mkdir -p distilbart-cnn-6-6/code\n",
    "! cp 2_src/inference.py distilbart-cnn-6-6/code\n",
    "! cp 2_src/requirements.txt distilbart-cnn-6-6/code\n",
    "\n",
    "# Create model poackage tarball\n",
    "!tar -C \"$PWD\" -czf distilbart-cnn-6-6.tar.gz distilbart-cnn-6-6/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model data to S3\n",
    "\n",
    "Finally, we upload both packages to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'multi-container'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))\n",
    "\n",
    "summarization_model_data = sagemaker_session.upload_data('distilbart-cnn-6-6.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy MCE\n",
    "\n",
    "Now we are ready to configure and deploy our MCE endpoint. \n",
    "\n",
    "### Configure Inference Container\n",
    "For this we need to configure for our target models runtime containers. Execute cells below to fetch SageMaker PyTorch container image, associate model arctifacts with each container, and then provide runtime configuration via environmental variables (`qa_env` for TensorFlow container and `summarization_env` for PyTorch container). Note, that in our inference script we rely on variable `NLP_TASK` to identify which inference pipeline to run (refer to inference script for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "instance_type = \"ml.m5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_env = {\n",
    "    \"NLP_TASK\" : \"question-answering\"\n",
    "}\n",
    "\n",
    "tf_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.8\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "tensorflow_container = {\n",
    "    \"ContainerHostname\": \"tensorflow-distilbert-qa\",\n",
    "    \"Image\": tf_inference_image_uri,\n",
    "    \"ModelDataUrl\": qa_model_data,\n",
    "    \"Environment\" : qa_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_env = {\n",
    "    \"NLP_TASK\" : \"summarization\",\n",
    "    \"SAGEMAKER_PROGRAM\" : \"inference.py\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": summarization_model_data,\n",
    "}\n",
    "\n",
    "pt_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "pytorch_container = {\n",
    "    \"ContainerHostname\": \"pytorch-bart-summarizer\",\n",
    "    \"Image\": pt_inference_image_uri,\n",
    "    \"ModelDataUrl\": summarization_model_data,\n",
    "    \"Environment\" : summarization_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating MCE Endpoint\n",
    "\n",
    "To create model, endpoint configuration, and endpoint, we use SageMaker boto3 client. Run cells below for this. Note, that we supply both TensorFlow container and Pytorch container to a single model. We also set endpoint mode to `Direct`, so we can directly invoce both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sm_client = sagemaker_session.sagemaker_client # SageMaker boto3 client\n",
    "\n",
    "unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n",
    "\n",
    "model_name = f\"mce-nlp-model-{unique_id}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[tensorflow_container, pytorch_container],\n",
    "    InferenceExecutionConfig={\"Mode\": \"Direct\"},\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-ep-config\"\n",
    "\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# Code to wait for MCE deployment completion\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference \n",
    "\n",
    "Once MCE endpoint is deployed, we can run inference for Q&A and Summarization models. For this, we use a paragraph about Amazon rain forest. We expect that summarization model will be able to condense the article into shorter paragraph, while Q&A model will be able to provide us with answer on the question based on the input article.\n",
    "\n",
    "Run the cell below to define article and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "article = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What is Spanish name for Amazon?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Q&A model is implemented in TensorFlow framework and requries initial preparation to match TensorFlow Serving model signature. Run cell below to tokenize text and form payload according to model signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preparing data for TF Serving format\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "max_length = 384\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "encoded_input = tokenizer(question, article, padding='max_length', max_length=max_length)\n",
    "encoded_input = dict(encoded_input)\n",
    "qa_inputs = [{\"input_ids\": np.array(encoded_input[\"input_ids\"]).tolist(), \"attention_mask\":np.array(encoded_input[\"attention_mask\"]).tolist()}]\n",
    "qa_inputs = {\"instances\" : qa_inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can send inference request to TensorFlow endpoint.  Note that we supply the `TargetContainerHostname` header so that SageMaker knows where to route our inference request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "tf_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"tensorflow-distilbert-qa\",\n",
    "    Body=json.dumps(qa_inputs),\n",
    ")\n",
    "\n",
    "# Processing predictions\n",
    "\n",
    "predictions = json.loads(tf_response[\"Body\"].read().decode())\n",
    "answer_start_index = int(tf.math.argmax(predictions['predictions'][0]['output_0']))\n",
    "answer_end_index = int(tf.math.argmax(predictions['predictions'][0]['output_1']))\n",
    "\n",
    "predict_answer_tokens = encoded_input[\"input_ids\"][answer_start_index : answer_end_index + 1]\n",
    "tf_response = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}, answer: {tf_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cell below to get summary of text using PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_input = {\"article\":article, \"max_length\":100}\n",
    "\n",
    "pt_result = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"pytorch-bart-summarizer\", \n",
    "    Body=json.dumps(summarization_input),\n",
    ")\n",
    "\n",
    "print(pt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Clean up\n",
    "\n",
    "Run cell below to delete cloud resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName = model_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
