{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Container endpoints\n",
    "\n",
    "In this example we will deploy two different models for summarization and Q&A tasks.\n",
    "Please note that loading and packaging models may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifacts for DistilBert model for Question-Answering task\n",
    "\n",
    "! mkdir -p distilbert-base-uncased-distilled-squad/1\n",
    "! mkdir -p distilbert-base-uncased-distilled-squad/code\n",
    "\n",
    "! wget https://huggingface.co/distilbert-base-cased-distilled-squad/resolve/main/saved_model.tar.gz\n",
    "! tar -zxvf saved_model.tar.gz -C distilbert-base-uncased-distilled-squad/1\n",
    "\n",
    "! cp 1_src/inference.py distilbert-base-uncased-distilled-squad/code\n",
    "! cp 1_src/requirements.txt distilbert-base-uncased-distilled-squad/code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download artifacts for Bart model for Summarization task\n",
    "\n",
    "! mkdir -p distilbart-cnn-6-6/code\n",
    "! wget https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/pytorch_model.bin -P distilbart-cnn-6-6\n",
    "! wget https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/tokenizer.json -P distilbart-cnn-6-6\n",
    "! wget https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/vocab.json -P distilbart-cnn-6-6\n",
    "! wget https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/config.json -P distilbart-cnn-6-6\n",
    "! wget https://huggingface.co/sshleifer/distilbart-cnn-6-6/resolve/main/merges.txt -P distilbart-cnn-6-6\n",
    "! cp 1_src/inference.py distilbart-cnn-6-6/code\n",
    "! cp 1_src/requirements.txt distilbart-cnn-6-6/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C \"$PWD\" -czf distilbart-cnn-6-6.tar.gz distilbart-cnn-6-6/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'multi-container'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))\n",
    "\n",
    "summarization_model_data = sagemaker_session.upload_data('distilbart-cnn-6-6.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_src/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['attention_mask'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 384)\n",
      "      name: serving_default_attention_mask:0\n",
      "  inputs['input_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 384)\n",
      "      name: serving_default_input_ids:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_0'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 384)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 384)\n",
      "      name: StatefulPartitionedCall:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "### For TensorFlow we need to check signature of the model\n",
    "\n",
    "! saved_model_cli show --dir distilbert-base-uncased-distilled-squad/1 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Multi Container Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "instance_type = \"ml.m5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_env = {\n",
    "    \"NLP_TASK\" : \"summarization\",\n",
    "    \"SAGEMAKER_PROGRAM\" : \"inference.py\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": summarization_model_data,\n",
    "}\n",
    "\n",
    "pt_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.9.0\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "pytorch_container = {\n",
    "    \"ContainerHostname\": \"pytorch-bart-summarizer\",\n",
    "    \"Image\": pt_inference_image_uri,\n",
    "    \"ModelDataUrl\": summarization_model_data,\n",
    "    \"Environment\" : summarization_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_env = {\n",
    "    \"NLP_TASK\" : \"question-answering\"\n",
    "}\n",
    "\n",
    "tf_inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.8\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "tensorflow_container = {\n",
    "    \"ContainerHostname\": \"tensorflow-distilbert-qa\",\n",
    "    \"Image\": tf_inference_image_uri,\n",
    "    \"ModelDataUrl\": qa_model_data,\n",
    "    \"Environment\" : qa_env\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ContainerHostname': 'pytorch-bart-summarizer', 'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.8.1-cpu-py36', 'ModelDataUrl': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/distilbart-cnn-6-6.tar.gz', 'Environment': {'NLP_TASK': 'summarization', 'SAGEMAKER_PROGRAM': 'inference.py', 'SAGEMAKER_SUBMIT_DIRECTORY': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/distilbart-cnn-6-6.tar.gz'}}\n",
      "{'ContainerHostname': 'tensorflow-distilbert-qa', 'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.8-cpu', 'ModelDataUrl': 's3://sagemaker-us-east-1-941656036254/multi-container/model-artifacts/distilbert-base-uncased-distilled-squad.tar.gz', 'Environment': {'NLP_TASK': 'question-answering'}}\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_container)\n",
    "print(tensorflow_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.sagemaker_client\n",
    "runtime_sm_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n",
    "\n",
    "model_name = f\"mce-nlp-model-{unique_id}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[tensorflow_container, pytorch_container],\n",
    "    InferenceExecutionConfig={\"Mode\": \"Direct\"},\n",
    "    ExecutionRoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-ep-config\"\n",
    "\n",
    "endpoint_config = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"prod\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InstanceType\": instance_type,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "endpoint = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mce-nlp-model-2022-08-1609-34-19-ep\n",
      "mce-nlp-model-2022-08-1609-34-19\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_name)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multi Container Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "article = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What is Spanish name for Amazon?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_input = {\"article\":article, \"max_length\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#  preparing data for TF Serving format\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "max_length = 384\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "encoded_input = tokenizer(question, article, padding='max_length', max_length=max_length)\n",
    "encoded_input = dict(encoded_input)\n",
    "qa_inputs = [{\"input_ids\": np.array(encoded_input[\"input_ids\"]).tolist(), \"attention_mask\":np.array(encoded_input[\"attention_mask\"]).tolist()}]\n",
    "qa_inputs = {\"instances\" : qa_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf_response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"tensorflow-distilbert-qa\",\n",
    "    Body=json.dumps(qa_inputs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.loads(tf_response[\"Body\"].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Spanish name for Amazon?, answer: Selva Amazónica\n"
     ]
    }
   ],
   "source": [
    "answer_start_index = int(tf.math.argmax(predictions['predictions'][0]['output_0']))\n",
    "answer_end_index = int(tf.math.argmax(predictions['predictions'][0]['output_1']))\n",
    "\n",
    "predict_answer_tokens = encoded_input[\"input_ids\"][answer_start_index : answer_end_index + 1]\n",
    "tf_response = tokenizer.decode(predict_answer_tokens)\n",
    "\n",
    "print(f\"Question: {question}, answer: {tf_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_result = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetContainerHostname=\"pytorch-bart-summarizer\", \n",
    "    Body=json.dumps(summarization_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
