{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SageMaker Production Variants for A/B testing\n",
    "\n",
    "A **Production Variant** is a SageMaker-specific concept that defines a combination of the model, its container, and the resources required to run this model. As such, this is an extremely flexible concept that can be used for different use cases, such as the following:\n",
    "- Different model versions with the same runtime and resource requirements\n",
    "- Different models with different runtimes and/or resource requirements\n",
    "- The same model with different runtimes and/or resource requirements\n",
    "\n",
    "Additionally, as part of the variant configuration, you also define its traffic weights, which can be then updated without them having any impact on endpoint availability. Once deployed, the production variant can be invoked directly (so you can bypass SageMaker traffic shaping) or as part of the SageMaker endpoint call (then, SageMaker traffic shaping is not bypassed). \n",
    "\n",
    "In this example, we will register two different models for the same Q&A NLP task. Then, we will shape the inference traffic using the production variant weights and invoke the models directly. \n",
    "\n",
    "## Deploy Endpoint with Two Production Variants\n",
    "\n",
    "Follow steps below to prepare two different production variants with `DistilBert` and `RoBERTa` models. \n",
    "\n",
    "1. We start by identifiying appropriate container image using SageMaker `image_uris.retrieve()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "TRANSFORMER_VERSION = \"4.17.0\"\n",
    "INSTANCE_TYPE = \"ml.c5.4xlarge\"\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"huggingface\",\n",
    "    base_framework_version=f\"pytorch{PYTORCH_VERSION}\",\n",
    "    region=sagemaker.Session().boto_region_name,\n",
    "    version=TRANSFORMER_VERSION,\n",
    "    py_version=PYTHON_VERSION,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "print(f\"Container to be used: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we create two HuggingFace models objects which defines which model to download from HuggingFace Model Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_name = \"DistilBERT\"\n",
    "\n",
    "model1_env = {\n",
    "    'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad',\n",
    "    'HF_TASK':'question-answering'\n",
    "}\n",
    "\n",
    "model1 = HuggingFaceModel(\n",
    "   name=model1_name,\n",
    "   env=model1_env,\n",
    "   role= role,\n",
    "   transformers_version=TRANSFORMER_VERSION,\n",
    "   pytorch_version=PYTORCH_VERSION,\n",
    "   py_version=PYTHON_VERSION,\n",
    "   image_uri=image_uri,\n",
    ")\n",
    "\n",
    "container1_def = model1.prepare_container_def(INSTANCE_TYPE)\n",
    "\n",
    "sagemaker_session.create_model(\n",
    "    name=model1_name, role=role, container_defs=container1_def\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_name = \"RoBERTa\"\n",
    "\n",
    "model2_env = {\n",
    "    'HF_MODEL_ID':'deepset/roberta-base-squad2',\n",
    "    'HF_TASK':'question-answering'\n",
    "}\n",
    "\n",
    "model2 = HuggingFaceModel(\n",
    "   name=model2_name,\n",
    "   env=model2_env,\n",
    "   role= role,\n",
    "   transformers_version=TRANSFORMER_VERSION,\n",
    "   pytorch_version=PYTORCH_VERSION,\n",
    "   py_version=PYTHON_VERSION,\n",
    "   image_uri=image_uri,\n",
    ")\n",
    "\n",
    "container2_def = model2.prepare_container_def(INSTANCE_TYPE)\n",
    "\n",
    "sagemaker_session.create_model(\n",
    "    name=model2_name, role=role, container_defs=container2_def\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we will create two different endpoint variants. We start with the equal `initial_weight` parameter, which tells SageMaker that inference traffic should split evenly between model variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "variant1 = production_variant(\n",
    "    model_name=model1_name,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant1\",\n",
    "    initial_weight=1,\n",
    ")\n",
    "variant2 = production_variant(\n",
    "    model_name=model2_name,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant2\",\n",
    "    initial_weight=1,\n",
    ")\n",
    "\n",
    "print(f\"variant1 parameters = {variant1},\\nvariant2 parameters = {variant2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After that, we create the endpoint based on our configured production variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "endpoint_name = f\"ab-testing-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "print(f\"EndpointName={endpoint_name}\")\n",
    "\n",
    "sagemaker_session.endpoint_from_production_variants(\n",
    "    name=endpoint_name, production_variants=[variant1, variant2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Production Variants\n",
    "\n",
    "Now, let's test our endpoint with two production variants.\n",
    "\n",
    "1. Let's confirm that each production variant gets roughly 50% of inference requests as we set equal initial weights. For this we generate multiple inference requests and check production variant in the response payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "context = r\"\"\"\n",
    "The Nile is a major north-flowing river in northeastern Africa. It flows into the Mediterranean Sea. The Nile is the longest river in Africa and has historically been considered the longest river in the world, though this has been contested by research suggesting that the Amazon River is slightly longer. Of the world's major rivers, the Nile is one of the smallest, as measured by annual flow in cubic metres of water.\n",
    "\"\"\"\n",
    "\n",
    "question=\"where does the Nile flow into?\"\n",
    "\n",
    "data = {\"context\":context, \"question\":question}\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "\n",
    "# initiate results object\n",
    "results = {\"Variant1\": 0, \"Variant2\": 0, \"total_count\": 0}\n",
    "\n",
    "for i in range(20):\n",
    "    response = sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(data))\n",
    "    results[response['InvokedProductionVariant']] += 1\n",
    "    results[\"total_count\"] += 1\n",
    "\n",
    "print(f\"Invokations per endpoint variant: \\n Variant1: {results['Variant1']/results['total_count']*100}%; \\n Variant2: {results['Variant2']/results['total_count']*100}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, let's simulate situation when we want to send ~90% of the traffic to one production variant. Run the cell below to update production variants weights (changed from \"1 to 1\" to \"9 to 1\" respectively). We also added waiter method to wait for endpoint update completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\"DesiredWeight\": 1, \"VariantName\": \"Variant1\"},\n",
    "        {\"DesiredWeight\": 9, \"VariantName\": \"Variant2\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_description['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    endpoint_description = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = endpoint_description['EndpointStatus']\n",
    "    instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, let's confirm that traffic distibution changed according to weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Variant1\": 0, \"Variant2\": 0, \"total_count\": 0}\n",
    "\n",
    "for i in range(20):\n",
    "    response = sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(data))\n",
    "    results[response['InvokedProductionVariant']] += 1\n",
    "    results[\"total_count\"] += 1\n",
    "\n",
    "print(f\"Invokations per endpoint variant: \\n Variant1: {results['Variant1']/results['total_count']*100}%; \\n Variant2: {results['Variant2']/results['total_count']*100}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Clean up\n",
    "\n",
    "Run following cell to delete cloud resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_model(ModelName = model1_name)\n",
    "sm_client.delete_model(ModelName = model2_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
