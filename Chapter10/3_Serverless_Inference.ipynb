{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Inference\n",
    "\n",
    "Serverless Inference Endpoints (**\"SIE**) allows you to provision real-time inference endpoints without the need to provision and configure the underlying endpoint instances. SageMaker automatically provisions and scales the underlying available compute resources based on your inference traffic. Your SIE can scale them down to 0 in cases where there is no inference traffic.\n",
    "\n",
    "Serverless Inference is functionally similar to SageMaker real-time inference. It supports many types of inference containers, including PyTorch and TensorFlow inference containers. \n",
    "\n",
    "In this example, we will deploy the Q&A NLP model from the HuggingFace Model Hub as SIE. Follow the steps below for this.\n",
    "\n",
    "1. We start by making initial imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we need to define runtime container for serverless endpoint. For this, we can use sagemaker utility `image_uris.retrieve()`. We must provide target versions of frameworks as well as serverless configuration to identify approrpiate image. Note that in serverless config `memory_size_in_mb` parameter defines the initial memory behind your endpoint and the max_concurrency parameter defines the maximum number of concurrent invocations your endpoint can handle before inference traffic gets throttled by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "TRANSFORMER_VERSION = \"4.17.0\"\n",
    "\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=4096, max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"huggingface\",\n",
    "    base_framework_version=f\"pytorch{PYTORCH_VERSION}\",\n",
    "    region=sagemaker.Session().boto_region_name,\n",
    "    version=TRANSFORMER_VERSION,\n",
    "    py_version=PYTHON_VERSION,\n",
    "    serverless_inference_config=serverless_config,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "print(f\"Container to be used: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then, we will use the HuggingFaceModel instance to configure the model architecture and target NLP task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad',\n",
    "    'HF_TASK':'question-answering'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role= role,\n",
    "   transformers_version=TRANSFORMER_VERSION,\n",
    "   pytorch_version=PYTORCH_VERSION,\n",
    "   py_version=PYTHON_VERSION,\n",
    "   image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Finally, we deploy our model to serverless endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    serverless_inference_config=serverless_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To test our serverless endpoint, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n",
    "data = {\"context\":context, \"question\":question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictor.predict(data=data)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up resources\n",
    "\n",
    "Run following cell to delete cloud resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "huggingface_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
