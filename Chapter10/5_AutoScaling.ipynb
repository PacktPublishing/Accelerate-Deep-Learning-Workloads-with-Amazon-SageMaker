{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto scaling of SageMaker Endpoint\n",
    "\n",
    "SageMaker allows you to automatically scale out (increase the number of instances) and scale in (decrease the number of instances) for real-time endpoints and asynchronous endpoints. When inference traffic increases, scaling out maintains steady endpoint performance while keeping costs to a minimum. When inference traffic decreases, scaling in allows you to minimize the inference costs. For real-time endpoints, the minimum instance size is 1; asynchronous endpoints can scale to 0 instances. The following diagram shows this:\n",
    "\n",
    "In this example, we will learn how to apply the target tracking autoscaling policy to a real-time endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Real-time Endpoint\n",
    "\n",
    "We start by creating regular SageMaker Real-time endpoint. Follow the steps below to create an endpoint with `T5-small` model for HuggingFace model hub which can be used for different NLP texts such as summarization, translation, text classification and others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "TRANSFORMER_VERSION = \"4.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'t5-small',\n",
    "\t'HF_TASK':'translation'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\ttransformers_version=TRANSFORMER_VERSION,\n",
    "\tpytorch_version=PYTORCH_VERSION,\n",
    "\tpy_version=PYTHON_VERSION,\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint deploy, let's sent a test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict({\n",
    "\t'inputs': \"Berlin is the capital and largest city of Germany by both area and population\"\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Auto Scaling Policies\n",
    "\n",
    "Next, we apply Auto Scaling to running endpoint. For this we will create two autoscaling resources: `a scalable target` and `a scaling policy`. The scalable target defines a specific AWS resource that we want to scale using the Application Auto Scaling service. In the following code snippet, we are instantiating the client for the Application Auto Scaling service and registering our SageMaker endpoint as a scalable target with following parameters:\n",
    "- `ResourceId` parameter defines a reference to a specific endpoint and production variant.\n",
    "- `ScalableDimension` parameter for SageMaker resources always references the number of instances behind the production variant. \n",
    "- `MinCapacity` and `MaxCapacity` define the instance scaling range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "as_client = boto3.client('application-autoscaling')\n",
    " \n",
    "resource_id=f\"endpoint/{predictor.endpoint_name}/variant/AllTraffic\"\n",
    "policy_name = f'Request-ScalingPolicy-{predictor.endpoint_name}'\n",
    "scalable_dimension = 'sagemaker:variant:DesiredInstanceCount'\n",
    "\n",
    "# define scaling configuration\n",
    "response = as_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a policy for our scalable target. The scaling policy defines how endpoint should be scaled based on target metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = as_client.put_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # Threshold\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # duration until scale in\n",
    "        'ScaleOutCooldown': 60 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Scaling Endpoint\n",
    "\n",
    "Now, let's test that our endpoint can actually scale automatically according to applied above policy. For this, we need to generate sufficient inference traffic to breach the target metric value for a duration longer than the scale-out cooldown period. For this purpose, we can use the [Locust.io load testing framework](https://locust.io/), which provides a simple mechanism to mimic various load patterns. Follow the instructions in the notebook to create a Locust configuration for your endpoint and provide your AWS credentials for authorization purposes.\n",
    "\n",
    "1. We start by installing Locust Python package locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r \"../utils/load_testing/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we need to generate a config file for Locust to generate inference requests to SageMaker endpoint. Run the cell below to create configuration file. Please make sure to correctly fill following placeholder parameters:\n",
    "    - AWS region.\n",
    "    - your endpoint name.\n",
    "    - AWS access and secret keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/load_testing/config.py\n",
    "\n",
    "# provide configuration parameters\n",
    "# TODO: clean up config from personal data\n",
    "\n",
    "HOST = 'runtime.sagemaker.<USE YOUR REGION>.amazonaws.com'\n",
    "REGION = '<USE YOUR REGION>'\n",
    "# replace the url below with the sagemaker endpoint you are load testing\n",
    "ENDPOINT_NAME = \"USE YOUR ENDPOINT NAME\"\n",
    "SAGEMAKER_ENDPOINT_URL = f'https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{ENDPOINT_NAME}/invocations'\n",
    "ACCESS_KEY = '<USE YOUR AWS ACCESS KEY HERE>'\n",
    "SECRET_KEY = '<USE YOUR AWS SECRET KEY HERE>'\n",
    "# replace the context type below as per your requirements\n",
    "CONTENT_TYPE = 'application/json'\n",
    "METHOD = 'POST'\n",
    "SERVICE = 'sagemaker'\n",
    "SIGNED_HEADERS = 'content-type;host;x-amz-date'\n",
    "CANONICAL_QUERY_STRING = ''\n",
    "ALGORITHM = 'AWS4-HMAC-SHA256'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, run following command in separate console to start generating simulatenous requests to SageMaker endpoint where `u` is a number of concurrent users and `r` is a spawn rate (users per sec):\n",
    "```bash \n",
    "    locust -f ../utils/load_testing/locustfile.py --headless -u 20 -r 1 --run-time 5m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. During the load test, you can observe your endpoint status as well as the associated scaling alerts in the Amazon CloudWatch console. First, you can see that scale-out and scale-in alerts have been configured based on the provided cooldown periods and target metric value:\n",
    "\n",
    "    <img src=\"static/scaling_alerts.png\" width=\"600\">\n",
    "\n",
    "5. After the initial scale-out cooldown period has passed, the scale-out alert switches to the In alarm state, which causes the endpoint to scale out. Note that in the following screenshot, the red line is the desired value of the tracking metric, while the blue line is the number of invocations per endpoint instance:\n",
    "\n",
    "    <img src=\"static/triggered_alert.png\" width=\"600\">    \n",
    "6. After triggering scaling out, your endpoint status will change from in Service to Updating. Now, we can run the `describe_endpoint()` method to confirm that the number of instances has been increased. Since we are generating a sufficiently large concurrent load in a short period, SageMaker immediately scaled our endpoint to the maximum number of instances. Run cell below to observe how SageMaker updates instance cound behind endpoint once scaling out is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = sagemaker_session.sagemaker_client # SageMaker boto3 client\n",
    "\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "status = endpoint_description['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    status = endpoint_description['EndpointStatus']\n",
    "    instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Once we've finished loading our endpoint and cool down period has passed, we should expect our endpoint to scale in. Run the cell below to get current instance count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "print(f\"Endpoint instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Endpoint Manually\n",
    "\n",
    "SageMaker also allows you to manually update instance count behind endpoint. Let's see how this can be done.\n",
    "\n",
    "1. First we deklete scaling policy and scalable target to disable Auto Scaling for our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = as_client.delete_scaling_policy(\n",
    "    PolicyName=policy_name,\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension\n",
    ")\n",
    "\n",
    "response = as_client.deregister_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we add additional instance to current instance fleet behind the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "\n",
    "print(f\"Current instance count: {instance_count}\")\n",
    "\n",
    "target_instance_count = (int(instance_count)+1)\n",
    "\n",
    "sm_client.update_endpoint_weights_and_capacities(EndpointName=predictor.endpoint_name,\n",
    "                            DesiredWeightsAndCapacities=[\n",
    "                                {\n",
    "                                    'VariantName': 'AllTraffic',\n",
    "                                    'DesiredInstanceCount': target_instance_count\n",
    "                                }\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's observe endpoint update process and confirm that endpoint count is updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "status = endpoint_description['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Updating':\n",
    "    time.sleep(1)\n",
    "    endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "    status = endpoint_description['EndpointStatus']\n",
    "    instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Current Instance count: {instance_count}\")\n",
    "\n",
    "endpoint_description = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "instance_count = endpoint_description['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "print(f\"Current instance count: {instance_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Clean up\n",
    "\n",
    "Run following cell to delete cloud resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=predictor.endpoint_name)\n",
    "sm_client.delete_model(ModelName = huggingface_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
