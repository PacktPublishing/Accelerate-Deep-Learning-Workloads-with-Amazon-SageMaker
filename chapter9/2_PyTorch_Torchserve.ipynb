{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchserve on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 2_src/pipeline-predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-17 08:23:21--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 2600:1f18:147f:e850:db35:e0c7:187b:c770, 2600:1f18:147f:e800:afa4:a769:1b42:e343, 52.202.207.64, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:1f18:147f:e850:db35:e0c7:187b:c770|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/distilbert-base-uncased-distilled-squad/22cbcd1c2d2e3190cdb7658f0fd330e4c2bc18056a1e6612a4430197b7368372?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22 [following]\n",
      "--2022-08-17 08:23:21--  https://cdn-lfs.huggingface.co/distilbert-base-uncased-distilled-squad/22cbcd1c2d2e3190cdb7658f0fd330e4c2bc18056a1e6612a4430197b7368372?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 2600:9000:23ca:f000:11:f807:5180:93a1, 2600:9000:23ca:7600:11:f807:5180:93a1, 2600:9000:23ca:d400:11:f807:5180:93a1, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:23ca:f000:11:f807:5180:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 265481570 (253M) [application/macbinary]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>] 253.18M  34.7MB/s    in 7.2s    \n",
      "\n",
      "2022-08-17 08:23:28 (35.3 MB/s) - ‘distilbert-base-uncased-distilled-squad/pytorch_model.bin’ saved [265481570/265481570]\n",
      "\n",
      "--2022-08-17 08:23:29--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json\n",
      "Resolving huggingface.co (huggingface.co)... 2600:1f18:147f:e800:afa4:a769:1b42:e343, 2600:1f18:147f:e850:db35:e0c7:187b:c770, 52.202.207.64, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:1f18:147f:e800:afa4:a769:1b42:e343|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 466062 (455K) [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/tokenizer.json’\n",
      "\n",
      "tokenizer.json      100%[===================>] 455.14K   749KB/s    in 0.6s    \n",
      "\n",
      "2022-08-17 08:23:30 (749 KB/s) - ‘distilbert-base-uncased-distilled-squad/tokenizer.json’ saved [466062/466062]\n",
      "\n",
      "--2022-08-17 08:23:31--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 2600:1f18:147f:e850:db35:e0c7:187b:c770, 2600:1f18:147f:e800:afa4:a769:1b42:e343, 52.202.207.64, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:1f18:147f:e850:db35:e0c7:187b:c770|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28 [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/tokenizer_config.json’\n",
      "\n",
      "tokenizer_config.js 100%[===================>]      28  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-17 08:23:31 (26.7 MB/s) - ‘distilbert-base-uncased-distilled-squad/tokenizer_config.json’ saved [28/28]\n",
      "\n",
      "--2022-08-17 08:23:32--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt\n",
      "Resolving huggingface.co (huggingface.co)... 2600:1f18:147f:e850:db35:e0c7:187b:c770, 2600:1f18:147f:e800:afa4:a769:1b42:e343, 52.202.207.64, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:1f18:147f:e850:db35:e0c7:187b:c770|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 226.08K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-17 08:23:32 (2.20 MB/s) - ‘distilbert-base-uncased-distilled-squad/vocab.txt’ saved [231508/231508]\n",
      "\n",
      "--2022-08-17 08:23:33--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json\n",
      "Resolving huggingface.co (huggingface.co)... 2600:1f18:147f:e850:db35:e0c7:187b:c770, 2600:1f18:147f:e800:afa4:a769:1b42:e343, 52.202.207.64, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:1f18:147f:e850:db35:e0c7:187b:c770|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451 [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/config.json’\n",
      "\n",
      "config.json         100%[===================>]     451  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-17 08:23:33 (430 MB/s) - ‘distilbert-base-uncased-distilled-squad/config.json’ saved [451/451]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download artifacts for Q&A model\n",
    "\n",
    "! mkdir distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json -P distilbert-base-uncased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz  distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#role = get_execution_role()  # TODO: replace it\n",
    "role=\"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\" # TODO: this has to be replaced\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-941656036254/torchserve/model-artifacts/distilbert-base-uncased-distilled-squad.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vdabravolski/miniconda/envs/sagemaker2/lib/python3.9/site-packages/sagemaker/local/local_session.py:562: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self.config = yaml.load(open(sagemaker_config_file, \"r\"))\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_local_session = LocalSession()\n",
    "\n",
    "sagemaker_local_session.config = {'local': {'local_code': True}}\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = \"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env = {    \"SAGEMAKER_TS_BATCH_SIZE\": \"3\",\n",
    "    \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"100000\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Note: You can update the 'torchserve-predictor.py' file as needed according to the model you want to use (ie BERT) \n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                   role=role, \n",
    "                   entry_point='pipeline-predictor.py',\n",
    "                   source_dir='2_src',\n",
    "                   framework_version='1.9.0',\n",
    "                   py_version='py38',\n",
    "                   env=env,\n",
    "                   sagemaker_session=sagemaker_local_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to zu7r4nfrsk-algo-1-9oixb\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Sagemaker TS environment variables have been set and will be used for single model endpoint.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.20.3)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (1.3.3)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Collecting transformers\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "     |████████████████████████████████| 4.7 MB 908 kB/s            \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.3)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (2.26.0)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading regex-2022.7.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
      "     |████████████████████████████████| 768 kB 571 kB/s            \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (20.4)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (4.62.2)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Collecting filelock\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Collecting huggingface-hub<1.0,>=0.1.0\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "     |████████████████████████████████| 101 kB 555 kB/s           \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (5.4.1)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     |████████████████████████████████| 6.6 MB 4.3 MB/s            \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[?25hCollecting packaging>=20.0\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 6.5 MB/s             \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (3.10.0.2)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (2.4.7)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->-r /opt/ml/model/code/requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (2021.10.8)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (1.26.7)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (3.2)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers->-r /opt/ml/model/code/requirements.txt (line 3)) (2.0.4)\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Installing collected packages: packaging, filelock, tokenizers, regex, huggingface-hub, transformers\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m   Attempting uninstall: packaging\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m     Found existing installation: packaging 20.4\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m     Uninstalling packaging-20.4:\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m       Successfully uninstalled packaging-20.4\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Successfully installed filelock-3.8.0 huggingface-hub-0.8.1 packaging-21.3 regex-2022.7.25 tokenizers-0.12.1 transformers-4.21.1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:06,478 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:06,650 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Torchserve version: 0.4.2\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m TS Home: /opt/conda/lib/python3.8/site-packages\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Current directory: /\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Number of CPUs: 6\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Max heap size: 1486 M\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Python executable: /opt/conda/bin/python3.8\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Log dir: /logs\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Netty threads: 0\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Default workers per model: 6\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Enable metrics API: true\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Workflow Store: /.sagemaker/ts/models\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Model config: {\"model\": {\"1.0\": {\"defaultVersion\": true,\"marName\": \"model.mar\",\"minWorkers\": 1,\"maxWorkers\": 1,\"batchSize\": 3,\"maxBatchDelay\": 100000,\"responseTimeout\": 60}}}\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:06,668 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:06,670 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:13,894 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:13,931 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,177 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,177 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,195 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,781 [INFO ] W-9005-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,784 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,790 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]48\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,790 [INFO ] W-9005-model_1-stdout MODEL_LOG - [PID]51\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,790 [INFO ] W-9005-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,791 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,793 [INFO ] W-9005-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,794 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,813 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,813 [INFO ] W-9005-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,849 [INFO ] W-9003-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,851 [INFO ] W-9003-model_1-stdout MODEL_LOG - [PID]52\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,852 [INFO ] W-9003-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,852 [INFO ] W-9003-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,853 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,863 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,864 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]49\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,866 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,866 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,866 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,881 [INFO ] W-9005-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,893 [INFO ] W-9003-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,897 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,900 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,913 [INFO ] W-9002-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,914 [INFO ] W-9002-model_1-stdout MODEL_LOG - [PID]56\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,914 [INFO ] W-9002-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,915 [INFO ] W-9002-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,915 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,921 [INFO ] W-9002-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,925 [INFO ] W-9004-model_1-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,926 [INFO ] W-9004-model_1-stdout MODEL_LOG - [PID]50\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,927 [INFO ] W-9004-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,927 [INFO ] W-9004-model_1-stdout MODEL_LOG - Python runtime: 3.8.10\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,928 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:14,947 [INFO ] W-9004-model_1-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m Model server started.\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,314 [INFO ] W-9002-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,314 [INFO ] W-9005-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,318 [INFO ] W-9003-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,325 [INFO ] W-9000-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,349 [INFO ] W-9004-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,349 [INFO ] W-9001-model_1-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,511 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,514 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:176.7337989807129|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,515 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:107.02857208251953|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,516 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:37.7|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,520 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4146.59765625|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,520 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1169.17578125|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:15,521 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:30.2|#Level:Host|#hostname:6a149ec7ff42,timestamp:1660742475\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:17,362 [INFO ] pool-1-thread-7 ACCESS_LOG - /172.18.0.1:62690 \"GET /ping HTTP/1.1\" 200 27\n",
      "!\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-17 13:21:17,372 [INFO ] pool-1-thread-7 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:6a149ec7ff42,timestamp:null\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "local_predictor = model.deploy(initial_instance_count=1, instance_type=instance_type, serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What kind of forest is Amazon?', 'context': '\\nThe Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\\n'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "context = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n",
    "\n",
    "\n",
    "data = {\"question\":question, \"context\":context}\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env = {    \"SAGEMAKER_TS_BATCH_SIZE\": \"3\",\n",
    "    \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"100000\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Note: You can update the 'torchserve-predictor.py' file as needed according to the model you want to use (ie BERT) \n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                   role=role, \n",
    "                   entry_point='pipeline-predictor.py',\n",
    "                   source_dir='2_src',\n",
    "                   framework_version='1.9.0',\n",
    "                   py_version='py38',\n",
    "                   env=env,\n",
    "                   sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.4xlarge\", serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4394714832305908,\n",
       " 'start': 238,\n",
       " 'end': 253,\n",
       " 'answer': 'moist broadleaf'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
