{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchserve on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pipeline\n",
      "\n",
      "\n",
      "JSON_CONTENT_TYPE = \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    model_pipeline = pipeline(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mquestion-answering\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        model=os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased-distilled-squad\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "    )\n",
      "    \u001b[34mreturn\u001b[39;49;00m model_pipeline\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
      "    \u001b[36mprint\u001b[39;49;00m(content_type)\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\n",
      "        input_data = json.loads(serialized_input_data)\n",
      "        \u001b[34mreturn\u001b[39;49;00m input_data\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + content_type)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model_pipeline):\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        results = model_pipeline(\n",
      "            question=input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mquestion\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], context=input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mcontext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        )\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(e)\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(e)\n",
      "    \u001b[34mreturn\u001b[39;49;00m results\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept=JSON_CONTENT_TYPE):\n",
      "    \u001b[34mif\u001b[39;49;00m accept == JSON_CONTENT_TYPE:\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps(prediction_output), accept\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + accept)\n"
     ]
    }
   ],
   "source": [
    "! pygmentize 2_src/pipeline_predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,458 [INFO ] pool-2-thread-2 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - DiskAvailable.Gigabytes:162.69421005249023|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - DiskUsage.Gigabytes:121.06816101074219|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - DiskUtilization.Percent:42.7|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - MemoryAvailable.Megabytes:2416.23828125|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUsed.Megabytes:2950.66796875|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:17:08,459 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUtilization.Percent:59.3|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131028\n",
      "--2022-08-21 21:17:10--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 52.202.207.64, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|52.202.207.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/distilbert-base-uncased-distilled-squad/22cbcd1c2d2e3190cdb7658f0fd330e4c2bc18056a1e6612a4430197b7368372?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22 [following]\n",
      "--2022-08-21 21:17:10--  https://cdn-lfs.huggingface.co/distilbert-base-uncased-distilled-squad/22cbcd1c2d2e3190cdb7658f0fd330e4c2bc18056a1e6612a4430197b7368372?response-content-disposition=attachment%3B%20filename%3D%22pytorch_model.bin%22\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.139.29.77, 108.139.29.38, 108.139.29.114, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.139.29.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 265481570 (253M) [application/macbinary]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>] 253.18M  31.7MB/s    in 8.0s    \n",
      "\n",
      "2022-08-21 21:17:18 (31.8 MB/s) - ‘distilbert-base-uncased-distilled-squad/pytorch_model.bin’ saved [265481570/265481570]\n",
      "\n",
      "--2022-08-21 21:17:19--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json\n",
      "Resolving huggingface.co (huggingface.co)... 52.202.207.64, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|52.202.207.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 466062 (455K) [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/tokenizer.json’\n",
      "\n",
      "tokenizer.json      100%[===================>] 455.14K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-08-21 21:17:19 (6.56 MB/s) - ‘distilbert-base-uncased-distilled-squad/tokenizer.json’ saved [466062/466062]\n",
      "\n",
      "--2022-08-21 21:17:19--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 52.202.207.64, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|52.202.207.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28 [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/tokenizer_config.json’\n",
      "\n",
      "tokenizer_config.js 100%[===================>]      28  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-21 21:17:20 (13.4 MB/s) - ‘distilbert-base-uncased-distilled-squad/tokenizer_config.json’ saved [28/28]\n",
      "\n",
      "--2022-08-21 21:17:20--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt\n",
      "Resolving huggingface.co (huggingface.co)... 52.202.207.64, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|52.202.207.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 226.08K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-08-21 21:17:20 (4.16 MB/s) - ‘distilbert-base-uncased-distilled-squad/vocab.txt’ saved [231508/231508]\n",
      "\n",
      "--2022-08-21 21:17:21--  https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json\n",
      "Resolving huggingface.co (huggingface.co)... 52.202.207.64, 52.6.16.131\n",
      "Connecting to huggingface.co (huggingface.co)|52.202.207.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451 [text/plain]\n",
      "Saving to: ‘distilbert-base-uncased-distilled-squad/config.json’\n",
      "\n",
      "config.json         100%[===================>]     451  --.-KB/s    in 0s      \n",
      "\n",
      "2022-08-21 21:17:21 (47.8 MB/s) - ‘distilbert-base-uncased-distilled-squad/config.json’ saved [451/451]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download artifacts for Q&A model\n",
    "\n",
    "! mkdir distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt -P distilbert-base-uncased-distilled-squad\n",
    "! wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json -P distilbert-base-uncased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,388 [INFO ] pool-2-thread-2 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,390 [INFO ] pool-2-thread-2 TS_METRICS - DiskAvailable.Gigabytes:162.69420623779297|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,390 [INFO ] pool-2-thread-2 TS_METRICS - DiskUsage.Gigabytes:121.06816482543945|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,390 [INFO ] pool-2-thread-2 TS_METRICS - DiskUtilization.Percent:42.7|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,391 [INFO ] pool-2-thread-2 TS_METRICS - MemoryAvailable.Megabytes:2417.24609375|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,392 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUsed.Megabytes:2949.65625|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:18:08,392 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUtilization.Percent:59.3|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131088\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,319 [INFO ] pool-2-thread-2 TS_METRICS - CPUUtilization.Percent:50.0|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - DiskAvailable.Gigabytes:162.69420623779297|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - DiskUsage.Gigabytes:121.06816482543945|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - DiskUtilization.Percent:42.7|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - MemoryAvailable.Megabytes:2416.51953125|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUsed.Megabytes:2950.38671875|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n",
      "\u001b[36mzu7r4nfrsk-algo-1-9oixb |\u001b[0m 2022-08-22 01:19:08,320 [INFO ] pool-2-thread-2 TS_METRICS - MemoryUtilization.Percent:59.3|#Level:Host|#hostname:6a149ec7ff42,timestamp:1661131148\n"
     ]
    }
   ],
   "source": [
    "!tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz  distilbert-base-uncased-distilled-squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model-artifacts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-941656036254/torchserve/model-artifacts/distilbert-base-uncased-distilled-squad.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env = {\n",
    "    \"SAGEMAKER_TS_BATCH_SIZE\": \"3\",\n",
    "    \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"1000\",\n",
    "    \"SAGEMAKER_TS_RESPONSE_TIMEOUT\" : \"120\",\n",
    "    \"SAGEMAKER_TS_MIN_WORKERS\" : \"1\",\n",
    "    \"SAGEMAKER_TS_MAX_WORKERS\" : \"2\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Note: You can update the 'torchserve-predictor.py' file as needed according to the model you want to use (ie BERT) \n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                   role=role, \n",
    "                   entry_point='pipeline_predictor.py',\n",
    "                   source_dir='2_src',\n",
    "                   framework_version='1.9.0',\n",
    "                   py_version='py38',\n",
    "                   env=env,\n",
    "                   sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "context = r\"\"\"\n",
    "The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\n",
    "\"\"\"\n",
    "\n",
    "question=\"What kind of forest is Amazon?\"\n",
    "\n",
    "\n",
    "data = {\"question\":question, \"context\":context}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "remote_predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.4xlarge\", serializer=JSONSerializer(), deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
