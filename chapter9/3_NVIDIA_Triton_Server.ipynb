{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ca49e0-7b9d-42e4-bc8d-c97b413e1145",
   "metadata": {},
   "source": [
    "# Triton Model Server\n",
    "\n",
    "!!!!!!!!! Note, that this will only work on Linux (requirement of Triton Client libs)\n",
    "\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3659a2-dfbc-4be5-8e00-0fde0bb3bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Looking in links: https://github.com/pytorch/TensorRT/releases\n",
      "Collecting torch-tensorrt\n",
      "  Downloading https://github.com/pytorch/TensorRT/releases/download/v1.1.0/torch_tensorrt-1.1.0-cp38-cp38-linux_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.11.0+cu113<1.12.0\n",
      "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch>=1.11.0+cu113<1.12.0->torch-tensorrt) (4.0.0)\n",
      "Installing collected packages: torch, torch-tensorrt\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.0\n",
      "    Uninstalling torch-1.10.0:\n",
      "      Successfully uninstalled torch-1.10.0\n",
      "Successfully installed torch-1.12.1 torch-tensorrt-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install torch-tensorrt -f https://github.com/pytorch/TensorRT/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cba2-28c1-4334-ba3d-dfde4efbbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source activate pytorch_p38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7f4a70-f8d9-4906-b6e7-b2fe8e367ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter9\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b83798-0c50-4711-b8d6-3d37a1f33370",
   "metadata": {},
   "source": [
    "## Run TensorRT compiler runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07906c4-e6cf-43f0-b0c7-dac3eed91a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it --rm -v /home/ec2-user/SageMaker/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter9/3_src:/workspace/3_src nvcr.io/nvidia/pytorch:22.06-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a9f96-6b6b-4e4c-8597-e0fdf93af53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside container \n",
    "\n",
    "python 3_src/compile_tensorrt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527d733-b215-4ae6-a36c-b8c920a79429",
   "metadata": {},
   "source": [
    "# Packaging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec5e1ca-bebe-4932-a4f6-3c579f4a8dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter9\n"
     ]
    }
   ],
   "source": [
    "! pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a8931d9-bafe-4469-8a9e-3746554f4a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./3_src/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./3_src/resnet50/config.pbtxt\n",
    "name: \"resnet50\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 3, 224, 224 ]\n",
    "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1, 1000 ,1, 1]\n",
    "    reshape { shape: [ 1, 1000 ] }\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e40fa3e-2eb5-4de9-bc06-dab72df696d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_src/resnet50/\n",
      "3_src/resnet50/1/\n",
      "3_src/resnet50/1/model.pt\n",
      "3_src/resnet50/.ipynb_checkpoints/\n",
      "3_src/resnet50/.ipynb_checkpoints/config-checkpoint.pbtxt\n",
      "3_src/resnet50/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# here we need to cd first inside 3_src and then archive\n",
    "!tar -czvf resnet50.tar.gz resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee37bec3-30d7-4ffb-b88d-357c790c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role=\"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\" # TODO: this has to be replaced\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'triton'\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732b796e-557b-4406-9ec6-f097ca67e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_data = sagemaker_session.upload_data(\"resnet50.tar.gz\",\n",
    "                                           bucket,\n",
    "                                           prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd33120-d059-45d7-9975-24cf3f640f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = \"s3://sagemaker-us-east-1-941656036254/triton/resnet50.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bde4d-f4f4-4367-a787-496386b08ec3",
   "metadata": {},
   "source": [
    "# Deploy inference endpoint\n",
    "\n",
    "Triton containers are as of now not supported by SageMaker Python SDK. Hence, we will use `boto3` for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8445c78-72f8-4df7-9119-5936782acb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json,  time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c57bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:21.08-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64184dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:21.08-py3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1715f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:941656036254:model/triton-resnet50-2022-08-09-19-46-04\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"model\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe7ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:941656036254:endpoint-config/triton-resnet50-2022-08-09-19-46-05\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78894fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:941656036254:endpoint/triton-resnet50-2022-08-09-19-46-06\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"triton-resnet50-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199cfde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Failed\n",
      "Arn: arn:aws:sagemaker:us-east-1:941656036254:endpoint/triton-resnet50-2022-08-09-19-46-06\n",
      "Status: Failed\n"
     ]
    }
   ],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdd47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b24d85",
   "metadata": {},
   "source": [
    "# Creating Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ce2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget  -O img1.jpg \"https://www.hakaimagazine.com/wp-content/uploads/header-gulf-birds.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a67ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d46986",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
