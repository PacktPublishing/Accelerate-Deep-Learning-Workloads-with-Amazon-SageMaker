{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab865e98",
   "metadata": {},
   "source": [
    "# Extending SageMaker Training Container\n",
    "\n",
    "## Overview\n",
    "In this example we will learn how to extend pre-built SageMaker containers. This can be beneficial in certain scenarios, such as:\n",
    "- you need to add additional dependencies (for instance, ones which needs to be compiled from sources) or significantly re-configure runtime environment (e.g., update CUDA version or configuration).\n",
    "- you want to minimize development and testing efforts of your container and rely for most part on tested by AWS functionality of base container.\n",
    "\n",
    "In this notebook we will learn how to extend SageMaker container as a base image for your custom container image. We will modify our runtime environment and install latest HuggingFace Transformer framework from GitHub `main` branch.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. This sample assumes that you have AWS CLI v2 installed. Refer to this article for installation details: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    "2. To push containers to private Elastic Container Registry service (\"ECR\"), make sure that your current IAM role has enough permissions for this operation. Refer to this article for details: https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-push.html\n",
    "\n",
    "\n",
    "## Developing Training Container\n",
    "\n",
    "First of, we need to identify which base image we will use. AWS publishes all available Deep Learning containers here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "Since we plan to use re-install from scratch HugggingFace Transformer library anyway, we may choose PyTorch base image. We start by retrieving URI of SageMaker PyTorch training container. For this, we first define framework versions. Then use `image_uris.retrieve()` utility to get container URI according to specificed Python and PyTorch versions and target training instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "INSTANCE_TYPE = \"ml.p2.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b61db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "container_uri = sagemaker.image_uris.retrieve(\"pytorch\", session.boto_region_name, version=PYTORCH_VERSION, py_version=PYTHON_VERSION, image_scope=\"training\", instance_type=INSTANCE_TYPE)\n",
    "print(f\"Pre-built SageMaker DL container: {container_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0468c",
   "metadata": {},
   "source": [
    "## Reviewing Dockerfile\n",
    "\n",
    "To build a new containers we will need to:\n",
    "- create Dockerfile with runtime instructions.\n",
    "- build container image locally.\n",
    "- push new container image to Docker container registry. As a container registry in this example we will use Elastic Container Registry - a managed service from AWS well integrated with SageMaker ecosystem.\n",
    "\n",
    "Let's take a look on key components of our Dockerfile (please execute cell below to render Dockerfile content):\n",
    "- we choose to use SageMaker PyTorch image as a base. Please update base images with URI from `container_uri` directly in Dockerfile.\n",
    "- install latest HuggingFace Transformers framework form Github `main` branch.\n",
    "- copy our training script for previous lab into container.\n",
    "- define `SAGEMAKER_SUBMIT_DIRECTORY` and `SAGEMAKER_PROGRAM` environmental variables, so SageMaker knows which training script to execute at container start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize -l docker 2_sources/Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb5daf",
   "metadata": {},
   "source": [
    "### Building and Pushing Container Image\n",
    "\n",
    "Once we have our Dockerfile ready, we need to build and push container image to registry. We start by authentificating in AWS public ECR (which hosts DL containers) and your private ECR (which will host our extended image). For this, we first retrieve `account` and `region` parameters from SageMaker session instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5880343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804494d",
   "metadata": {},
   "source": [
    "Next, we perform docker login operations for public and private ECRs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ee274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28a9e7",
   "metadata": {},
   "source": [
    "Now, we are ready to build and push container to private ECR. For this, we provide as part of this repo a utility script `build_and_push.sh` to automate this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dff23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"extended-pytorch-training\"\n",
    "image_uri = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image_name}\"\n",
    "\n",
    "!./build_and_push.sh {image_name} 2_sources/Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacca02",
   "metadata": {},
   "source": [
    "### Scheduling Training Job\n",
    "\n",
    "Now, we have our extended PyTorch container in ECR, and we are ready to execute SageMaker training job. Training job configuration will be similar to Script Mode example with one noteable different: instead of `HuggingFaceEstimator` object we will use a generic `Sagemaker Estimator` which allows to work with custom images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df68939",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\":1,\n",
    "    # 2 params below may need to updated if non-GPU instances is used for training\n",
    "    \"per-device-train-batch-size\":16, \n",
    "    \"per-device-eval-batch-size\":64,\n",
    "    \"warmup-steps\":100,\n",
    "    \"logging-steps\":100,\n",
    "    \"weight-decay\":0.01    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide S3 URIs of test dataset from \"Script Mode\" example (`1_Using_SageMaker_Script_Mode.ipynb` notebook)\n",
    "train_dataset_uri=\"s3://<YOUR S3 BUCKET>/newsgroups/train_dataset.csv\"\n",
    "test_dataset_uri=\"s3://<YOUR S3 BUCKET>/newsgroups/test_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\":train_dataset_uri,\n",
    "    \"test\":test_dataset_uri\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26a20b",
   "metadata": {},
   "source": [
    "### Resource Cleanup\n",
    "\n",
    "Execute the cell below to delete cloud resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbe9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Delete container image\n",
    "ecr = boto3.client(\"ecr\")\n",
    "ecr.delete_repository(repositoryName=image_name, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8973c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you learned how to extend SageMaker PyTorch training container to address some specific runtime requirements with now code changes in training scripts and minimal development efforts.\n",
    "\n",
    "In next example we will learn how to build SageMaker-compatible container using official TensorFlow image. This approach allows for maximum flexibility while requires more development efforts."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
