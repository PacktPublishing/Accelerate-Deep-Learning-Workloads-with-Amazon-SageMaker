{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb6e548",
   "metadata": {},
   "source": [
    "# Extending SageMaker Training Container\n",
    "\n",
    "## Overview\n",
    "In this notebook we will learn how to extend SageMaker container as a base image for your custom container image. Modifying pre-build containers can be beneficial in following scenarios:\n",
    "- you need to add additional dependencies (for instance, ones which needs to be compiled from sources) or significantly re-configure runtime environment.\n",
    "- you want to minimize development and testing efforts of your container and rely for most part on tested by AWS functionality of base container.\n",
    "\n",
    "## Problem Statement\n",
    "We will re-use code assets from the our previous notebook in this chapter, where we trained and deploy NLP model to classify articles based on their content. But this time we will modify our runtime environment and install latest stable HuggingFace Transformer from Github master branch. This modification will be implemented in our custom container image.\n",
    "\n",
    "## Notice On Support\n",
    "This notebook assumes that you build container in nvidia-docker runtime environment. In other words, runtime environment with NVIDIA GPU available. If you don't have nvidia-docker runtime envrionment, you may switch to CPU-based containers. See below instance_type parameter which defines wether to use GPU or CPU versions of container.\n",
    "\n",
    "## Developing Training Container\n",
    "\n",
    "First of, we need to identify which base image we will use. AWS publishes all available Deep Learning containers here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "Since we plan to use re-install from scratch HugggingFace Transformer library anyway, we may choose PyTorch base image. We start by retrieving URI of SageMaker PyTorch training container. For this, we first define framework versions. Then use `image_uris.retrieve()` utility to get container URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb65abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "#INSTANCE_TYPE = \"ml.p2.xlarge\" # if you have runtime with nvidia-docker\n",
    "INSTANCE_TYPE = \"ml.m5.xlarge\" # uncomment this to use CPU-based instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a404c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-cpu-py38\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "container_uri = sagemaker.image_uris.retrieve(\"pytorch\", session.boto_region_name, version=PYTORCH_VERSION, py_version=PYTHON_VERSION, image_scope=\"training\", instance_type=INSTANCE_TYPE)\n",
    "print(container_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d1183",
   "metadata": {},
   "source": [
    "To build a new containers we will need to:\n",
    "- create Dockerfile with runtime instructions.\n",
    "- build container image locally.\n",
    "- push new container image to `container registry`. As a container registry in this example we will use Elastic Container Registry - a managed service from AWS well integrated with SageMaker ecosystem.\n",
    "\n",
    "\n",
    "### Reviewing Dockerfile\n",
    "Let's take a look on key components of our Dockerfile (please execute cell below to render Dockerfile content):\n",
    "- we choose to use SageMaker PyTorch image as a base.\n",
    "- install latest PyTorch and HuggingFace Transformers.\n",
    "- copy our training script for previous lab into container.\n",
    "- define `SAGEMAKER_SUBMIT_DIRECTORY` and `SAGEMAKER_PROGRAM` environmental variables, so SageMaker knows which training script to execute at container start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60eeee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[33m<REPLACE_WITH_YOUR_CONTAINER_URI>\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m\u001b[37m \u001b[39;49;00mpip3 install git+https://github.com/huggingface/transformers\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[37m \u001b[39;49;00mSAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[37m \u001b[39;49;00mSAGEMAKER_PROGRAM train.py\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m\u001b[37m \u001b[39;49;00m1_sources/train.py  \u001b[31m$SAGEMAKER_SUBMIT_DIRECTORY\u001b[39;49;00m/\u001b[31m$SAGEMAKER_PROGRAM\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize -l docker 2_sources/Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac21af",
   "metadata": {},
   "source": [
    "### Building and Pushing Container Image\n",
    "\n",
    "Once we have our Dockerfile ready, we need to build and push container image to registry. We start by authentificating with ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89fa8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b80829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "\n",
      "Logging in with your password grants your terminal complete access to your account. \n",
      "For better security, log in with a limited-privilege personal access token. Learn more at https://docs.docker.com/go/access-tokens/\n",
      "Login Succeeded\n",
      "\n",
      "Logging in with your password grants your terminal complete access to your account. \n",
      "For better security, log in with a limited-privilege personal access token. Learn more at https://docs.docker.com/go/access-tokens/\n"
     ]
    }
   ],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region $region | docker login --username AWS --password-stdin {account}.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92685b8b",
   "metadata": {},
   "source": [
    "Now, we are ready to build and push container to ECR. For this, we provide as part of this repo a utility script `build_and_push.sh` to automate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a3cd728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in region us-east-1\n",
      "\n",
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\n",
      "To see help text, you can run:\n",
      "\n",
      "  aws help\n",
      "  aws <command> help\n",
      "  aws <command> <subcommand> help\n",
      "\n",
      "aws: error: argument operation: Invalid choice, valid choices are:\n",
      "\n",
      "batch-check-layer-availability           | batch-delete-image                      \n",
      "batch-get-image                          | complete-layer-upload                   \n",
      "create-repository                        | delete-lifecycle-policy                 \n",
      "delete-registry-policy                   | delete-repository                       \n",
      "delete-repository-policy                 | describe-image-scan-findings            \n",
      "describe-images                          | describe-registry                       \n",
      "describe-repositories                    | get-authorization-token                 \n",
      "get-download-url-for-layer               | get-lifecycle-policy                    \n",
      "get-lifecycle-policy-preview             | get-registry-policy                     \n",
      "get-repository-policy                    | initiate-layer-upload                   \n",
      "list-images                              | list-tags-for-resource                  \n",
      "put-image                                | put-image-scanning-configuration        \n",
      "put-image-tag-mutability                 | put-lifecycle-policy                    \n",
      "put-registry-policy                      | put-replication-configuration           \n",
      "set-repository-policy                    | start-image-scan                        \n",
      "start-lifecycle-policy-preview           | tag-resource                            \n",
      "untag-resource                           | upload-layer-part                       \n",
      "get-login-password                       | wait                                    \n",
      "help                                    \n",
      "\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (1/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m => [internal] load .dockerignore                                          0.1s\n",
      " => => transferring context:                                               0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m => [internal] load .dockerignore                                          0.2s\n",
      " => => transferring context:                                               0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (1/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m => [internal] load .dockerignore                                          0.4s\n",
      " => => transferring context:                                               0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (1/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m => [internal] load .dockerignore                                          0.5s\n",
      " => => transferring context:                                               0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.5s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.5s\n",
      "\u001b[0m => [internal] load metadata for 763104351884.dkr.ecr.us-east-1.amazonaws  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.5s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.5s\n",
      "\u001b[0m => [internal] load metadata for 763104351884.dkr.ecr.us-east-1.amazonaws  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.5s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.5s\n",
      "\u001b[0m => [internal] load metadata for 763104351884.dkr.ecr.us-east-1.amazonaws  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.5s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.5s\n",
      "\u001b[0m\u001b[31m => ERROR [internal] load metadata for 763104351884.dkr.ecr.us-east-1.ama  0.5s\n",
      "\u001b[0m\u001b[34m => [auth] sharing credentials for 763104351884.dkr.ecr.us-east-1.amazona  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (4/4) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile.training              0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 352B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.5s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.5s\n",
      "\u001b[0m\u001b[31m => ERROR [internal] load metadata for 763104351884.dkr.ecr.us-east-1.ama  0.5s\n",
      "\u001b[0m\u001b[34m => [auth] sharing credentials for 763104351884.dkr.ecr.us-east-1.amazona  0.0s\n",
      "\u001b[0m\u001b[?25h------\n",
      " > [internal] load metadata for 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-cpu-py38:\n",
      "------\n",
      "failed to solve with frontend dockerfile.v0: failed to create LLB definition: unexpected status code [manifests 1.10.2-cpu-py38]: 403 Forbidden\n",
      "Error response from daemon: No such image: extended-pytorch-training:latest\n",
      "The push refers to repository [941656036254.dkr.ecr.us-east-1.amazonaws.com/extended-pytorch-training]\n",
      "An image does not exist locally with the tag: 941656036254.dkr.ecr.us-east-1.amazonaws.com/extended-pytorch-training\n"
     ]
    }
   ],
   "source": [
    "image_name = \"extended-pytorch-training\"\n",
    "tag = \"latest\"\n",
    "\n",
    "!./build_and_push.sh {image_name} {tag} 2_sources/Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f7c71",
   "metadata": {},
   "source": [
    "### Scheduling Training Job\n",
    "\n",
    "We have our extended PyTorch container in ECR, and we are ready to execute SageMaker training job. Training job configuration will be similar to Script Mode example with one noteable different: instead of `HuggingFaceEstimator` object we will use a generic `Sagemaker Estimator` which allows to work with custom images. Note, that you need to update parameter `iamge_uri` with reference to image URI in your ECR. You can find it by navigating to \"ECR\" service in your AWS Console and finding extended container there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825b156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\":1,\n",
    "    # 2 params below may need to updated if non-GPU instances is used for training\n",
    "    \"per-device-train-batch-size\":16, \n",
    "    \"per-device-eval-batch-size\":64,\n",
    "    \"warmup-steps\":100,\n",
    "    \"logging-steps\":100,\n",
    "    \"weight-decay\":0.01    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e45b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please provide S3 URIs of test dataset from \"Script Mode\" example\n",
    "train_dataset_uri=\"s3://<YOUR S3 BUCKET>/newsgroups/train_dataset.csv\"\n",
    "test_dataset_uri=\"s3://<YOUR S3 BUCKET>/newsgroups/test_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri=\"<UPDATE WITH YOUR IMAGE URI FROM ECR>\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\":train_dataset_uri,\n",
    "    \"test\":test_dataset_uri\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd99df",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you learned how to extend SageMaker PyTorch training container to address some specific runtime requirements with now code changes in training scripts and minimal development efforts.\n",
    "\n",
    "In next example we will learn how to build SageMaker-compatible container using official TensorFlow image. This approach allows for maximum flexibility while requires more development efforts."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
