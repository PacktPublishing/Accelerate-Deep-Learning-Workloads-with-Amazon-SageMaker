{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e348ded",
   "metadata": {},
   "source": [
    "# Develop training and inference scripts for Script Mode\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will learn how to develop training and inference scripts using HuggingFace framework. We will leverage SageMaker pre-build containers for HuggingFace (with PyTorch backend).\n",
    "\n",
    "We chose to solve a typical NLP task - text classification. We will use `20 Newsgroups` dataset which assembles ~ 20,000 newsgroup documents across 20 different newsgroups (categories).\n",
    "\n",
    "By the end of this notebook you will learn how to:\n",
    "- prepare text corpus for training and inference using Amazon SageMaker;\n",
    "- develop training script to run in pre-build HugginFace container;\n",
    "- configure and schedule training job;\n",
    "- develop inference code;\n",
    "- configure and deploy real-time inference endpoint;\n",
    "- test SageMaker endpoint.\n",
    "\n",
    "**Estimated time to review and complete this sample**: ~30-40 mins.\n",
    "\n",
    "### Notes on environment\n",
    "Please note, that this notebook was tested on SageMaker Notebook instance with latest PyTorch dependencies installed (conda environment `conda_pytorch_latest_p38`). If you are using different environment, please make sure to install following Python dependencies via PIP or Conda installers:\n",
    "- `scikit-learn`\n",
    "- `sagemaker`\n",
    "\n",
    "You can install all required packages for this example by running `pip` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d328f9",
   "metadata": {},
   "source": [
    "\n",
    "### Selecting Model Architecture\n",
    "In this example we train model which can categorize newsgroup article based on its content into one of categories.\n",
    "\n",
    "There are number of model architecture which can address this task. Existing State-of-the-art (SOTA) models are usually based on Transformer architecture. Autoregressive models like BERT and its various derivatives are suitable for this task. We will use concept known as `Transfer learning` where pre-trained model on one task is used for a new task with minimal modifications. \n",
    "\n",
    "As a baseline model we will use model architecture known as `DistilBERT` which provides high accuracy on wide variety of tasks and is considerably smaller than other models (for instance, original BERT model). To adapt model for classification task, we would need to add a classification layer which will be trained during our training to recognize articles.\n",
    "\n",
    "![title](static/finetuning.png)\n",
    "\n",
    "`HuggingFace Transformers` simplifies model selection and modification for fine-tuning:\n",
    "- provides rich model zoo with number pre-trained models and tokenizers.\n",
    "- has simple model API to modify baseline model for finetuning for specific task.\n",
    "- implements inference pipelines, combining data preprocessing and actual inference together.\n",
    "\n",
    "### Selecting SageMaker Training Containers\n",
    "\n",
    "Amazon SageMaker supports HuggingFace Transformer framework for inference and trainining. Hence, we won't need to develop any custom container. Instead we will use `Script Mode` feature to provide our custom training and inference scripts and execute them in pre-build containers. In this example we will develop intution how to develop these scripts.\n",
    "\n",
    "## Preparing Dataset\n",
    "First of, we need to acquire `20 Newsgroups` dataset. For this, we can use `sklearn` module utility. To shorten training cycle, let's choose 6 newsgroup categories (original dataset contains 20). The datasets will be loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We select 6 out of 20 diverse newsgroups\n",
    "categories = [\n",
    "    \"comp.windows.x\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.electronics\",\n",
    "    \"misc.forsale\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"alt.atheism\"\n",
    "]\n",
    "\n",
    "train_dataset = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "test_dataset = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42\n",
    "                                 )\n",
    "\n",
    "n=6 # arbitrary sample index\n",
    "print(f\"Number of training samples: {len(train_dataset['data'])}\")\n",
    "print(f\"Number of test samples: {len(test_dataset['data'])}\")\n",
    "\n",
    "print(f\"\\n=========== Sample article for category {train_dataset['target'][n]} ============== \\n\")\n",
    "print(f\"{train_dataset['data'][n]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05233b04",
   "metadata": {},
   "source": [
    "Now, we need to save selected datasets into files and upload resulting files to Amazon S3 storage.\n",
    "SageMaker will download them to training container at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for file in ['train_dataset.csv', 'test_dataset.csv']:\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.DictWriter(f, ['data', 'category_id'])\n",
    "        w.writeheader()\n",
    "        for i in range(len(train_dataset[\"data\"])):\n",
    "            w.writerow({\"data\":train_dataset[\"data\"][i], \"category_id\":train_dataset[\"target\"][i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14585a90",
   "metadata": {},
   "source": [
    "`sagemaker.Session()` object provides a set of utilizities to manage interaction with Sagemaker and AWS services in general. Let's use it to upload our data files in dedicated S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "session = sagemaker.Session()\n",
    "train_dataset_uri=session.upload_data(\"train_dataset.csv\", key_prefix=\"newsgroups\")\n",
    "test_dataset_uri=session.upload_data(\"test_dataset.csv\", key_prefix=\"newsgroups\")\n",
    "\n",
    "print(f\"Datasets are available in following locations: {train_dataset_uri} and {test_dataset_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing local files\n",
    "! rm train_dataset.csv\n",
    "! rm test_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c960",
   "metadata": {},
   "source": [
    "## Developing training script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74e8e3",
   "metadata": {},
   "source": [
    "To train our model on SageMaker, we need to provide a training script: `1_sources/train.py`. Feel free to review the script yourself. Here are several highlights of this script:\n",
    "* At training time, SageMaker starts training by calling `user_training_script --arg1 value1 --arg2 value2 ...`. Here, `arg1..N` are hyperparameters provided by users as part of training job configuration.\n",
    "- To correctly capture hyperparameters, training script need to be able to parse command line arguments. We use Python `argparse` library to do it (see lines #104-#112)\n",
    "* Method `train()` method is resposible for running end-to-end training job. It includes following components:\n",
    "    - Method `_get_tokenized_dataset()` loads and tokenizes dataset using pretrained DistilBERT tokenizer from HuggingFace library;\n",
    "    - Method  `DistilBertForSequenceClassification.from_pretrained()` downloads DistilBERT model from HuggingFace Model Hub and loads it into memory. Please note that we update default config for classification task to adjust for our chosen number of categories (line #80);\n",
    "    - Class `Trainer()` captures training configuration and starts training process (lines #86-#93);\n",
    "    - Method `model.save_pretrained()` saves trained model (line #97)\n",
    "    \n",
    "\n",
    "\n",
    "SageMaker Training toolkit setups up several environmental variables which comes handy when writing your training script:\n",
    "- `\"SM_CHANNEL_TRAIN\"` and `\"SM_CHANNEL_TEST\"` are locations where data files are download before training begins;\n",
    "- `\"SM_OUTPUT_DIR\"` is a directory for any output artifacts, SageMaker will upload this directory to S3 whether training job succeeds or fails;\n",
    "- `\"SM_MODEL_DIR\"`is a directory to store resulting model artifacts, SageMaker will also upload the model to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -O linenos=1 1_sources/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516d100",
   "metadata": {},
   "source": [
    "## Running training job\n",
    "\n",
    "Once we have training script and dependencies ready, we can proceed and schedule training job via SageMaker Python SDK.\n",
    "\n",
    "We start with import of HuggingFace Estimator object and getting IAM execution role for our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFace\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role=get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0799263",
   "metadata": {},
   "source": [
    "Next, we need to define our hyperparameters of our model and training process. These variables will be passed to our script at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\":1,\n",
    "    # 2 params below may need to updated if non-GPU instances is used for training\n",
    "    \"per-device-train-batch-size\":16, \n",
    "    \"per-device-eval-batch-size\":64,\n",
    "    \"warmup-steps\":100,\n",
    "    \"logging-steps\":100,\n",
    "    \"weight-decay\":0.01    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a239f1",
   "metadata": {},
   "source": [
    "We then define versions of Python and DL frameworks which we intend to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb113ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"py38\"\n",
    "PYTORCH_VERSION = \"1.10.2\"\n",
    "TRANSFORMER_VERSION = \"4.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = HuggingFace(\n",
    "    py_version=PYTHON_VERSION,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"1_sources\",\n",
    "    pytorch_version=PYTORCH_VERSION,\n",
    "    transformers_version=TRANSFORMER_VERSION,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True,    \n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit({\n",
    "    \"train\":train_dataset_uri,\n",
    "    \"test\":test_dataset_uri\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f3ac3",
   "metadata": {},
   "source": [
    "## Developing Inference Code\n",
    "\n",
    "Now that we have trained model, let's deploy it as SageMaker Real-Time endpoint. Similar to training job, we will use SageMaker pre-build HuggingFace container and will only provide our inference script. The inference requests will be handled by [Multi-Model Server](https://github.com/awslabs/multi-model-server) which exposes model for inference via HTTP endpoint. \n",
    "\n",
    "When using pre-build inference containers, SageMaker automatically recognizes our inference script. According to SageMaker convention, inference script should implement following methods:\n",
    "- `model_fn(model_dir)` (lines #16-#45) is executed at container start time to load model in the memory. This method takes model directory as an input argument. You can use `model_fn()` to initiatilize other components of your inference pipeline, such as tokenizer in our case. Note, that HuggingFace Transformers has a convenient Pipeline API which allows to combine data pre-processing (in our example it's text tokenization) and actual inference in a single object. Hence, instead of loaded model, we return inference pipeline (line #45).\n",
    "- `transform_fn(inference_pipeline, data, content_type, accept_type)` is responsible for running actual inference (line #48). Since we are communicating with end-client via HTTP, we also need to deserialize inference request and serialize model response. In our sample example we expect JSON payload and return back JSON payload, however, any other formats based on the requirements (e.g. CSV, Protobuf).\n",
    "\n",
    "Sometimes combining deserialization, inference, and serialization in a single method can be undesirable. For such cases, SageMaker supports more granular API:\n",
    "- `input_fn(request_body, request_content_type)` performs deserialization.\n",
    "- `predict_fn(deser_input, model)` performs model inference.\n",
    "- `output_fn(prediction, response_content_type)` performs serialization of model predictions.\n",
    "\n",
    "Note, that `transform_fn()` and `input_fn(); predict_fn(); output_fn()` are mutually exclusive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize -O linenos=1 1_sources/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b57fc",
   "metadata": {},
   "source": [
    "## Deploying Inference Endpoint\n",
    "\n",
    "Now we are ready to deploy and test our Newsgroup Classification endpoint. We can use method `estimator.create_model()` to prepare model deployment. SageMaker automatically identifies trained model artifacts from previous training job. Additionally, we need to supply reference to inference script. SageMaker will automatically package our script and all artifacts under `source_dir` and will upload it to inference instance along with our trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.estimator import HuggingFaceModel\n",
    "\n",
    "model = estimator.create_model(role=role, \n",
    "                               entry_point=\"inference.py\", \n",
    "                               source_dir=\"1_sources\",\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae67005",
   "metadata": {},
   "source": [
    "Next, we define parameters of our endpoint such as number and type of instances behind it. Remember, SageMaker supports horizontal scaling of your inference endpoints! `model.deploy()` method starts inference deployment (which usually takes several minutes) and returns `Predictor` object to run inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbdf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e7b39",
   "metadata": {},
   "source": [
    "Now that endpoint is deployed, let's test it out. Note that we don't expect stellar performance, since model is likely undertrained because we only trained for single epoch to shorten training cycle. However, we expect that model will get most predictions right. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    sample_id = random.randint(0, len(test_dataset['data']))\n",
    "    prediction = predictor.predict([test_dataset['data'][sample_id]])\n",
    "    print(f\"Sample index: {sample_id}; predicted newsgroup: {prediction[0]['label']}; actual newsgroup: {test_dataset['target'][sample_id]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd7ab8",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Execute the cell below to clean up all SageMaker resources and avoid any costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4fc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "model.delete_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0bfca",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, you learned how to train and deploy custom HuggingFace model using **SageMaker Script mode**. Script mode provide a lot of flexibility for developers when it comes to development of training and inference scripts.\n",
    "\n",
    "However, there are scenarios when you need to have more control over your runtime environments. SageMaker allows you to extend pre-built containers or Bring Your Own (\"BOY\") containers. In `2_Extending_Prebuilt_Training_Container.ipynb` example we will learn how to extend pre-built SageMaker container according to your specific requirements."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
