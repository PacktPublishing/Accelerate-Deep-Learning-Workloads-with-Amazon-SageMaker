{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Profiling Training Jobs With TensorBoard\n",
    "\n",
    "In this example we will learn how to use TensorBoard to debug and profile PyTorch training job. TensorBoard is an open source tool developed originally for the TensorFlow framework, but it is now available for other DL frameworks, including PyTorch. TensorBoard supports the following features for visualizing and inspecting the training process:\n",
    "- Tracking scalar values (loss, accuracy, and others) over time.\n",
    "- Capturing tensors such as weights, biases, and gradients and how they change over time. This can be useful for visualizing weights and biases and verifying that they are changing expectedly.\n",
    "- Experiment tracking via a dashboard of hyperparameters.\n",
    "- Projecting high-dimensional embeddings to a lower-dimensionality space.\n",
    "TensorBoard also allows you to profile resource utilization and resource comsuptions by different parts of your training program.\n",
    "\n",
    "In this example we will re-use `Hymenoptera` image classification problem, however, we integrate TensorBoard monitoring and profiling capabilities into training script and review the results. We use PyTorch framework for this. Note, that changes to TensorFlow script will be similar.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To be able to use TensorBoard, you need to install multiple Python packages locally. You can install all dependencies for this chapter by running cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from imports and SageMaker configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/tensorboard'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download training data locally and upload it to S3. Feel free to skip it if you already uploaded this dataset previously (in this case update `data_url` variable accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading dataset and unzipping it locally\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip hymenoptera_data.zip\n",
    "data_url = sagemaker_session.upload_data(path=\"./hymenoptera_data\", key_prefix=\"hymenoptera_data\")\n",
    "print(f\"S3 location of dataset {data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Training Script\n",
    "\n",
    "\n",
    "To use TensorBoard, we need to make minimal changes to our training script (see it here: `1_sources/train_resnet_tb.py`). In this example, we are using TensorBoard both for debugging and profiling purposes. Below we groups changes by purpose.\n",
    "\n",
    "### Modifications for Debugging\n",
    "Following modifications needed to capture data for debugging and monitoring:\n",
    "1. We must import and initialize TensorBoard’s SummaryWriter object. Here, we are using the S3 location to write TensorBoard summaries:\n",
    "```python\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    tb_writer = SummaryWriter(args.tb_s3_url)\n",
    "```\n",
    "2. Next, we must capture training artifacts that won’t change during training – in our case, the model graph. Note that we need to execute the model’s forward path on the sample data to do so:\n",
    "\n",
    "```python\n",
    "    sample_inputs, _ = next(iter(dataloaders_dict[\"val\"]))\n",
    "    tb_writer.add_graph(model, sample_inputs, verbose=False,\n",
    "    use_strict_trace=False)\n",
    "```\n",
    "\n",
    "3. In our training loop, we capture the scalars (such as `loss` or `accuracy`) and tensors (such as `weights`) that we wish to inspect. We use the epoch number as the time dimension.\n",
    "```python\n",
    "    tb_writer.add_scalar(f\"Loss/{phase}\", epoch_loss, epoch)\n",
    "    tb_writer.add_scalar(f\"Accuracy/{phase}\", epoch_accuracy, epoch)\n",
    "    tb_writer.add_histogram(\"conv1.weight\", model.conv1.weight, epoch)\n",
    "    tb_writer.add_histogram(\"conv1.weight_grad\", model.conv1.weight.grad, epoch)\n",
    "    tb_writer.add_histogram(\"fc.weight\", model.fc.weight, epoch)\n",
    "    tb_writer.add_histogram(\"fc.weight_grad\", model.fc.weight.grad, epoch)\n",
    "    tb_writer.add_scalar(f\"Loss/{phase}\", epoch_loss, epoch)\n",
    "    tb_writer.add_scalar(f\"Accuracy/{phase}\", epoch_accuracy, epoch)\n",
    "    tb_writer.add_hparams(hparam_dict=vars(args), metric_dict={epoch_accuracy})\n",
    "```\n",
    "After these modications, TensorBoard will continiously save requested data to Amazon S3 location as the training progresses. Now let's review modifications needed for profiling.\n",
    "\n",
    "### Modifications for Profiling\n",
    "TensorBoard provides out-of-the-box profiling capabilities for TensorFlow programs (including Keras). To profile PyTorch programs in TensorBoard, you can use the open source `torch_tb_profiler` plugin (we included this dependency in `2_sources/requirements.txt` file). \n",
    "\n",
    "To profile applications using torch_tb_profiler, we need to wrap our training loop with the plugin context manager, as shown in the following code block:\n",
    "\n",
    "```python\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=5),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(\n",
    "            os.path.join(os.environ[\"SM_OUTPUT_DATA_DIR\"], \"tb_profiler\")\n",
    "        ),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True,\n",
    "    ) as prof:\n",
    "        # Rest of the training loop goes below.\n",
    "```\n",
    "The parameters of the context manager that are passed at initialization time define what profiling data must be gathered and at what intervals. At the time of writing this book, the torch_db_profiler plugin doesn’t support writing to the S3 location. Hence, we must write the profiling data to the local output directory stored in the \"SM_OUTPUT_DATA_DIR\" environment variable. After training is done, SageMaker automatically archives and stores the content of this directory to the S3 location.\n",
    "\n",
    "If you'd like to review training script in full with all modifications, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 1_sources/train_resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training Job\n",
    "\n",
    "\n",
    "To start the SageMaker training job, we need to provide the S3 location where TensorBoard summaries will be written. We can do this by setting `tb-s3-url` hyperparameter, as shown below. The rest of configuration is similar to regular SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "job_name = \"pytorch-tb-profiling\"\n",
    "tb_s3_url = f\"s3://{bucket}/tensorboard/{job_name}\"\n",
    "\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "instance_count = 1\n",
    "python_version = \"py38\"\n",
    "pytorch_version = \"1.10.2\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "          entry_point=\"train_resnet_tb.py\",\n",
    "          source_dir='1_sources',\n",
    "          role=role,\n",
    "          framework_version=pytorch_version,\n",
    "          py_version=python_version,          \n",
    "          instance_type=instance_type,\n",
    "          sagemaker_session=sagemaker_session,        \n",
    "          instance_count=instance_count,\n",
    "          hyperparameters={\n",
    "              \"batch-size\":16,\n",
    "              \"num-epochs\":5,\n",
    "              \"input-size\" : 224,\n",
    "              \"feature-extract\":False,\n",
    "              \"tb-s3-url\": tb_s3_url,\n",
    "              \"num-data-workers\": 4\n",
    "          },\n",
    "          disable_profiler=True,\n",
    "          debugger_hook_config=False,\n",
    "          base_job_name=job_name,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Debugging Results\n",
    "\n",
    "TensorBoard will start saving data (scalars, tensors, graphs etc.) to Amazon S3 location almost immediately after start of the training. To review the debugging results, start TensorBoard application locally as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir ${tb_debug_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard web application should start now and redirect you to its local address (by default it's `localhost:6006`). Note the following when using TensorBoard in cloud development environments:\n",
    "- If you are using a SageMaker notebook instance, then TensorBoard will be available here: `https://YOUR_NOTEBOOK_DOMAIN/proxy/6006/`\n",
    "- If you are using SageMaker Studio, then TensorBoard will available here: `https://<YOUR_ STUDIO_DOMAIN>/jupyter/default/proxy/6006/`\n",
    "\n",
    "The TensorBoard data will be updated in near-real time as the training job progresses. Please refer to the book for overview of visualizations in TensorBoard application.\n",
    "\n",
    "\n",
    "### Reviewing Profiling Results\n",
    "Unlike debugging data available in near real-time, profiling data will be only available after training job completion. This is due to current limitations of `torch_tb_profiler` plugin. Run the cell below to get location of TensorBoard Profiler output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_profiler_path = f\"{estimator.latest_training_job.describe()['OutputDataConfig']['S3OutputPath']}/output/output.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can run the following commands to unarchive the profiler data and start TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp ${ tb_profiler_path} .\n",
    "! mkdir profiler_output\n",
    "! tar -xf output.tar.gz -C profiler_output\n",
    "! tensorboard --logdir ./profiler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that TensorBoard application will open with loaded profiling results. Refer to the book for overview of the results."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
