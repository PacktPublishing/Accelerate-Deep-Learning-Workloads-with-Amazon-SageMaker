{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Training with SageMaker Debugger\n",
    "\n",
    "In this example we will learn how to use SageMaker Debugger to monitor and profile training jobs.\n",
    "\n",
    "SageMaker Debugger is a comprehensive SageMaker capability that allows you to automatically monitor, debug, and profile DL training jobs running on SageMaker. SageMaker Debugger provides you with insights into your DL training by capturing the internal state of your training loop and instances metrics in near-real time. Debugger also allows you to automatically detect common issues happening during training and take appropriate actions when issues are detected. This allows you to automatically find issues in complex DL training jobs earlier and react accordingly. Additionally, SageMaker Debugger supports writing custom rules for scenarios not covered by built-in rules.\n",
    "\n",
    "SageMaker has several key components:\n",
    "- The open source [smedebug library](https://github.com/awslabs/sagemaker-debugger), which integrates with DL frameworks and Linux instances to persist debugging and profiling data to Amazon S3, as well as to retrieve and analyze it once the training job has been started\n",
    "- The SageMaker Python SDK, which allows you to configure the smedebug library with no or minimal code changes in your training script\n",
    "- Automatically provisioned processing jobs to validate output tensors and profiling data against rules\n",
    "\n",
    "SageMaker Debugger supports TensorFlow, PyTorch, and MXNet DL frameworks. The `smedebug` library is installed by default in SageMaker DL containers, so you can start using SageMaker Debugger without having to make any modifications to your training script. You can also install the smdebug library in a custom Docker container and use all the features of SageMaker Debugger.\n",
    "\n",
    "In this examples, we will use the same CV `Hymenoptera` problem as in TensorBoard example. You will have an opportunity to compare both debugging frameworks on the same problem.\n",
    "\n",
    "### Prerequisites\n",
    "This examples requires to have `smedbug` installed locally. Feel free to run cell below to install all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "In the cell below, we make initial imports and download required data. Feel free to reuse dataset from previous example (make sure to update `data_url` respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sm-debugger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading dataset and unzipping it locally\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip hymenoptera_data.zip\n",
    "data_url = sagemaker_session.upload_data(path=\"./hymenoptera_data\", key_prefix=\"hymenoptera_data\")\n",
    "print(f\"S3 location of dataset {data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Your Training Job\n",
    "\n",
    "In this section we will learn how to configure monitoring of your training job and analyze results for debugging purposes.\n",
    "\n",
    "### Modifying Training Script\n",
    "\n",
    "The `smedebug` library requires minimal changes to capture tensors and scalars. Let's review them.\n",
    "\n",
    "1. You need to initiate the hook object outside of your training loop, as well as after model and optimizer initialization:\n",
    "\n",
    "```python\n",
    "    model = initialize_resnet_model(NUM_CLASSES, feature_extract=False, use_pretrained=True)\n",
    "    model.to(torch.device(\"cuda\"))\n",
    "    optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7,\n",
    "    gamma=0.1)\n",
    "    hook = smd.Hook.create_from_json_file()\n",
    "    hook.register_hook(model)\n",
    "    hook.register_loss(criterion)\n",
    "```\n",
    "\n",
    "Note that we are using `.create_from_json_file()` to create our hook object. This method instantiates hook based on the hook configuration you provide in the SageMaker training object. Since we are adding both the model and criterion objects to hook, we should expect to see both model parameters (weights, biases, and others), as well as loss scalar.\n",
    "\n",
    "2. Inside our training loop, the only modification we need to make is to differentiate between the training and validation phases by switching between smedebug.modes.Train and smedebug. modes.Eval. This will allow smedebug to segregate the tensors that are captured in the training and evaluation phases:\n",
    "\n",
    "```python\n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "                if hook:\n",
    "                    hook.set_mode(modes.TRAIN)\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                if hook:\n",
    "                    hook.set_mode(modes.EVAL)\n",
    "            # Rest of the training loop\n",
    "```\n",
    "\n",
    "Run the cell below to review training script in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 2_sources/train_resnet_sm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Training Job\n",
    "\n",
    "Now, let’s review how to configure `hook`, `rules`, `actions`, and tensor `collections` when running a SageMaker training job.\n",
    "\n",
    "1. We will start by importing Debugger entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import (CollectionConfig, DebuggerHookConfig,\n",
    "                                ProfilerRule, Rule, TensorBoardOutputConfig,\n",
    "                                rule_configs)\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then, we must define automatic actions and a set of rules. Here, we are using Debugger’s built-in rules to detect some common DL training issues. Note that we can assign different actions to different rules. In our case, we want to stop our training job immediately when the rule is triggered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = rule_configs.ActionList(\n",
    "    rule_configs.StopTraining())\n",
    "\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient(), actions=actions),\n",
    "    Rule.sagemaker(rule_configs.overfit(), actions=actions),\n",
    "    Rule.sagemaker(rule_configs.overtraining(), actions=actions),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization(), actions=actions),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, we must configure the collection of tensors and how they will be persisted. Here, we will define that we want to persist the weights and losses collection. For weights, we will also save a histogram that can be further visualized in TensorBoard. We will also set a saving interval for the training and evaluation phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_configs=[\n",
    "        CollectionConfig(\n",
    "            name=\"weights\",\n",
    "            parameters={\n",
    "                \"save_histogram\": \"True\"\n",
    "                }\n",
    "            ),\n",
    "        CollectionConfig(name=\"losses\"),\n",
    "    ]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"1\", \"eval.save_interval\": \"1\"},\n",
    "    collection_configs=collection_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training Job\n",
    "\n",
    "Now, we are ready to pass these objects to the SageMaker Estimator object and run training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "instance_count = 1\n",
    "job_name = \"pytorch-sm-debugging\"\n",
    "tb_debug_path = f\"s3://{bucket}/tensorboard/{job_name}\"\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=tb_debug_path\n",
    ")\n",
    "\n",
    "\n",
    "debug_estimator = PyTorch(\n",
    "          entry_point=\"train_resnet_sm.py\",\n",
    "          source_dir='2_sources',\n",
    "          role=role,\n",
    "          instance_type=instance_type,\n",
    "          sagemaker_session=sagemaker_session,\n",
    "          image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
    "          instance_count=instance_count,\n",
    "          hyperparameters={\n",
    "              \"batch-size\":64,\n",
    "              \"num-epochs\":5,\n",
    "              \"input-size\" : 224,\n",
    "              \"num-data-workers\" : 4,\n",
    "              \"feature-extract\":False,\n",
    "          },\n",
    "          disable_profiler=True,\n",
    "          rules=rules,\n",
    "          debugger_hook_config=hook_config,\n",
    "          tensorboard_output_config=tensorboard_output_config,\n",
    "          base_job_name=job_name,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_estimator.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Debugger Results\n",
    "\n",
    "SageMaker Debugger provides functionality to retrieve and analyze collected tensors from training jobs as part of the smedebug library. In the following steps, we will highlight some key APIs:\n",
    "\n",
    "1. In the following code block, we are creating a new trial object using the S3 path where the tensors were persisted. Then, we print list of all available tensors and value of specific tensor `CrossEntropyLoss_output_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smdebug.pytorch as smd\n",
    "from smdebug import trials\n",
    "\n",
    "tensors_path = debug_estimator.latest_job_debugger_artifacts_path()\n",
    "\n",
    "trial = smd.create_trial(tensors_path)\n",
    "\n",
    "print(f\"Saved these tensors: {trial.tensor_names()}\")\n",
    "print(f\"Loss values during evaluation were {trial.tensor('CrossEntropyLoss_output_0').values()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using a plotting function `plot_tensor()`, we can visualize loss for the training and evaluation phases. Running the following command will result in a 2D loss chart. Similarly, you can access and process other tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "from smdebug import modes\n",
    "\n",
    "def get_data(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        vals.append(tensor.value(s, mode=mode))\n",
    "    return steps, vals\n",
    "\n",
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode=modes.TRAIN)\n",
    "    print(\"loaded TRAIN data\")\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode=modes.EVAL)\n",
    "    print(\"loaded EVAL data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    print(\"completed TRAIN plot\")\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    print(\"completed EVAL plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_tensor(trial, \"CrossEntropyLoss_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's check if any rules were triggered during our training. The rule evaluation results should have no rule triggered. ou can experiment with rule settings. For instance, you can reset weights on one of the model layers. This will result in triggering the PoorWeightInitiailization rule and the training process being stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  summary in debug_estimator.latest_training_job.rule_job_summary():\n",
    "    print(f\"Rule: {summary['RuleConfigurationName']}, status: {summary['RuleEvaluationStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lastly, let’s visually inspect the saved tensors using TensorBoard. For this, we simply need to start TensorBoard using the S3 path we supplied to the Estimator object earlier. Feel free to explore TensorBoard on your own. You should expect to find histograms of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir  {tb_debug_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Training Application\n",
    "\n",
    "In this section we review how to profile resource utilization of your training application using SageMaker Debugger. SageMaker Debugger allows you to collect various types of advanced metrics from your training instances. Once these metrics have been collected, SageMaker generates detailed metrics visualizations, detects resource bottlenecks, and provides recommendations on how instance utilization can be improved.\n",
    "\n",
    "SageMaker Debugger collects two types of metrics:\n",
    "- **System metrics**: These are the resource utilization metrics of training instances such as CPU, GPU, network, and I/O.\n",
    "- **Framework metrics**: These are collected at the DL framework level. This includes metrics collected by native framework profiles (such as PyTorch profiler or TensorFlow Profiler), data loader metrics, and Python profiling metrics.\n",
    "\n",
    "As in the case of debugging, you can define rules that will be automatically evaluated against collected metrics. If a rule is triggered, you can define one or several actions that will be taken. For example, you can send an email if the training job has GPU utilization below a certain threshold.\n",
    "\n",
    "### Configuring Profiling\n",
    "\n",
    "SageMaker Debugger doesn't require training script modification to collect metrics. However, you need to provide profiler configuration as part of your SageMaker `Estimator` object. Let's see how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. We start with necessary imports. Similar to debugging examples, we also need to create `actions`, `rules`, and `hooks` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import (CollectionConfig, DebuggerHookConfig,\n",
    "                                ProfilerRule, Rule, TensorBoardOutputConfig,\n",
    "                                rule_configs)\n",
    "\n",
    "actions = rule_configs.ActionList(\n",
    "    rule_configs.StopTraining())\n",
    "\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport())\n",
    "]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\"train.save_interval\": \"1\", \"eval.save_interval\": \"1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next we define what system and framework metrics we want to collect. For instance, we can provide a custom configuration for the framework, data loader, and Python. Note that system profiling is enabled by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import (DataloaderProfilingConfig,\n",
    "                                DetailedProfilingConfig, FrameworkProfile,\n",
    "                                ProfilerConfig, PythonProfiler,\n",
    "                                PythonProfilingConfig, cProfileTimer)\n",
    "\n",
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "        detailed_profiling_config=DetailedProfilingConfig(\n",
    "            start_step=2, \n",
    "            num_steps=1\n",
    "        ),\n",
    "        dataloader_profiling_config=DataloaderProfilingConfig(\n",
    "            start_step=2, \n",
    "            num_steps=1\n",
    "        ),\n",
    "        python_profiling_config=PythonProfilingConfig(\n",
    "            start_step=2, \n",
    "            num_steps=1, \n",
    "            python_profiler=PythonProfiler.CPROFILE, \n",
    "            cprofile_timer=cProfileTimer.TOTAL_TIME\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training Job\n",
    "\n",
    "Next, we provide the profiling config to the SageMaker training job configuration and start the training. Note that we set `num-data-workers` to 8, while `ml.p2.xlarge` instance has only 4 CPU cores. Usually, it’s recommended to have the number of data workers equal to the number of CPUs. Let’s see if SageMaker Debugger will be able to detect this suboptimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p2.xlarge'\n",
    "instance_count = 1\n",
    "job_name = \"pytorch-sm-profiling\"\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "profiler_estimator = PyTorch(\n",
    "          entry_point=\"train_resnet_sm.py\",\n",
    "          source_dir='2_sources',\n",
    "          role=role,\n",
    "          instance_type=instance_type,\n",
    "          sagemaker_session=sagemaker_session,\n",
    "          image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.11.0-gpu-py38-cu113-ubuntu20.04-sagemaker\",\n",
    "          instance_count=instance_count,\n",
    "          hyperparameters={\n",
    "              \"batch-size\":64,\n",
    "              \"num-epochs\":5,\n",
    "              \"input-size\" : 224,\n",
    "              \"num-data-workers\" : 8,\n",
    "              \"feature-extract\":False,\n",
    "          },\n",
    "          disable_profiler=False,\n",
    "          profiler_config=profiler_config,\n",
    "          rules=rules,\n",
    "          base_job_name=job_name,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_estimator.fit(inputs={\"train\":f\"{data_url}/train\", \"val\":f\"{data_url}/val\"}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing Profiling Results\n",
    "\n",
    "You can start monitoring profiling outcomes in near-real time. We will use the semdebug.profiler API to process profiling outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "\n",
    "training_job_name = profiler_estimator.latest_training_job.job_name\n",
    "tj = TrainingJob(training_job_name, sagemaker_session.boto_region_name)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is available, we can retrieve and visualize it. Running the following code will chart the CPU, GPU, and GPU memory utilization from system metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import \\\n",
    "    TimelineCharts\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader=None,\n",
    "    select_dimensions=[\"CPU\", \"GPU\"],\n",
    "    select_events=[\"total\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can visualize other collected metrics. SageMaker Debugger also generates a detailed profiling report that aggregates all visualizations, insights, and recommendations in one place. Once your training job has finished, you can download the profile report and all collected data by running the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rule_output_path = f\"s3://{bucket}/{training_job_name}/rule-output\"\n",
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the assets have been downloaded, open the profiler-report.html file in your browser and review the generated information. Alternatively, you can open profiler-report.ipynb, which provides the same insights in the form of an executable Jupyter notebook.\n",
    "\n",
    "The report covers the following aspects:\n",
    "- System usage statistics\n",
    "- Framework metrics summary\n",
    "- Summary of rules and their status\n",
    "- Training loop analysis and recommendations for optimizations\n",
    "\n",
    "Note that, in the Dataloading analysis section, you should see a recommendation to decrease the number of data workers according to our expectations.\n",
    "\n",
    "As you can see, SageMaker Debugger provides extensive profiling capabilities, including a recommendation to improve and automate rule validation with minimal development efforts. Similar to other Debugger capabilities, profiling is free of charge, so long as you are using built-in rules."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
