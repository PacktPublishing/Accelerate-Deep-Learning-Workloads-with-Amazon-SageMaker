{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel Training with TensorFlow2\n",
    "\n",
    "In this notebook we will learn how to engineer TensorFlow2 training job using native `Allreduce` implementation as well as `Horovod` framework. You will be able to compare both implementations side by side. As a test task we choose everyone's favorite MNIST dataset and train small computer vision model to solve classification task. We will use convenient `Keras` API to build and train model and evaluate results.\n",
    "\n",
    "**Instance recommendations** <br>\n",
    "Feel free to experiment with various GPU SageMaker instances. By default we use single GPU `p2.xlarge` instances to minimize cost of training. \n",
    "\n",
    "**Disclaimer** <br>\n",
    "This example should not be used to draw any conclusions about training efficiencies since the dataset and model are very small.\n",
    "\n",
    "## TensorFlow MultiWorkerMirroredStrategy Training\n",
    "TensorFlow2 provides several native implementations of data parallel training known as `strategies`. In this examples we will use synchronous multi-GPU multi-node Allreduce implementation called `MultiWorkerMirroredStrategy` (`\"MWMS\"`). Refer to [this overview](https://www.tensorflow.org/guide/distributed_training) of data parallel strategies if you want to learn about others. \n",
    "\n",
    "As always, we start with necessary imports and basic SageMaker training configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role() # replace it with role ARN if you are not using SageMaker Notebook or Studio environments.\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/tf-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing training script\n",
    "\n",
    "Let's review modification we need to make in training script to enable MWMS. Full sources are here: `chapter6/1_sources/train_ms.py`\n",
    "\n",
    "#### Cluster configuration and setup\n",
    "MWMS is not natively supported by Amazon SageMaker, so we need to correctly configure MWMS environment in SageMaker. TF2 uses environment variable called `TF_CONFIG` to represent cluster configuration. This configuration is then used to start training processes. You can read about building `TF_CONFIG` variable [here](https://www.tensorflow.org/guide/distributed_training#TF_CONFIG). We use `_build_tf_config()` method below to setup this variable. Note, that we are using SageMaker environment variables `SM_HOSTS` and `SM_CURRENT_HOST` for it.\n",
    "\n",
    "```python\n",
    "def _build_tf_config():\n",
    "\n",
    "    hosts = json.loads(os.getenv(\"SM_HOSTS\"))\n",
    "    current_host = os.getenv(\"SM_CURRENT_HOST\")\n",
    "\n",
    "    workers = hosts\n",
    "\n",
    "    def host_addresses(hosts, port=7777):\n",
    "        return [\"{}:{}\".format(host, port) for host in hosts]\n",
    "\n",
    "    tf_config = {\"cluster\": {}, \"task\": {}}\n",
    "    tf_config[\"cluster\"][\"worker\"] = host_addresses(workers)\n",
    "    tf_config[\"task\"] = {\"index\": workers.index(current_host), \"type\": \"worker\"}\n",
    "\n",
    "    os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\n",
    "```\n",
    "\n",
    "\n",
    "By default we use in this sample two `p2.xlarge` instances with total world size of just 2 training processes. So `_build_tf_config()` will produce following `TF_CONFIG` on rank=0 node:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"cluster\": \n",
    "    {\n",
    "        \"worker\": [\"algo-1:7777\", \"algo-2:7777\"]},\n",
    "        \"task\": {\"index\": 0, \"type\": \"worker\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Once TF config is properly configured, TF2 should be able to start training processes on all nodes and utilize all available GPU devices (this is a default setting but you can provide a list of specific GPU devices to use too).\n",
    "\n",
    "To complete cluster setup we also need to make sure that NCCL environment is properly configurate (see `_set_nccl_environment()` method) and that all nodes in cluster can communicate with each other (see `_dns_lookup()` method). Note, that these methods are required because TensorFlow2 strategies are not officially supported by SageMaker. For supported data parallel implementations, SageMaker provides these utilities out of the box and run them as part of training cluster initiation.\n",
    "\n",
    "\n",
    "#### Using MWMS\n",
    "\n",
    "To use MWMS we start by initiating strategy object like below. Please note, that here we explicitly set communication backend to `AUTO` which means that TF2 will identify which backend to use. You can also define a specific backend manually. `NCCL` and custom `RING` backends are available for GPU devices.\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "    communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation=tf.distribute.experimental.CollectiveCommunication.AUTO\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "Once strategy is correctly initiated, you can confirm your cluster configuration by checking properly `strategy.num_replicas_in_sync` which will return your world size. It should match with number of GPUs per node *multiplied by number of nodes.\n",
    "\n",
    "In this example we are using Keras API which fully supports MWMS, simplifying our training script. For instance, to create model copies on all workers you just need to initiate your Keras model within strategy.scope like below:\n",
    "\n",
    "```python\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_and_compile_cnn_model()\n",
    "```\n",
    "\n",
    "MWMS also automatically shards your dataset based on world sze. You only need to setup proper global batch size like below. Note, that auto sharding can be turned out if some custom sharding logic is needed.\n",
    "\n",
    "```python\n",
    "    global_batch_size = args.batch_size_per_device * _get_world_size()\n",
    "    multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "```\n",
    "\n",
    "The rest of training script is similar to your single process Keras training script. As you can see, using MWMS is quite straighforward, and TF2 does a good job abstracting complexities from developers but at the same time giving flexibility to adjust default settings if needed.\n",
    "\n",
    "\n",
    "#### Running SageMaker job\n",
    "\n",
    "So far we discussed training script. In source directory you will also see `mnist_setup.py` script to download and configure MNIST dataset. Now we are ready to run Data Parallel training on SageMaker.\n",
    "\n",
    "In cell  below we define TF version (2.8), Python version (3.9), instance type and number of instances. Additionally, we also pass several training hyperparameters. Since MNIST dataset is downloaded from internet as part of our training script, no data is passed to `estimator_ms.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ps_instance_type = 'ml.p2.xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "hyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\n",
    "\n",
    "estimator_ms = TensorFlow(\n",
    "                       source_dir='1_sources',\n",
    "                       entry_point='train_ms.py', \n",
    "                       role=role,\n",
    "                       framework_version='2.8',\n",
    "                       py_version='py39',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       )\n",
    "\n",
    "estimator_ms.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 10-12 minutes with default settings. Feel free to experiment with number of nodes in cluster and instasnce types and observe changes `TF_CONFIG`, training speed and convergence. \n",
    "\n",
    "## TensorFlow Horovod Training\n",
    "\n",
    "Horovod is an open-source framework for data parallel training whic supports TF1, TF2, PyTorch, and MXNet. Given its popularity, Amazon SageMaker provides native support for Horovod as well, making using Horovod even simplier than native TF2 strategies. \n",
    "\n",
    "We will re-use same MNIST problem and `Keras` API.\n",
    "\n",
    "### Configuring Horovod cluster\n",
    "\n",
    "Unlike in case of MWMS, we don't have to configure and setup training cluster in training script since Horovod is supported by SageMaker. Horovod cluster configuration is done on level of `Tensorflow.Estimator` API via distributions object like below:\n",
    "\n",
    "```python\n",
    "distribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\n",
    "```\n",
    "Note parameter `processes_per_host` which should match number of GPUs on chosen instance type. You can also set `custom_mpi_options` as needed which SageMaker will pass to `mpirun` run utility. See list of supported MPI options [here](https://www.open-mpi.org/doc/v4.0/man1/mpirun.1.php).\n",
    "\n",
    "### Developing training script\n",
    "You can find full training script in `chapter6/1_sources/train_hvd.py`. We start by initiating Horovod in training script via `_initiate_hvd()` method. We need to associate Horovod training processes with available GPU devices (one device per process).\n",
    "\n",
    "```python\n",
    "def _initiate_hvd():\n",
    "    # Horovod: initialize Horovod.\n",
    "    hvd.init()\n",
    "\n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n",
    "```\n",
    "\n",
    "Next we need to shard our dataset based on world size, so each process can get a slice of data based on its global rank. For this we use `shard` method of `TensorFlow.Dataset`. Note that we are getting local and global ranks of given training process using Horovod properties `size()` and `rank()`.\n",
    "\n",
    "```python\n",
    "train_dataset = train_dataset.shard(hvd.size(), hvd.rank())\n",
    "```\n",
    "\n",
    "Next we need to use Horovod `DistributedOptimizer` wrapper to enable distributed gradient update. Note, that we are wrapping instance of native TF2 optimizer.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001 * hvd.size())\n",
    "optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "```\n",
    "\n",
    "Lastly we use use special Keras callbacks:\n",
    "- `hvd.callbacks.BroadcastGlobalVariablesCallback(0)` to distribute initial variables from rank=0 process to other training processes in the cluster.\n",
    "- `hvd.callbacks.MetricAverageCallback()` to calculate global average of metrics across all training processes.\n",
    "\n",
    "These callbacks then passed to `model.fit()` method like below:\n",
    "```python\n",
    "    hvd_model.fit(\n",
    "        shareded_by_rank_dataset,\n",
    "        epochs=args.epochs,\n",
    "        steps_per_epoch=args.steps_per_epoch // hvd.size(),\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "```\n",
    "\n",
    "These are minimal additions to your training script which allows to use Horovod.\n",
    "\n",
    "### Runing SageMaker job\n",
    "\n",
    "SageMaker Training job configuration is similar to MWMS example with exception of `distributions` parameter above which we discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "ps_instance_type = 'ml.p2.xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "distribution = {\"mpi\": {\"enabled\": True, \"custom_mpi_options\": \"-verbose --NCCL_DEBUG=INFO\", \"processes_per_host\": 1}}\n",
    "hyperparameters = {'epochs': 4, 'batch-size-per-device' : 16, 'steps-per-epoch': 100}\n",
    "\n",
    "estimator_hvd = TensorFlow(\n",
    "                       source_dir='1_sources',\n",
    "                       entry_point='train_hvd.py', \n",
    "                       role=role,\n",
    "                       framework_version='2.8',\n",
    "                       py_version='py39',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                    #   model_dir = \"/opt/ml/model\",\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       distribution=distribution\n",
    "                       )\n",
    "\n",
    "estimator_hvd.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We implemented minimal viable examples of data parallel training jobs using TensorFlow2 MultiWorkerMirrored Strategy and TensorFlow2 Horovod. Now you should have some practical experience in developing baseline training jobs. There are certainly more knobs and capabilities to explore of both Allreduce implementations which we encorage to explore and try on your real-life use cases."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
