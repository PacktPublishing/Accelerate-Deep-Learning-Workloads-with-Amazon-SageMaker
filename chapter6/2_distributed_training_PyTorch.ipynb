{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket:\n",
      "sagemaker-us-east-1-941656036254\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::941656036254:role/service-role/AmazonSageMaker-ExecutionRole-20210904T193230\"\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/pytorch-distribution-options'\n",
    "print('Bucket:\\n{}'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-07 22:36:54 Starting - Starting the training job...\n",
      "2022-04-07 22:37:21 Starting - Preparing the instances for training............\n",
      "2022-04-07 22:39:03 Downloading - Downloading input data...\n",
      "2022-04-07 22:39:29 Training - Downloading the training image..............................\n",
      "2022-04-07 22:44:57 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-04-07 22:45:01,402 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-07 22:45:01,426 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-07 22:45:01,440 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-07 22:45:02,307 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size-per-device\": 16,\n",
      "        \"epochs\": 4,\n",
      "        \"steps-per-epoch\": 100,\n",
      "        \"train-script\": \"train_ddp.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-07-22-36-53-872\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p2.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p2.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launcher.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"}\n",
      "SM_USER_ENTRY_POINT=launcher.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=launcher\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=4\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-07-22-36-53-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}\n",
      "SM_USER_ARGS=[\"--batch-size-per-device\",\"16\",\"--epochs\",\"4\",\"--steps-per-epoch\",\"100\",\"--train-script\",\"train_ddp.py\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_HP_BATCH-SIZE-PER-DEVICE=16\n",
      "SM_HP_EPOCHS=4\n",
      "SM_HP_STEPS-PER-EPOCH=100\n",
      "SM_HP_TRAIN-SCRIPT=train_ddp.py\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python launcher.py --batch-size-per-device 16 --epochs 4 --steps-per-epoch 100 --train-script train_ddp.py\n",
      "INFO:__main__:Arguments: Namespace(train_script='train_ddp.py')\n",
      "INFO:__main__:Training CMD to be executed on each node once:['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=0', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-04-07 22:45:01,361 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-04-07 22:45:01,392 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-04-07 22:45:01,405 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-04-07 22:45:02,443 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size-per-device\": 16,\n",
      "        \"epochs\": 4,\n",
      "        \"steps-per-epoch\": 100,\n",
      "        \"train-script\": \"train_ddp.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2022-04-07-22-36-53-872\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p2.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p2.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launcher.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"}\n",
      "SM_USER_ENTRY_POINT=launcher.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_MODULE_NAME=launcher\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=4\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2022-04-07-22-36-53-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}\n",
      "SM_USER_ARGS=[\"--batch-size-per-device\",\"16\",\"--epochs\",\"4\",\"--steps-per-epoch\",\"100\",\"--train-script\",\"train_ddp.py\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_HP_BATCH-SIZE-PER-DEVICE=16\n",
      "SM_HP_EPOCHS=4\n",
      "SM_HP_STEPS-PER-EPOCH=100\n",
      "SM_HP_TRAIN-SCRIPT=train_ddp.py\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python launcher.py --batch-size-per-device 16 --epochs 4 --steps-per-epoch 100 --train-script train_ddp.py\n",
      "INFO:__main__:Arguments: Namespace(train_script='train_ddp.py')\n",
      "INFO:__main__:Training CMD to be executed on each node once:['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=1', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']\n",
      "Envvar inside training script: environ({'SM_INPUT_DIR': '/opt/ml/input', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-1', 'NCCL_DEBUG': 'WARN', 'HOROVOD_VERSION': '0.21.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'launcher.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"}', 'EFA_VERSION': '1.12.1', 'MASTER_ADDR': 'algo-1', 'SM_MODULE_NAME': 'launcher', 'HOSTNAME': 'ip-10-2-141-240.ec2.internal', 'CURRENT_HOST': 'algo-1', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'HOME': '/root', 'MASTER_PORT': '42023', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{}', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:941656036254:training-job/pytorch-training-2022-04-07-22-36-53-872', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'BRANCH_OFI': '1.1.3-aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-ea0dbf6022a5e95ac19a8171769c6ab1838dd40d50bd8c4290f4a5e97afb764f-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '4', 'CUDA_VERSION': '11.1.1', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'SM_NUM_GPUS': '1', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.1 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'RDMAV_FORK_SAFE': '1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_HP_EPOCHS': '4', 'NV_CUDA_CUDART_VERSION': '11.1.74-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-07-22-36-53-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}', 'PATH': '/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'DMLC_INTERFACE': 'eth0', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_HPS': '{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"}', 'NCCL_IB_DISABLE': '1', 'CUDNN_VERSION': '8.0.5.39', 'SM_CHANNELS': '[]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_JOB_NAME': 'pytorch-training-2022-04-07-22-36-53-872', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'pytorch-training-2022-04-07-22-36-53-872', 'LC_ALL': 'C.UTF-8', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_VERSION': '4.1.1', 'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-east-1', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.7.8', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--batch-size-per-device\",\"16\",\"--epochs\",\"4\",\"--steps-per-epoch\",\"100\",\"--train-script\",\"train_ddp.py\"]', 'NV_ML_REPO_ENABLED': '1', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'LOCAL_RANK': '0', 'RANK': '0', 'GROUP_RANK': '0', 'ROLE_RANK': '0', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '1', 'WORLD_SIZE': '2', 'GROUP_WORLD_SIZE': '2', 'ROLE_WORLD_SIZE': '2', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'pytorch-training-2022-04-07-22-36-53-872', 'TORCHELASTIC_USE_AGENT_STORE': 'False', 'NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_pkrhspq0/pytorch-training-2022-04-07-22-36-53-872_jv2voorp/attempt_0/0/error.json'})\n",
      "Inside main of training script\n",
      "world size: 2\n",
      "global rank: 0\n",
      "Collected args: Namespace(batch_size=64, epochs=4, feature_extract=True, gamma=0.7, log_interval=10, lr=1.0, model_dir='/opt/ml/model', model_name='squeezenet', num_classes=2, save_model=False, seed=1, sync_s3_path='', test_batch_size=1000, use_pretrained=True, verbose=False). Following args are not parsed and won't be used: ['--batch-size-per-device', '16', '--steps-per-epoch', '100'].\n",
      "Traceback (most recent call last):\n",
      "  File \"train_ddp.py\", line 335, in <module>\n",
      "main()\n",
      "  File \"train_ddp.py\", line 235, in main\n",
      "args.batch_size //= args.world_size // 8\n",
      "ZeroDivisionError: integer division or modulo by zero\n",
      "Envvar inside training script: environ({'SM_INPUT_DIR': '/opt/ml/input', 'PYTHONIOENCODING': 'UTF-8', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-1', 'NCCL_DEBUG': 'WARN', 'HOROVOD_VERSION': '0.21.3', 'NCCL_SOCKET_IFNAME': 'eth0', 'SM_USER_ENTRY_POINT': 'launcher.py', 'SM_RESOURCE_CONFIG': '{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"}', 'EFA_VERSION': '1.12.1', 'MASTER_ADDR': 'algo-1', 'SM_MODULE_NAME': 'launcher', 'HOSTNAME': 'ip-10-2-148-222.ec2.internal', 'CURRENT_HOST': 'algo-2', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'HOME': '/root', 'MASTER_PORT': '42023', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'SM_INPUT_DATA_CONFIG': '{}', 'PYTHONUNBUFFERED': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:941656036254:training-job/pytorch-training-2022-04-07-22-36-53-872', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_CURRENT_HOST': 'algo-2', 'SM_MODEL_DIR': '/opt/ml/model', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'BRANCH_OFI': '1.1.3-aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/proxy-ea0dbf6022a5e95ac19a8171769c6ab1838dd40d50bd8c4290f4a5e97afb764f-customer', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_NUM_CPUS': '4', 'CUDA_VERSION': '11.1.1', 'TORCH_CUDA_ARCH_LIST': '3.7 5.0 7.0+PTX 8.0', 'SM_NUM_GPUS': '1', 'PYTHONDONTWRITEBYTECODE': '1', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.1 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450', 'RDMAV_FORK_SAFE': '1', '_': '/opt/conda/bin/train', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'SM_LOG_LEVEL': '20', 'SM_HP_EPOCHS': '4', 'NV_CUDA_CUDART_VERSION': '11.1.74-1', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2022-04-07-22-36-53-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-941656036254/pytorch-training-2022-04-07-22-36-53-872/source/sourcedir.tar.gz\",\"module_name\":\"launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p2.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p2.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launcher.py\"}', 'PATH': '/opt/amazon/openmpi/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NVARCH': 'x86_64', 'DMLC_INTERFACE': 'eth0', 'LD_PRELOAD': '/libchangehostname.so', 'LANG': 'C.UTF-8', 'NV_ML_REPO_URL': 'https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64', 'SM_HPS': '{\"batch-size-per-device\":16,\"epochs\":4,\"steps-per-epoch\":100,\"train-script\":\"train_ddp.py\"}', 'NCCL_IB_DISABLE': '1', 'CUDNN_VERSION': '8.0.5.39', 'SM_CHANNELS': '[]', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'USE_SMDEBUG': '0', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_JOB_NAME': 'pytorch-training-2022-04-07-22-36-53-872', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'PWD': '/opt/ml/code', 'TRAINING_JOB_NAME': 'pytorch-training-2022-04-07-22-36-53-872', 'LC_ALL': 'C.UTF-8', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'OMPI_VERSION': '4.1.1', 'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'MANUAL_BUILD': '0', 'SAGEMAKER_REGION': 'us-east-1', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.7.8', 'DGLBACKEND': 'pytorch', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_USER_ARGS': '[\"--batch-size-per-device\",\"16\",\"--epochs\",\"4\",\"--steps-per-epoch\",\"100\",\"--train-script\",\"train_ddp.py\"]', 'NV_ML_REPO_ENABLED': '1', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'LOCAL_RANK': '0', 'RANK': '1', 'GROUP_RANK': '1', 'ROLE_RANK': '1', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '1', 'WORLD_SIZE': '2', 'GROUP_WORLD_SIZE': '2', 'ROLE_WORLD_SIZE': '2', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'pytorch-training-2022-04-07-22-36-53-872', 'TORCHELASTIC_USE_AGENT_STORE': 'False', 'NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_x47sf711/pytorch-training-2022-04-07-22-36-53-872_q_vt5rnv/attempt_0/0/error.json'})\n",
      "Inside main of training script\n",
      "world size: 2\n",
      "global rank: 1\n",
      "Collected args: Namespace(batch_size=64, epochs=4, feature_extract=True, gamma=0.7, log_interval=10, lr=1.0, model_dir='/opt/ml/model', model_name='squeezenet', num_classes=2, save_model=False, seed=1, sync_s3_path='', test_batch_size=1000, use_pretrained=True, verbose=False). Following args are not parsed and won't be used: ['--batch-size-per-device', '16', '--steps-per-epoch', '100'].\n",
      "Traceback (most recent call last):\n",
      "  File \"train_ddp.py\", line 335, in <module>\n",
      "    main()\n",
      "  File \"train_ddp.py\", line 235, in main\n",
      "args.batch_size //= args.world_size // 8\n",
      "ZeroDivisionError: integer division or modulo by zero\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 41) of binary: /opt/conda/bin/python\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: \n",
      "**********************************************************************\n",
      "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
      "**********************************************************************\n",
      "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
      "Child process 41 (local_rank 0) FAILED (exitcode 1)\n",
      "Error msg: Process failed with exitcode 1\n",
      "Without writing an error file to <N/A>.\n",
      "While this DOES NOT affect the correctness of your application,\n",
      "no trace information about the error will be available for inspection.\n",
      "Consider decorating your top level entrypoint function with\n",
      "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
      "  from torch.distributed.elastic.multiprocessing.errors import record\n",
      "  @record\n",
      "  def trainer_main(args):\n",
      "     # do train\n",
      "**********************************************************************\n",
      "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
      "Traceback (most recent call last):\n",
      "File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 702, in <module>\n",
      "main()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 361, in wrapper\n",
      "return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main\n",
      "run(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run\n",
      "elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 116, in __call__\n",
      "return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 244, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "***************************************\n",
      "          train_ddp.py FAILED          \n",
      "=======================================\n",
      "Root Cause:\n",
      "[0]:\n",
      "  time: 2022-04-07_22:45:12\n",
      "  rank: 0 (local_rank: 0)\n",
      "  exitcode: 1 (pid: 41)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "=======================================\n",
      "Other Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "***************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"launcher.py\", line 82, in <module>\n",
      "main()\n",
      "  File \"launcher.py\", line 78, in main\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=0', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']' returned non-zero exit status 1.\n",
      "2022-04-07 22:45:12,530 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2022-04-07 22:45:12,531 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"ZeroDivisionError: integer division or modulo by zero\n",
      " ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 41) of binary: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning:   **********************************************************************                CHILD PROCESS FAILED WITH NO ERROR_FILE                 CHILD PROCESS FAILED WITH NO ERROR_FILE Child process 41 (local_rank 0) FAILED (exitcode 1) Error msg: Process failed with exitcode 1 Without writing an error file to <N/A>. While this DOES NOT affect the correctness of your application, no trace information about the error will be available for inspection. Consider decorating your top level entrypoint function with torch.distributed.elastic.multiprocessing.errors.record. Example:   from torch.distributed.elastic.multiprocessing.errors import record   @record   def trainer_main(args):      # do train   warnings.warn(_no_error_file_warning_msg(rank, failure)) Traceback (most recent call last): File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main return _run_code(code, main_globals, None,   File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code exec(code, run_globals)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 702, in <module> main()   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 361, in wrapper return f(*args, **kwargs)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main run(args)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run elastic_launch(   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 116, in __call__ return launch_agent(self._config, self._entrypoint, list(args))   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 244, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ***************************************           train_ddp.py FAILED           ======================================= Root Cause: [0]:   time: 2022-04-07_22:45:12   rank: 0 (local_rank: 0)   exitcode: 1 (pid: 41)   error_file: <N/A>   msg: \"Process failed with exitcode 1\" Other Failures:   <NO_OTHER_FAILURES>   File \"launcher.py\", line 82, in <module>   File \"launcher.py\", line 78, in main     raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd) subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=0', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']' returned non-zero exit status 1.\"\n",
      "Command \"/opt/conda/bin/python launcher.py --batch-size-per-device 16 --epochs 4 --steps-per-epoch 100 --train-script train_ddp.py\"\n",
      "2022-04-07 22:45:12,531 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 40) of binary: /opt/conda/bin/python\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: \n",
      "**********************************************************************\n",
      "               CHILD PROCESS FAILED WITH NO ERROR_FILE                \n",
      "**********************************************************************\n",
      "CHILD PROCESS FAILED WITH NO ERROR_FILE\n",
      "Child process 40 (local_rank 1) FAILED (exitcode 1)\n",
      "Error msg: Process failed with exitcode 1\n",
      "Without writing an error file to <N/A>.\n",
      "While this DOES NOT affect the correctness of your application,\n",
      "no trace information about the error will be available for inspection.\n",
      "Consider decorating your top level entrypoint function with\n",
      "torch.distributed.elastic.multiprocessing.errors.record. Example:\n",
      "  from torch.distributed.elastic.multiprocessing.errors import record\n",
      "  @record\n",
      "  def trainer_main(args):\n",
      "     # do train\n",
      "**********************************************************************\n",
      "  warnings.warn(_no_error_file_warning_msg(rank, failure))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 702, in <module>\n",
      "main()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 361, in wrapper\n",
      "return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main\n",
      "run(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run\n",
      "elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 116, in __call__\n",
      "return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 244, in launch_agent\n",
      "raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "***************************************\n",
      "          train_ddp.py FAILED          \n",
      "=======================================\n",
      "Root Cause:\n",
      "[0]:\n",
      "  time: 2022-04-07_22:45:12\n",
      "  rank: 1 (local_rank: 0)\n",
      "  exitcode: 1 (pid: 40)\n",
      "  error_file: <N/A>\n",
      "  msg: \"Process failed with exitcode 1\"\n",
      "=======================================\n",
      "Other Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "***************************************\n",
      "Traceback (most recent call last):\n",
      "  File \"launcher.py\", line 82, in <module>\n",
      "main()\n",
      "  File \"launcher.py\", line 78, in main\n",
      "raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=1', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']' returned non-zero exit status 1.\n",
      "2022-04-07 22:45:12,537 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2022-04-07 22:45:12,537 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"ZeroDivisionError: integer division or modulo by zero\n",
      " ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 40) of binary: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning:   **********************************************************************                CHILD PROCESS FAILED WITH NO ERROR_FILE                 CHILD PROCESS FAILED WITH NO ERROR_FILE Child process 40 (local_rank 1) FAILED (exitcode 1) Error msg: Process failed with exitcode 1 Without writing an error file to <N/A>. While this DOES NOT affect the correctness of your application, no trace information about the error will be available for inspection. Consider decorating your top level entrypoint function with torch.distributed.elastic.multiprocessing.errors.record. Example:   from torch.distributed.elastic.multiprocessing.errors import record   @record   def trainer_main(args):      # do train   warnings.warn(_no_error_file_warning_msg(rank, failure)) Traceback (most recent call last):   File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main return _run_code(code, main_globals, None,   File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code     exec(code, run_globals)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 702, in <module> main()   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 361, in wrapper return f(*args, **kwargs)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 698, in main run(args)   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 689, in run elastic_launch(   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 116, in __call__ return launch_agent(self._config, self._entrypoint, list(args))   File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 244, in launch_agent raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError:  ***************************************           train_ddp.py FAILED           ======================================= Root Cause: [0]:   time: 2022-04-07_22:45:12   rank: 1 (local_rank: 0)   exitcode: 1 (pid: 40)   error_file: <N/A>   msg: \"Process failed with exitcode 1\" Other Failures:   <NO_OTHER_FAILURES>   File \"launcher.py\", line 82, in <module>   File \"launcher.py\", line 78, in main raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd) subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-m', 'torch.distributed.run', '--nproc_per_node=1', '--nnodes=2', '--node_rank=1', '--rdzv_id=pytorch-training-2022-04-07-22-36-53-872', '--rdzv_backend=c10d', '--rdzv_endpoint=algo-1:7777', 'train_ddp.py', '--batch-size-per-device', '16', '--epochs', '4', '--steps-per-epoch', '100']' returned non-zero exit status 1.\"\n",
      "Command \"/opt/conda/bin/python launcher.py --batch-size-per-device 16 --epochs 4 --steps-per-epoch 100 --train-script train_ddp.py\"\n",
      "2022-04-07 22:45:12,537 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\n",
      "2022-04-07 22:45:23 Uploading - Uploading generated training model\n",
      "2022-04-07 22:45:23 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2022-04-07-22-36-53-872: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"ZeroDivisionError: integer division or modulo by zero\n ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 40) of binary: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning:   **********************************************************************                CHILD PROCESS FAILED WITH NO ERROR_FILE                 CHILD PROCESS FAILED WITH NO ERROR_FILE Child process 40 (local_rank 1) FAILED (exitcode 1) Error msg: Process failed with exitcode 1 Without writing an error file to <N/A>. While this DOES NOT affect the correctness of your application, no trace information about the error will be available for inspection. Consider decorating your top level entrypoint function with torch.distributed.elastic.multiprocessing.errors.record. Example:   from torch.distributed.elastic.multiprocessing.errors import record   @reco",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=8'>9</a>\u001b[0m hyperparameters \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=9'>10</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mtrain-script\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtrain_ddp.py\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=10'>11</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=11'>12</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39mbatch-size-per-device\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m16\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=12'>13</a>\u001b[0m   \u001b[39m'\u001b[39m\u001b[39msteps-per-epoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=13'>14</a>\u001b[0m   }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=15'>16</a>\u001b[0m estimator_ms \u001b[39m=\u001b[39m PyTorch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=16'>17</a>\u001b[0m                        source_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m2_sources\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=17'>18</a>\u001b[0m                        entry_point\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlauncher.py\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=25'>26</a>\u001b[0m                        instance_type\u001b[39m=\u001b[39mps_instance_type,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=26'>27</a>\u001b[0m                        )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vdabravolski/repos/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/chapter6/2_distributed_training_PyTorch.ipynb#ch0000002?line=28'>29</a>\u001b[0m estimator_ms\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py:955\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=952'>953</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m    <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=953'>954</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=954'>955</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py:1954\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1951'>1952</a>\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1952'>1953</a>\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1953'>1954</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1954'>1955</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/estimator.py?line=1955'>1956</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py:3792\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3788'>3789</a>\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3790'>3791</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3791'>3792</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3792'>3793</a>\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3793'>3794</a>\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py:3330\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3323'>3324</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3324'>3325</a>\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3325'>3326</a>\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3326'>3327</a>\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3327'>3328</a>\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3328'>3329</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3329'>3330</a>\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3330'>3331</a>\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3331'>3332</a>\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3332'>3333</a>\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/vdabravolski/miniconda/envs/sagemaker/lib/python3.9/site-packages/sagemaker/session.py?line=3333'>3334</a>\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2022-04-07-22-36-53-872: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"ZeroDivisionError: integer division or modulo by zero\n ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 40) of binary: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning:   **********************************************************************                CHILD PROCESS FAILED WITH NO ERROR_FILE                 CHILD PROCESS FAILED WITH NO ERROR_FILE Child process 40 (local_rank 1) FAILED (exitcode 1) Error msg: Process failed with exitcode 1 Without writing an error file to <N/A>. While this DOES NOT affect the correctness of your application, no trace information about the error will be available for inspection. Consider decorating your top level entrypoint function with torch.distributed.elastic.multiprocessing.errors.record. Example:   from torch.distributed.elastic.multiprocessing.errors import record   @reco"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "ps_instance_type = 'ml.p2.xlarge'\n",
    "ps_instance_count = 2\n",
    "\n",
    "#distribution = {'parameter_server': {\n",
    "#                    'enabled': True}\n",
    "#                }\n",
    "hyperparameters = {\n",
    "  'train-script': 'train_ddp.py',\n",
    "  'epochs': 4,\n",
    "  #'batch-size-per-device' : 16,\n",
    "  #'steps-per-epoch': 100\n",
    "  }\n",
    "\n",
    "estimator_ms = PyTorch(\n",
    "                       source_dir='2_sources',\n",
    "                       entry_point='launcher.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.9',\n",
    "                       py_version='py38',\n",
    "                       disable_profiler=True,\n",
    "                       debugger_hook_config=False,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       instance_count=ps_instance_count, \n",
    "                       instance_type=ps_instance_type,\n",
    "                       )\n",
    "\n",
    "estimator_ms.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sagemaker': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
