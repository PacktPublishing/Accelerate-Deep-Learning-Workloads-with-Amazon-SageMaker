{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling TensorFlow Models Using SageMaker Neo\n",
    "\n",
    "In this example, we will train the `ResNet50` model, compile it for several hardware platforms, and deploy inference endpoints of optimized models. \n",
    "\n",
    "SageMaker Neo allows you to compile and optimize DL models for a wide range of target hardware platforms. It supports PyTorch, TensorFlow, MXNet, and ONNX models for hardware platforms such as Ambarella, ARM, Intel, NVIDIA. NXP, Qualcomm, Texas Instruments, and Xilinx. SageMaker Neo also supports deployment for cloud instances, as well as edge devices.\n",
    "\n",
    "Under the hood, SageMaker Neo converts your trained model from a framework-specific representation into an intermediate framework-agnostic representation. Then, it applies automatic optimizations and generates binary code for the optimized operations. Once the model has been compiled, you can deploy it to the target instance type it using the SageMaker Inference service. Neo also provides a runtime for each target platform that loads and executes the compiled model. \n",
    "\n",
    "Run cell below for initial imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use public MNIST dataset hosted on S3 by AWS team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_uri = 's3://sagemaker-sample-data-{}/tensorflow/mnist'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes we also need to download data locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp s3://sagemaker-sample-data-{region}/tensorflow/mnist . --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run code below to load data samples and labels into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inference_data = np.load(\"eval_data.npy\")\n",
    "inference_labels = np.load(\"eval_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "\n",
    "Before we can compile model, we need to train it first. For this example, we prepared a single script for both training and inference. \n",
    "\n",
    "Note, that to serve TensorFlow models, we implemented the simple `serving_input_fn()` method, which passes inputs to the model and returns predictions:\n",
    "\n",
    "```python\n",
    "    def serving_input_fn():\n",
    "        inputs = {\"x\": tf.placeholder(tf.float32, [None, 784])}\n",
    "        return tf.estimator.export.ServingInputReceiver(inputs,\n",
    "    inputs)\n",
    "```\n",
    "\n",
    "Feel free to review full script by running cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize 3_src/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training Job\n",
    "\n",
    "Run the cell below to train the model on SageMaker:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             source_dir=\"3_src\",\n",
    "                             role=role,\n",
    "                             instance_count=1,\n",
    "                             instance_type='ml.p3.2xlarge',\n",
    "                             framework_version='1.15.0',\n",
    "                             py_version='py3',\n",
    "                             )\n",
    "\n",
    "mnist_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Model For Different Target Platforms\n",
    "\n",
    "Let's compile our model for two inference platforms:\n",
    "- `inf` instance with Inferentia accelerator.\n",
    "- `c5` instance.\n",
    "\n",
    "You can find a full list of supported target platforms here: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html. Refer to `TargetPlatform` and `TargetDevice` parameters.\n",
    "\n",
    "### Using Inferentia\n",
    "\n",
    "1. We start by compiling model for `ml_inf1` target platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "\n",
    "inf_estimator = mnist_estimator.compile_model(target_instance_family='ml_inf1', \n",
    "                              input_shape={'data':[1, 784]},\n",
    "                              output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, we deploy our compiled model to Inferentia instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_predictor = inf_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.inf1.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can test inference results by running cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_predictor.predict(inference_data[0:4].reshape(4,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CPU Instance\n",
    "\n",
    "To optimize model for CPU instance looks very similar:\n",
    "1. First, we compile the model for `ml_c5` target platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c5_estimator = mnist_estimator.compile_model(target_instance_family='ml_c5', \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 1 channels, 28x28 Images.\n",
    "                              output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we deploy model to `c5` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c5_predictor = c5_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You can test inference results by running cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c5_predictor.predict(inference_data[0].reshape(1,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Cleanup\n",
    "\n",
    "Run the cell below to delete created cloud  resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c5_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "inf_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02fb69b38420c3d4e00e3a2af627e83f052bc85ba6fe46654fe57240b48dcaee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sagemaker2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
