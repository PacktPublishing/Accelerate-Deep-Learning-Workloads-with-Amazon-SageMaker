{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Models with Neuron SDK\n",
    "\n",
    "In this tutorial we will compile using Neuron SDK and run inference on AWS Inferentia instance. For this, we will fist compile `ResNet50` model and infer with a batch size of 1. After that we will tune model performance using NeuronCores `torch.neuron.DataParallel` and dynamic batching capabilities. \n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "1. **Selecting instance.** To run this exmple, you need to run this notebook on `inf1.6xlarge` instance. At the time of writing SageMaker Notebook Instances and SageMaker Studio Notebooks don't support Inferentia-based instances. Hence, you will need to use AWS EC2 instance instead. It's recommended to use latest Deep Learning AMI GPU PyTorch image for it which comes with Jupyter environment pre-installed.\n",
    "\n",
    "2. **Setting up Neuron SDK.** You need to follow setup NeuronSDK guide to install it and other dependencies. Refer to latest documentation here: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/setup/pytorch-install.html\n",
    "\n",
    "3. **Using correct Jupyter Kernel.** When using this notebook, make sure that you selected `Python (Neuron PyTorch)`.\n",
    "\n",
    "## Compile Model for Neuron SDK\n",
    "Run following steps to compile ResNet50 with Neuron SDK:\n",
    "\n",
    "1. We start by importing required libraries, including `torch_neuron`, and download the ResNet50 model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch_neuron\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "# set model into eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we must analyze the model operators to identify if any model operators are not supported by Inferentia/Neuron SDK. For this, we  use random input image. Since the ResNet50 model is supported, the output of this command should confirm that all the model operators are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.zeros([1, 3, 224, 224], dtype=torch.float32)\n",
    "torch.neuron.analyze_model(model, example_inputs=[image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we are ready to compile by running the following command. You will see the compilation statistics (such as number of supported operators) and overal compilation status in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron = torch.neuron.trace(model, example_inputs=[image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Since Neuron SDK compiles into a TorchScript program, saving and loading the model is similar to what you would do in regular PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neuron.save(\"resnet50_neuron.pt\")\n",
    "\n",
    "# model_neuron = torch.jit.load('resnet50_neuron.pt') # loading compiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference \n",
    "\n",
    "Let's test our compiledd model. In the example below, we run inference using the CPU model and compiled Neuron model. We then will compare the predicted labels from the CPU model and Neuron model to verify that they are the same.\n",
    "\n",
    "    Important: Do not perform inference with a Neuron traced model on a non-Neuron supported instance, as the results will not be calculated properly.\n",
    "\n",
    "### Define Helper Functions\n",
    "Before we begin, we need to define functions to preprocess images and benchmark inference. \n",
    "\n",
    "1. We define a basic image preprocessing function that loads a sample image and labels, normalizes and batches the image, and transforms the image into a tensor for inference using the compiled Neuron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(batch_size=1, num_neuron_cores=1):\n",
    "    # Define a normalization function using the ImageNet mean and standard deviation\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Resize the sample image to [1, 3, 224, 224], normalize it, and turn it into a tensor\n",
    "    eval_dataset = datasets.ImageFolder(\n",
    "        os.path.dirname(\"./torch_neuron_test/\"),\n",
    "        transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "    )\n",
    "    image, _ = eval_dataset[0]\n",
    "    image = torch.tensor(image.numpy()[np.newaxis, ...])\n",
    "\n",
    "    # Create a \"batched\" image with enough images to go on each of the available NeuronCores\n",
    "    # batch_size is the per-core batch size\n",
    "    # num_neuron_cores is the number of NeuronCores being used\n",
    "    batch_image = image\n",
    "    for i in range(batch_size * num_neuron_cores - 1):\n",
    "        batch_image = torch.cat([batch_image, image], 0)\n",
    "     \n",
    "    return batch_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We also need to define benchmarking function to compare inference performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def benchmark(model, image):\n",
    "    print('Input image shape is {}'.format(list(image.shape)))\n",
    "    \n",
    "    # The first inference loads the model so exclude it from timing \n",
    "    results = model(image)\n",
    "    \n",
    "    # Collect throughput and latency metrics\n",
    "    latency = []\n",
    "    throughput = []\n",
    "\n",
    "    # Run inference for 100 iterations and calculate metrics\n",
    "    num_infers = 100\n",
    "    for _ in range(num_infers):\n",
    "        delta_start = time()\n",
    "        results = model(image)\n",
    "        delta = time() - delta_start\n",
    "        latency.append(delta)\n",
    "        throughput.append(image.size(0)/delta)\n",
    "    \n",
    "    # Calculate and print the model throughput and latency\n",
    "    print(\"Avg. Throughput: {:.0f}, Max Throughput: {:.0f}\".format(np.mean(throughput), np.max(throughput)))\n",
    "    print(\"Latency P50: {:.0f}\".format(np.percentile(latency, 50)*1000.0))\n",
    "    print(\"Latency P90: {:.0f}\".format(np.percentile(latency, 90)*1000.0))\n",
    "    print(\"Latency P95: {:.0f}\".format(np.percentile(latency, 95)*1000.0))\n",
    "    print(\"Latency P99: {:.0f}\\n\".format(np.percentile(latency, 99)*1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Below we downloadn several image samples which we'll use for benchmarking and accuracy verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from urllib import request\n",
    "\n",
    "os.makedirs(\"./torch_neuron_test/images\", exist_ok=True)\n",
    "request.urlretrieve(\"https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg\",\n",
    "                    \"./torch_neuron_test/images/kitten_small.jpg\")\n",
    "\n",
    "request.urlretrieve(\"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\",\"imagenet_class_index.json\")\n",
    "idx2label = []\n",
    "\n",
    "with open(\"imagenet_class_index.json\", \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Compiled Model Accuracy\n",
    "\n",
    "Let's compare if compiled model has similar accuracy to non-compiled model. Execute cell below to test model performance on sample images and compare it to accuracy for CPU ResNet model. We expect that predictions of both models will match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch_neuron\n",
    "\n",
    "# Get a sample image\n",
    "image = preprocess()\n",
    "\n",
    "# Run inference using the CPU model\n",
    "output_cpu = model(image)\n",
    "\n",
    "# Load the compiled Neuron model\n",
    "model_neuron = torch.jit.load('resnet50_neuron.pt')\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "output_neuron = model_neuron(image)\n",
    "\n",
    "# Verify that the CPU and Neuron predictions are the same by comparing\n",
    "# the top-5 results\n",
    "top5_cpu = output_cpu[0].sort()[1][-5:]\n",
    "top5_neuron = output_neuron[0].sort()[1][-5:]\n",
    "\n",
    "# Lookup and print the top-5 labels\n",
    "top5_labels_cpu = [idx2label[idx] for idx in top5_cpu]\n",
    "top5_labels_neuron = [idx2label[idx] for idx in top5_neuron]\n",
    "print(\"CPU top-5 labels: {}\".format(top5_labels_cpu))\n",
    "print(\"Neuron top-5 labels: {}\".format(top5_labels_neuron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Different Model Configuration\n",
    "\n",
    "Neuron SDK provides several performance optimization capabilities. Below we run a set of experiments for different model configuration. \n",
    "\n",
    "### Running Model On Multiple Neuron Cores\n",
    "\n",
    "To fully leverage the Inferentia hardware we want to use all avaialable NeuronCores. An inf1.xlarge and inf1.2xlarge have four NeuronCores, an inf1.6xlarge has 16 NeuronCores, and an inf1.24xlarge has 64 NeuronCores. For maximum performance on Inferentia hardware, we can use `torch.neuron.DataParallel` to utilize all available NeuronCores. Neuron DataParallel implements data parallelism at the module level by duplicating the Neuron model on all available NeuronCores and distributing data across the different cores for parallelized inference.\n",
    "\n",
    "In the example below we set number of neuron cores to 4. Feel free to change it depending on how many Neuron Cores are available on your Inferentia instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an inf1.xlarge or inf1.2xlarge set num_neuron_cores=4; \n",
    "# For inf1.6xlarge set it to 16; for inf1.24xlarge - 24.\n",
    "num_neuron_cores = 4\n",
    "\n",
    "model_neuron_parallel = torch.neuron.DataParallel(model_neuron)\n",
    "\n",
    "# Get sample image with batch size=1 per NeuronCore\n",
    "batch_size = 1\n",
    "\n",
    "image = preprocess(batch_size=batch_size, num_neuron_cores=num_neuron_cores)\n",
    "\n",
    "# Benchmark the model\n",
    "benchmark(model_neuron_parallel, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark results should be similar to the following:\n",
    "```python\n",
    "Input image shape is [4, 3, 224, 224]\n",
    "Avg. Throughput: 551, Max Throughput: 562\n",
    "Latency P50: 7\n",
    "Latency P90: 7\n",
    "Latency P95: 7\n",
    "Latency P99: 7\n",
    "```\n",
    "\n",
    "### Running Inference With Different Batch Size\n",
    "In this experiment, we will compile our model to run inference on batched samples to improve throughput. \n",
    "\n",
    "Note, that `dynamic batching` using small batch sizes can result in sub-optimal throughput because it involves slicing tensors into chunks and iteratively sending data to the hardware. Using a larger batch size at compilation time can use the Inferentia hardware more efficiently in order to maximize throughput. You can test the tradeoff between individual request latency and total throughput by fine-tuning the input batch size.\n",
    "\n",
    "In the following example, we recompile our model using a batch size of 5 and run the model using `torch.neuron.DataParallel` to fully saturate our Inferentia hardware for optimal performance.\n",
    "\n",
    "1. We start by recompiling model for batch size equal to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input with batch size 5 for compilation\n",
    "batch_size = 5\n",
    "image = torch.zeros([batch_size, 3, 224, 224], dtype=torch.float32)\n",
    "\n",
    "# Recompile the ResNet50 model for inference with batch size 5\n",
    "model_neuron = torch.neuron.trace(model, example_inputs=[image])\n",
    "\n",
    "# Export to saved model\n",
    "model_neuron.save(\"resnet50_neuron_b{}.pt\".format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's benchmark inference for newly compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compiled Neuron model\n",
    "model_neuron = torch.jit.load(\"resnet50_neuron_b{}.pt\".format(batch_size))\n",
    "\n",
    "# Create DataParallel model\n",
    "model_neuron_parallel = torch.neuron.DataParallel(model_neuron)\n",
    "\n",
    "# Get sample image with batch size=5\n",
    "image = preprocess(batch_size=batch_size, num_neuron_cores=num_neuron_cores)\n",
    "\n",
    "# Benchmark the model\n",
    "benchmark(model_neuron_parallel, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmarkâ€™s output should look as follows:\n",
    "\n",
    "```python\n",
    "Input image shape is [20, 3, 224, 224]\n",
    "Avg. Throughput: 979, Max Throughput: 998\n",
    "Latency P50: 20\n",
    "Latency P90: 21\n",
    "Latency P95: 21\n",
    "Latency P99: 24\n",
    "```\n",
    "\n",
    "Note, that while latency has increased, the overall throughput is increased as expected.\n",
    "\n",
    "### Running Inference with NeuronCore Pipeline\n",
    "In our last experiment, we will use Pipeline features.\n",
    "\n",
    "1. We first recompile model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Cores in the Pipeline Mode\n",
    "neuroncore_pipeline_cores = 4\n",
    "\n",
    "image = preprocess(batch_size=batch_size, num_neuron_cores=num_neuron_cores)\n",
    "benchmark(neuron_pipeline_model, image)\n",
    "\n",
    "# Compiling for neuroncore-pipeline-cores='16'\n",
    "neuron_pipeline_model = torch.neuron.trace(model,\n",
    "                                           example_inputs=[image],\n",
    "                                           verbose=1,\n",
    "                                           compiler_args = ['--neuroncore-pipeline-cores', str(neuroncore_pipeline_cores)]\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = neuron_pipeline_model(*image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cae6ef5e525c6d5a8daa33565a4e32326fcdb22bb4405c41032726ef6ebbb77e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('sagemaker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
